{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "# for building DQN model\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the environment\n",
    "from Env import CabDriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data shape:\n",
      " (5, 5, 24, 7)\n",
      "dimensions =  4\n"
     ]
    }
   ],
   "source": [
    "# Loading the time matrix provided\n",
    "Time_matrix = np.load(\"TM.npy\")\n",
    "print(\"\\nData shape:\\n\", np.shape(Time_matrix))\n",
    "print(\"dimensions = \",len(Time_matrix.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking the state-action pairs for checking convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Define size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Write here: Specify you hyper parameters for the DQN\n",
    "        self.discount_factor =0.9\n",
    "        self.learning_rate =0.001        \n",
    "        self.epsilon_max = 1\n",
    "        self.epsilon_decay =  .995 #.999\n",
    "        self.epsilon_min = 0.001\n",
    "        self.epsilon = 1\n",
    "\n",
    "        \n",
    "        self.batch_size = 32        \n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        # Write your code here: Add layers to your neural nets       \n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "\n",
    "        # the output layer: output is of size num_actions\n",
    "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        \n",
    "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "    def get_action(self, state,env):\n",
    "    # Write your code here:\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    # Decay in Îµ after we generate each sample from the environment       \n",
    "        possible_actions_index,actions = env.requests(state) # Find possible action indexes and append 0\n",
    "        possible_actions_index.append(0)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.sample(possible_actions_index,1)[0]\n",
    "        else:\n",
    "            state = state.reshape(1, self.state_size)\n",
    "            q_value = self.model.predict(state)\n",
    "            # Give action with max q_value only amongst possible action\n",
    "            return np.where(q_value[0] == np.max(np.array([q_value[0][i] for i in possible_actions_index])))[0][0]    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def append_sample(self, state, action, reward, next_state,done):\n",
    "    # Write your code here:\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "        self.memory.append((state, action, reward, next_state,done))\n",
    "    \n",
    "    \n",
    "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
    "    def train_model(self,env):\n",
    "        \n",
    "        if len(self.memory) > self.batch_size:\n",
    "            # Sample batch from the memory\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\n",
    "            update_output = np.zeros((self.batch_size, self.state_size)) # write here\n",
    "            update_input = np.zeros((self.batch_size, self.state_size)) # write here\n",
    "            \n",
    "            actions, rewards, done = [], [], []\n",
    "            \n",
    "            for i in range(self.batch_size):\n",
    "                state, action, reward, next_state, done_boolean = mini_batch[i]\n",
    "                \n",
    "                update_input[i] = state\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                done.append(done_boolean)\n",
    "                update_output[i] = next_state\n",
    "                \n",
    "                # Write your code from here\n",
    "                \n",
    "            # 1. Predict the target from earlier model           \n",
    "            target = self.model.predict(update_input)\n",
    "            \n",
    "            # 2. Get the target for the Q-network\n",
    "            target_qval = self.model.predict(update_output)\n",
    "                \n",
    "                #3. Update your 'update_output' and 'update_input' batch\n",
    "            for i in range(self.batch_size):\n",
    "                # Find possible actions from next state\n",
    "                next_possible_actions_index,_ = env.requests(update_output[i])\n",
    "                next_possible_actions_index.append(0)\n",
    "                if not done[i]:\n",
    "                    # Only take the max q_value from valid actions from next state\n",
    "                    target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(np.array([target_qval[i][j] for j in next_possible_actions_index]))\n",
    "                else:\n",
    "                    target[i][actions[i]] = rewards[i]\n",
    "                \n",
    "                \n",
    "        # 4. Fit your model and track the loss values\n",
    "            #print(\"Training Model\")\n",
    "            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "            #print(\"Model Training Model\")\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episodes = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State is  [3, 9, 3]\n",
      "episode 0, reward -89.0, memory_length 133, epsilon 0.995, time 732.0, rides 132\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 1, reward -140.0, memory_length 268, epsilon 0.990025, time 732.0, rides 134\n",
      "Initial State is  [4, 8, 6]\n",
      "episode 2, reward -208.0, memory_length 401, epsilon 0.985074875, time 730.0, rides 132\n",
      "Initial State is  [1, 19, 5]\n",
      "episode 3, reward -392.0, memory_length 510, epsilon 0.9801495006250001, time 724.0, rides 108\n",
      "Initial State is  [4, 17, 6]\n",
      "episode 4, reward -443.0, memory_length 657, epsilon 0.9752487531218751, time 732.0, rides 146\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 5, reward -236.0, memory_length 779, epsilon 0.9703725093562657, time 725.0, rides 121\n",
      "Initial State is  [4, 19, 5]\n",
      "episode 6, reward -52.0, memory_length 905, epsilon 0.9655206468094844, time 727.0, rides 125\n",
      "Initial State is  [1, 16, 5]\n",
      "episode 7, reward 71.0, memory_length 1018, epsilon 0.960693043575437, time 731.0, rides 112\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 8, reward -275.0, memory_length 1148, epsilon 0.9558895783575597, time 725.0, rides 129\n",
      "Initial State is  [3, 13, 6]\n",
      "episode 9, reward 50.0, memory_length 1275, epsilon 0.9511101304657719, time 731.0, rides 126\n",
      "Initial State is  [4, 2, 3]\n",
      "episode 10, reward -172.0, memory_length 1403, epsilon 0.946354579813443, time 727.0, rides 127\n",
      "Initial State is  [4, 16, 1]\n",
      "episode 11, reward -223.0, memory_length 1533, epsilon 0.9416228069143757, time 737.0, rides 129\n",
      "Initial State is  [4, 10, 1]\n",
      "episode 12, reward -193.0, memory_length 1655, epsilon 0.9369146928798039, time 728.0, rides 121\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 13, reward -337.0, memory_length 1780, epsilon 0.9322301194154049, time 731.0, rides 124\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 14, reward 11.0, memory_length 1912, epsilon 0.9275689688183278, time 730.0, rides 131\n",
      "Initial State is  [0, 4, 1]\n",
      "episode 15, reward -179.0, memory_length 2000, epsilon 0.9229311239742362, time 728.0, rides 116\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 16, reward -74.0, memory_length 2000, epsilon 0.918316468354365, time 736.0, rides 127\n",
      "Initial State is  [2, 6, 1]\n",
      "episode 17, reward -130.0, memory_length 2000, epsilon 0.9137248860125932, time 731.0, rides 120\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 18, reward -56.0, memory_length 2000, epsilon 0.9091562615825302, time 743.0, rides 133\n",
      "Initial State is  [3, 5, 0]\n",
      "episode 19, reward -8.0, memory_length 2000, epsilon 0.9046104802746175, time 733.0, rides 116\n",
      "Initial State is  [0, 5, 6]\n",
      "episode 20, reward -216.0, memory_length 2000, epsilon 0.9000874278732445, time 731.0, rides 115\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 21, reward -391.0, memory_length 2000, epsilon 0.8955869907338783, time 731.0, rides 119\n",
      "Initial State is  [1, 6, 4]\n",
      "episode 22, reward -91.0, memory_length 2000, epsilon 0.8911090557802088, time 732.0, rides 125\n",
      "Initial State is  [4, 6, 2]\n",
      "episode 23, reward -148.0, memory_length 2000, epsilon 0.8866535105013078, time 734.0, rides 119\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 24, reward 204.0, memory_length 2000, epsilon 0.8822202429488013, time 732.0, rides 127\n",
      "Initial State is  [1, 19, 0]\n",
      "episode 25, reward 28.0, memory_length 2000, epsilon 0.8778091417340573, time 736.0, rides 130\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 26, reward -190.0, memory_length 2000, epsilon 0.8734200960253871, time 720.0, rides 118\n",
      "Initial State is  [3, 5, 3]\n",
      "episode 27, reward 10.0, memory_length 2000, epsilon 0.8690529955452602, time 734.0, rides 121\n",
      "Initial State is  [4, 20, 5]\n",
      "episode 28, reward -71.0, memory_length 2000, epsilon 0.8647077305675338, time 741.0, rides 144\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 29, reward -58.0, memory_length 2000, epsilon 0.8603841919146962, time 723.0, rides 125\n",
      "Initial State is  [1, 10, 6]\n",
      "episode 30, reward -335.0, memory_length 2000, epsilon 0.8560822709551227, time 728.0, rides 120\n",
      "Initial State is  [0, 22, 6]\n",
      "episode 31, reward -194.0, memory_length 2000, epsilon 0.851801859600347, time 739.0, rides 126\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 32, reward 81.0, memory_length 2000, epsilon 0.8475428503023453, time 724.0, rides 120\n",
      "Initial State is  [1, 20, 4]\n",
      "episode 33, reward -24.0, memory_length 2000, epsilon 0.8433051360508336, time 729.0, rides 122\n",
      "Initial State is  [3, 10, 4]\n",
      "episode 34, reward -328.0, memory_length 2000, epsilon 0.8390886103705794, time 725.0, rides 131\n",
      "Initial State is  [2, 16, 2]\n",
      "episode 35, reward -223.0, memory_length 2000, epsilon 0.8348931673187264, time 736.0, rides 130\n",
      "Initial State is  [4, 14, 4]\n",
      "episode 36, reward -122.0, memory_length 2000, epsilon 0.8307187014821328, time 736.0, rides 133\n",
      "Initial State is  [2, 8, 0]\n",
      "episode 37, reward -64.0, memory_length 2000, epsilon 0.8265651079747222, time 728.0, rides 120\n",
      "Initial State is  [3, 8, 1]\n",
      "episode 38, reward -169.0, memory_length 2000, epsilon 0.8224322824348486, time 723.0, rides 131\n",
      "Initial State is  [1, 15, 0]\n",
      "episode 39, reward -156.0, memory_length 2000, epsilon 0.8183201210226743, time 731.0, rides 125\n",
      "Initial State is  [1, 4, 5]\n",
      "episode 40, reward -39.0, memory_length 2000, epsilon 0.8142285204175609, time 731.0, rides 111\n",
      "Initial State is  [4, 19, 3]\n",
      "episode 41, reward 56.0, memory_length 2000, epsilon 0.810157377815473, time 725.0, rides 124\n",
      "Initial State is  [4, 5, 1]\n",
      "episode 42, reward -49.0, memory_length 2000, epsilon 0.8061065909263957, time 728.0, rides 119\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 43, reward -122.0, memory_length 2000, epsilon 0.8020760579717637, time 722.0, rides 126\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 44, reward 88.0, memory_length 2000, epsilon 0.798065677681905, time 730.0, rides 117\n",
      "Initial State is  [0, 11, 6]\n",
      "episode 45, reward -151.0, memory_length 2000, epsilon 0.7940753492934954, time 737.0, rides 141\n",
      "Initial State is  [4, 15, 3]\n",
      "episode 46, reward -162.0, memory_length 2000, epsilon 0.7901049725470279, time 727.0, rides 118\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 47, reward -408.0, memory_length 2000, epsilon 0.7861544476842928, time 723.0, rides 134\n",
      "Initial State is  [0, 18, 1]\n",
      "episode 48, reward -270.0, memory_length 2000, epsilon 0.7822236754458713, time 727.0, rides 125\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 49, reward 60.0, memory_length 2000, epsilon 0.778312557068642, time 731.0, rides 119\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 50, reward 90.0, memory_length 2000, epsilon 0.7744209942832988, time 729.0, rides 131\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 51, reward -161.0, memory_length 2000, epsilon 0.7705488893118823, time 733.0, rides 119\n",
      "Initial State is  [0, 20, 4]\n",
      "episode 52, reward 63.0, memory_length 2000, epsilon 0.7666961448653229, time 735.0, rides 133\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 53, reward -38.0, memory_length 2000, epsilon 0.7628626641409962, time 727.0, rides 125\n",
      "Initial State is  [1, 13, 3]\n",
      "episode 54, reward 115.0, memory_length 2000, epsilon 0.7590483508202912, time 729.0, rides 127\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 55, reward -158.0, memory_length 2000, epsilon 0.7552531090661897, time 727.0, rides 117\n",
      "Initial State is  [0, 4, 0]\n",
      "episode 56, reward -188.0, memory_length 2000, epsilon 0.7514768435208588, time 740.0, rides 121\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 57, reward 25.0, memory_length 2000, epsilon 0.7477194593032545, time 728.0, rides 113\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 58, reward -276.0, memory_length 2000, epsilon 0.7439808620067382, time 735.0, rides 119\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 59, reward 74.0, memory_length 2000, epsilon 0.7402609576967045, time 735.0, rides 119\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 60, reward 206.0, memory_length 2000, epsilon 0.736559652908221, time 736.0, rides 125\n",
      "Initial State is  [2, 11, 4]\n",
      "episode 61, reward 140.0, memory_length 2000, epsilon 0.7328768546436799, time 729.0, rides 139\n",
      "Initial State is  [1, 13, 4]\n",
      "episode 62, reward -65.0, memory_length 2000, epsilon 0.7292124703704616, time 733.0, rides 122\n",
      "Initial State is  [2, 5, 4]\n",
      "episode 63, reward 52.0, memory_length 2000, epsilon 0.7255664080186093, time 725.0, rides 136\n",
      "Initial State is  [3, 5, 0]\n",
      "episode 64, reward -14.0, memory_length 2000, epsilon 0.7219385759785162, time 732.0, rides 112\n",
      "Initial State is  [0, 0, 5]\n",
      "episode 65, reward 323.0, memory_length 2000, epsilon 0.7183288830986236, time 736.0, rides 123\n",
      "Initial State is  [2, 23, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 66, reward 349.0, memory_length 2000, epsilon 0.7147372386831305, time 724.0, rides 126\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 67, reward 113.0, memory_length 2000, epsilon 0.7111635524897149, time 721.0, rides 125\n",
      "Initial State is  [4, 3, 5]\n",
      "episode 68, reward 246.0, memory_length 2000, epsilon 0.7076077347272662, time 731.0, rides 134\n",
      "Initial State is  [0, 2, 6]\n",
      "episode 69, reward 34.0, memory_length 2000, epsilon 0.7040696960536299, time 724.0, rides 125\n",
      "Initial State is  [3, 3, 1]\n",
      "episode 70, reward 371.0, memory_length 2000, epsilon 0.7005493475733617, time 730.0, rides 124\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 71, reward 65.0, memory_length 2000, epsilon 0.697046600835495, time 731.0, rides 129\n",
      "Initial State is  [3, 17, 1]\n",
      "episode 72, reward 338.0, memory_length 2000, epsilon 0.6935613678313175, time 724.0, rides 118\n",
      "Initial State is  [4, 12, 1]\n",
      "episode 73, reward 191.0, memory_length 2000, epsilon 0.6900935609921609, time 731.0, rides 143\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 74, reward -84.0, memory_length 2000, epsilon 0.6866430931872001, time 733.0, rides 107\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 75, reward 168.0, memory_length 2000, epsilon 0.6832098777212641, time 732.0, rides 123\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 76, reward 145.0, memory_length 2000, epsilon 0.6797938283326578, time 728.0, rides 117\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 77, reward 272.0, memory_length 2000, epsilon 0.6763948591909945, time 737.0, rides 131\n",
      "Initial State is  [2, 15, 5]\n",
      "episode 78, reward 139.0, memory_length 2000, epsilon 0.6730128848950395, time 729.0, rides 116\n",
      "Initial State is  [2, 16, 1]\n",
      "episode 79, reward 92.0, memory_length 2000, epsilon 0.6696478204705644, time 725.0, rides 130\n",
      "Initial State is  [1, 11, 1]\n",
      "episode 80, reward 86.0, memory_length 2000, epsilon 0.6662995813682115, time 735.0, rides 115\n",
      "Initial State is  [0, 18, 3]\n",
      "episode 81, reward 235.0, memory_length 2000, epsilon 0.6629680834613705, time 726.0, rides 121\n",
      "Initial State is  [3, 6, 2]\n",
      "episode 82, reward 139.0, memory_length 2000, epsilon 0.6596532430440636, time 725.0, rides 124\n",
      "Initial State is  [1, 5, 2]\n",
      "episode 83, reward 124.0, memory_length 2000, epsilon 0.6563549768288433, time 736.0, rides 120\n",
      "Initial State is  [2, 8, 0]\n",
      "episode 84, reward 4.0, memory_length 2000, epsilon 0.653073201944699, time 726.0, rides 115\n",
      "Initial State is  [4, 14, 1]\n",
      "episode 85, reward 234.0, memory_length 2000, epsilon 0.6498078359349755, time 735.0, rides 133\n",
      "Initial State is  [0, 17, 5]\n",
      "episode 86, reward 170.0, memory_length 2000, epsilon 0.6465587967553006, time 729.0, rides 115\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 87, reward 214.0, memory_length 2000, epsilon 0.6433260027715241, time 735.0, rides 132\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 88, reward 101.0, memory_length 2000, epsilon 0.6401093727576664, time 736.0, rides 116\n",
      "Initial State is  [1, 15, 1]\n",
      "episode 89, reward -30.0, memory_length 2000, epsilon 0.6369088258938781, time 725.0, rides 116\n",
      "Initial State is  [1, 12, 4]\n",
      "episode 90, reward 427.0, memory_length 2000, epsilon 0.6337242817644086, time 731.0, rides 133\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 91, reward 398.0, memory_length 2000, epsilon 0.6305556603555866, time 728.0, rides 131\n",
      "Initial State is  [1, 18, 3]\n",
      "episode 92, reward -12.0, memory_length 2000, epsilon 0.6274028820538087, time 722.0, rides 121\n",
      "Initial State is  [1, 22, 2]\n",
      "episode 93, reward 375.0, memory_length 2000, epsilon 0.6242658676435396, time 732.0, rides 130\n",
      "Initial State is  [2, 22, 6]\n",
      "episode 94, reward 204.0, memory_length 2000, epsilon 0.6211445383053219, time 729.0, rides 121\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 95, reward 127.0, memory_length 2000, epsilon 0.6180388156137953, time 727.0, rides 123\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 96, reward 452.0, memory_length 2000, epsilon 0.6149486215357263, time 726.0, rides 131\n",
      "Initial State is  [4, 23, 0]\n",
      "episode 97, reward 384.0, memory_length 2000, epsilon 0.6118738784280476, time 729.0, rides 130\n",
      "Initial State is  [1, 12, 2]\n",
      "episode 98, reward 324.0, memory_length 2000, epsilon 0.6088145090359074, time 734.0, rides 128\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 99, reward 445.0, memory_length 2000, epsilon 0.6057704364907278, time 729.0, rides 129\n",
      "Initial State is  [4, 11, 0]\n",
      "episode 100, reward 418.0, memory_length 2000, epsilon 0.6027415843082742, time 731.0, rides 121\n",
      "Initial State is  [2, 21, 0]\n",
      "episode 101, reward 61.0, memory_length 2000, epsilon 0.5997278763867329, time 725.0, rides 114\n",
      "Initial State is  [4, 23, 3]\n",
      "episode 102, reward 30.0, memory_length 2000, epsilon 0.5967292370047992, time 723.0, rides 124\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 103, reward 61.0, memory_length 2000, epsilon 0.5937455908197752, time 733.0, rides 119\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 104, reward 349.0, memory_length 2000, epsilon 0.5907768628656763, time 726.0, rides 111\n",
      "Initial State is  [2, 1, 3]\n",
      "episode 105, reward 262.0, memory_length 2000, epsilon 0.5878229785513479, time 731.0, rides 118\n",
      "Initial State is  [4, 14, 1]\n",
      "episode 106, reward 285.0, memory_length 2000, epsilon 0.5848838636585911, time 722.0, rides 126\n",
      "Initial State is  [2, 19, 4]\n",
      "episode 107, reward -67.0, memory_length 2000, epsilon 0.5819594443402982, time 730.0, rides 117\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 108, reward 296.0, memory_length 2000, epsilon 0.5790496471185967, time 732.0, rides 120\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 109, reward 307.0, memory_length 2000, epsilon 0.5761543988830038, time 724.0, rides 126\n",
      "Initial State is  [3, 21, 5]\n",
      "episode 110, reward 36.0, memory_length 2000, epsilon 0.5732736268885887, time 734.0, rides 112\n",
      "Initial State is  [2, 18, 1]\n",
      "episode 111, reward 153.0, memory_length 2000, epsilon 0.5704072587541458, time 729.0, rides 110\n",
      "Initial State is  [4, 15, 0]\n",
      "episode 112, reward 120.0, memory_length 2000, epsilon 0.567555222460375, time 722.0, rides 126\n",
      "Initial State is  [4, 4, 2]\n",
      "episode 113, reward 466.0, memory_length 2000, epsilon 0.5647174463480732, time 731.0, rides 127\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 114, reward 204.0, memory_length 2000, epsilon 0.5618938591163328, time 725.0, rides 124\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 115, reward 77.0, memory_length 2000, epsilon 0.5590843898207511, time 727.0, rides 125\n",
      "Initial State is  [1, 12, 6]\n",
      "episode 116, reward 633.0, memory_length 2000, epsilon 0.5562889678716474, time 729.0, rides 133\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 117, reward 210.0, memory_length 2000, epsilon 0.5535075230322891, time 732.0, rides 114\n",
      "Initial State is  [3, 15, 0]\n",
      "episode 118, reward 249.0, memory_length 2000, epsilon 0.5507399854171277, time 731.0, rides 121\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 119, reward 212.0, memory_length 2000, epsilon 0.547986285490042, time 727.0, rides 126\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 120, reward 516.0, memory_length 2000, epsilon 0.5452463540625918, time 724.0, rides 122\n",
      "Initial State is  [4, 22, 4]\n",
      "episode 121, reward 358.0, memory_length 2000, epsilon 0.5425201222922789, time 730.0, rides 123\n",
      "Initial State is  [3, 4, 2]\n",
      "episode 122, reward 71.0, memory_length 2000, epsilon 0.5398075216808175, time 725.0, rides 116\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 123, reward 1.0, memory_length 2000, epsilon 0.5371084840724134, time 736.0, rides 118\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 124, reward 431.0, memory_length 2000, epsilon 0.5344229416520513, time 730.0, rides 116\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 125, reward 149.0, memory_length 2000, epsilon 0.531750826943791, time 722.0, rides 134\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 126, reward 164.0, memory_length 2000, epsilon 0.5290920728090721, time 731.0, rides 112\n",
      "Initial State is  [3, 11, 3]\n",
      "episode 127, reward 238.0, memory_length 2000, epsilon 0.5264466124450268, time 743.0, rides 118\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 128, reward 444.0, memory_length 2000, epsilon 0.5238143793828016, time 727.0, rides 127\n",
      "Initial State is  [4, 14, 2]\n",
      "episode 129, reward 294.0, memory_length 2000, epsilon 0.5211953074858876, time 738.0, rides 113\n",
      "Initial State is  [3, 16, 5]\n",
      "episode 130, reward 495.0, memory_length 2000, epsilon 0.5185893309484582, time 731.0, rides 120\n",
      "Initial State is  [4, 2, 4]\n",
      "episode 131, reward -5.0, memory_length 2000, epsilon 0.5159963842937159, time 724.0, rides 113\n",
      "Initial State is  [4, 17, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 132, reward 375.0, memory_length 2000, epsilon 0.5134164023722473, time 724.0, rides 114\n",
      "Initial State is  [2, 20, 3]\n",
      "episode 133, reward 457.0, memory_length 2000, epsilon 0.510849320360386, time 724.0, rides 117\n",
      "Initial State is  [3, 20, 4]\n",
      "episode 134, reward 391.0, memory_length 2000, epsilon 0.5082950737585841, time 732.0, rides 114\n",
      "Initial State is  [3, 3, 2]\n",
      "episode 135, reward 181.0, memory_length 2000, epsilon 0.5057535983897912, time 742.0, rides 118\n",
      "Initial State is  [2, 21, 4]\n",
      "episode 136, reward 640.0, memory_length 2000, epsilon 0.5032248303978422, time 731.0, rides 138\n",
      "Initial State is  [4, 12, 5]\n",
      "episode 137, reward 96.0, memory_length 2000, epsilon 0.500708706245853, time 734.0, rides 122\n",
      "Initial State is  [3, 18, 4]\n",
      "episode 138, reward 365.0, memory_length 2000, epsilon 0.4982051627146237, time 734.0, rides 119\n",
      "Initial State is  [3, 1, 1]\n",
      "episode 139, reward 673.0, memory_length 2000, epsilon 0.49571413690105054, time 726.0, rides 113\n",
      "Initial State is  [3, 3, 5]\n",
      "episode 140, reward 313.0, memory_length 2000, epsilon 0.4932355662165453, time 737.0, rides 128\n",
      "Initial State is  [4, 23, 2]\n",
      "episode 141, reward 193.0, memory_length 2000, epsilon 0.4907693883854626, time 731.0, rides 118\n",
      "Initial State is  [1, 0, 0]\n",
      "episode 142, reward 546.0, memory_length 2000, epsilon 0.4883155414435353, time 726.0, rides 126\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 143, reward 790.0, memory_length 2000, epsilon 0.4858739637363176, time 733.0, rides 135\n",
      "Initial State is  [1, 3, 2]\n",
      "episode 144, reward 396.0, memory_length 2000, epsilon 0.483444593917636, time 731.0, rides 119\n",
      "Initial State is  [4, 21, 4]\n",
      "episode 145, reward 167.0, memory_length 2000, epsilon 0.4810273709480478, time 731.0, rides 122\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 146, reward 437.0, memory_length 2000, epsilon 0.47862223409330756, time 727.0, rides 128\n",
      "Initial State is  [3, 2, 2]\n",
      "episode 147, reward 326.0, memory_length 2000, epsilon 0.47622912292284103, time 738.0, rides 121\n",
      "Initial State is  [1, 7, 5]\n",
      "episode 148, reward 363.0, memory_length 2000, epsilon 0.4738479773082268, time 732.0, rides 115\n",
      "Initial State is  [4, 2, 3]\n",
      "episode 149, reward 635.0, memory_length 2000, epsilon 0.47147873742168567, time 730.0, rides 116\n",
      "Initial State is  [1, 11, 6]\n",
      "episode 150, reward 392.0, memory_length 2000, epsilon 0.46912134373457726, time 728.0, rides 122\n",
      "Initial State is  [2, 8, 1]\n",
      "episode 151, reward 330.0, memory_length 2000, epsilon 0.46677573701590436, time 729.0, rides 109\n",
      "Initial State is  [0, 0, 2]\n",
      "episode 152, reward 408.0, memory_length 2000, epsilon 0.46444185833082485, time 740.0, rides 122\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 153, reward 884.0, memory_length 2000, epsilon 0.46211964903917074, time 725.0, rides 118\n",
      "Initial State is  [1, 21, 6]\n",
      "episode 154, reward 668.0, memory_length 2000, epsilon 0.4598090507939749, time 730.0, rides 114\n",
      "Initial State is  [1, 12, 4]\n",
      "episode 155, reward 91.0, memory_length 2000, epsilon 0.457510005540005, time 727.0, rides 120\n",
      "Initial State is  [4, 4, 2]\n",
      "episode 156, reward 219.0, memory_length 2000, epsilon 0.45522245551230495, time 734.0, rides 115\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 157, reward 339.0, memory_length 2000, epsilon 0.4529463432347434, time 728.0, rides 113\n",
      "Initial State is  [3, 4, 5]\n",
      "episode 158, reward 337.0, memory_length 2000, epsilon 0.4506816115185697, time 723.0, rides 114\n",
      "Initial State is  [4, 8, 0]\n",
      "episode 159, reward 168.0, memory_length 2000, epsilon 0.4484282034609769, time 729.0, rides 110\n",
      "Initial State is  [1, 4, 5]\n",
      "episode 160, reward 307.0, memory_length 2000, epsilon 0.446186062443672, time 730.0, rides 105\n",
      "Initial State is  [3, 7, 5]\n",
      "episode 161, reward 437.0, memory_length 2000, epsilon 0.4439551321314536, time 730.0, rides 121\n",
      "Initial State is  [2, 16, 2]\n",
      "episode 162, reward 163.0, memory_length 2000, epsilon 0.4417353564707963, time 731.0, rides 132\n",
      "Initial State is  [3, 20, 2]\n",
      "episode 163, reward 710.0, memory_length 2000, epsilon 0.43952667968844233, time 724.0, rides 120\n",
      "Initial State is  [3, 10, 4]\n",
      "episode 164, reward 392.0, memory_length 2000, epsilon 0.43732904629000013, time 733.0, rides 106\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 165, reward 591.0, memory_length 2000, epsilon 0.4351424010585501, time 726.0, rides 121\n",
      "Initial State is  [2, 13, 1]\n",
      "episode 166, reward 206.0, memory_length 2000, epsilon 0.43296668905325736, time 730.0, rides 116\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 167, reward 636.0, memory_length 2000, epsilon 0.43080185560799106, time 731.0, rides 117\n",
      "Initial State is  [0, 15, 0]\n",
      "episode 168, reward 735.0, memory_length 2000, epsilon 0.4286478463299511, time 732.0, rides 129\n",
      "Initial State is  [4, 13, 3]\n",
      "episode 169, reward 597.0, memory_length 2000, epsilon 0.42650460709830135, time 729.0, rides 114\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 170, reward 499.0, memory_length 2000, epsilon 0.42437208406280985, time 731.0, rides 116\n",
      "Initial State is  [0, 0, 3]\n",
      "episode 171, reward 488.0, memory_length 2000, epsilon 0.4222502236424958, time 732.0, rides 111\n",
      "Initial State is  [2, 3, 1]\n",
      "episode 172, reward 697.0, memory_length 2000, epsilon 0.42013897252428334, time 731.0, rides 128\n",
      "Initial State is  [3, 2, 1]\n",
      "episode 173, reward 363.0, memory_length 2000, epsilon 0.4180382776616619, time 731.0, rides 123\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 174, reward 719.0, memory_length 2000, epsilon 0.4159480862733536, time 735.0, rides 133\n",
      "Initial State is  [3, 3, 0]\n",
      "episode 175, reward 453.0, memory_length 2000, epsilon 0.41386834584198684, time 731.0, rides 117\n",
      "Initial State is  [1, 7, 6]\n",
      "episode 176, reward 475.0, memory_length 2000, epsilon 0.4117990041127769, time 723.0, rides 134\n",
      "Initial State is  [3, 3, 1]\n",
      "episode 177, reward 200.0, memory_length 2000, epsilon 0.40974000909221303, time 735.0, rides 117\n",
      "Initial State is  [3, 1, 6]\n",
      "episode 178, reward 184.0, memory_length 2000, epsilon 0.40769130904675194, time 736.0, rides 107\n",
      "Initial State is  [4, 15, 5]\n",
      "episode 179, reward 486.0, memory_length 2000, epsilon 0.40565285250151817, time 733.0, rides 122\n",
      "Initial State is  [3, 22, 1]\n",
      "episode 180, reward 510.0, memory_length 2000, epsilon 0.4036245882390106, time 730.0, rides 129\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 181, reward 741.0, memory_length 2000, epsilon 0.4016064652978155, time 724.0, rides 130\n",
      "Initial State is  [1, 0, 4]\n",
      "episode 182, reward 269.0, memory_length 2000, epsilon 0.3995984329713264, time 729.0, rides 119\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 183, reward 487.0, memory_length 2000, epsilon 0.3976004408064698, time 741.0, rides 126\n",
      "Initial State is  [3, 8, 3]\n",
      "episode 184, reward 396.0, memory_length 2000, epsilon 0.39561243860243744, time 725.0, rides 120\n",
      "Initial State is  [2, 2, 0]\n",
      "episode 185, reward 691.0, memory_length 2000, epsilon 0.3936343764094253, time 735.0, rides 123\n",
      "Initial State is  [0, 13, 4]\n",
      "episode 186, reward 635.0, memory_length 2000, epsilon 0.39166620452737816, time 732.0, rides 115\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 187, reward 785.0, memory_length 2000, epsilon 0.3897078735047413, time 731.0, rides 123\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 188, reward 491.0, memory_length 2000, epsilon 0.3877593341372176, time 735.0, rides 127\n",
      "Initial State is  [2, 14, 2]\n",
      "episode 189, reward 682.0, memory_length 2000, epsilon 0.3858205374665315, time 728.0, rides 106\n",
      "Initial State is  [2, 20, 2]\n",
      "episode 190, reward 687.0, memory_length 2000, epsilon 0.38389143477919885, time 735.0, rides 120\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 191, reward 352.0, memory_length 2000, epsilon 0.3819719776053028, time 739.0, rides 128\n",
      "Initial State is  [4, 21, 0]\n",
      "episode 192, reward 721.0, memory_length 2000, epsilon 0.3800621177172763, time 728.0, rides 112\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 193, reward 436.0, memory_length 2000, epsilon 0.37816180712868996, time 737.0, rides 128\n",
      "Initial State is  [1, 1, 3]\n",
      "episode 194, reward 630.0, memory_length 2000, epsilon 0.37627099809304654, time 727.0, rides 132\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 195, reward 570.0, memory_length 2000, epsilon 0.3743896431025813, time 731.0, rides 112\n",
      "Initial State is  [4, 8, 5]\n",
      "episode 196, reward 358.0, memory_length 2000, epsilon 0.37251769488706843, time 725.0, rides 109\n",
      "Initial State is  [1, 9, 0]\n",
      "episode 197, reward 433.0, memory_length 2000, epsilon 0.3706551064126331, time 728.0, rides 128\n",
      "Initial State is  [0, 22, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 198, reward 415.0, memory_length 2000, epsilon 0.36880183088056995, time 742.0, rides 122\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 199, reward 469.0, memory_length 2000, epsilon 0.3669578217261671, time 731.0, rides 117\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 200, reward 834.0, memory_length 2000, epsilon 0.36512303261753626, time 728.0, rides 125\n",
      "Initial State is  [3, 8, 0]\n",
      "episode 201, reward 567.0, memory_length 2000, epsilon 0.3632974174544486, time 725.0, rides 114\n",
      "Initial State is  [0, 22, 2]\n",
      "episode 202, reward 659.0, memory_length 2000, epsilon 0.3614809303671764, time 734.0, rides 128\n",
      "Initial State is  [3, 23, 3]\n",
      "episode 203, reward 310.0, memory_length 2000, epsilon 0.3596735257153405, time 733.0, rides 115\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 204, reward 444.0, memory_length 2000, epsilon 0.3578751580867638, time 736.0, rides 111\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 205, reward 454.0, memory_length 2000, epsilon 0.35608578229633, time 733.0, rides 122\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 206, reward 519.0, memory_length 2000, epsilon 0.3543053533848483, time 726.0, rides 129\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 207, reward 491.0, memory_length 2000, epsilon 0.35253382661792404, time 722.0, rides 117\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 208, reward 415.0, memory_length 2000, epsilon 0.3507711574848344, time 738.0, rides 121\n",
      "Initial State is  [2, 21, 1]\n",
      "episode 209, reward 601.0, memory_length 2000, epsilon 0.34901730169741024, time 731.0, rides 120\n",
      "Initial State is  [3, 4, 4]\n",
      "episode 210, reward 712.0, memory_length 2000, epsilon 0.3472722151889232, time 733.0, rides 123\n",
      "Initial State is  [1, 21, 3]\n",
      "episode 211, reward 826.0, memory_length 2000, epsilon 0.3455358541129786, time 732.0, rides 127\n",
      "Initial State is  [0, 21, 6]\n",
      "episode 212, reward 325.0, memory_length 2000, epsilon 0.3438081748424137, time 733.0, rides 116\n",
      "Initial State is  [4, 6, 5]\n",
      "episode 213, reward 572.0, memory_length 2000, epsilon 0.3420891339682016, time 730.0, rides 122\n",
      "Initial State is  [1, 0, 4]\n",
      "episode 214, reward 778.0, memory_length 2000, epsilon 0.3403786882983606, time 730.0, rides 132\n",
      "Initial State is  [3, 3, 2]\n",
      "episode 215, reward 809.0, memory_length 2000, epsilon 0.3386767948568688, time 728.0, rides 116\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 216, reward 747.0, memory_length 2000, epsilon 0.33698341088258443, time 722.0, rides 127\n",
      "Initial State is  [4, 22, 5]\n",
      "episode 217, reward 868.0, memory_length 2000, epsilon 0.3352984938281715, time 740.0, rides 129\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 218, reward 700.0, memory_length 2000, epsilon 0.33362200135903064, time 722.0, rides 119\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 219, reward 624.0, memory_length 2000, epsilon 0.33195389135223546, time 733.0, rides 125\n",
      "Initial State is  [3, 12, 1]\n",
      "episode 220, reward 512.0, memory_length 2000, epsilon 0.3302941218954743, time 725.0, rides 131\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 221, reward 423.0, memory_length 2000, epsilon 0.32864265128599696, time 736.0, rides 124\n",
      "Initial State is  [1, 18, 4]\n",
      "episode 222, reward 677.0, memory_length 2000, epsilon 0.326999438029567, time 721.0, rides 122\n",
      "Initial State is  [1, 20, 6]\n",
      "episode 223, reward 513.0, memory_length 2000, epsilon 0.3253644408394192, time 724.0, rides 121\n",
      "Initial State is  [0, 12, 0]\n",
      "episode 224, reward 378.0, memory_length 2000, epsilon 0.3237376186352221, time 732.0, rides 117\n",
      "Initial State is  [0, 21, 4]\n",
      "episode 225, reward 335.0, memory_length 2000, epsilon 0.322118930542046, time 726.0, rides 115\n",
      "Initial State is  [0, 5, 6]\n",
      "episode 226, reward 993.0, memory_length 2000, epsilon 0.32050833588933575, time 730.0, rides 118\n",
      "Initial State is  [3, 10, 2]\n",
      "episode 227, reward 571.0, memory_length 2000, epsilon 0.31890579420988907, time 726.0, rides 120\n",
      "Initial State is  [4, 5, 2]\n",
      "episode 228, reward 628.0, memory_length 2000, epsilon 0.3173112652388396, time 721.0, rides 125\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 229, reward 698.0, memory_length 2000, epsilon 0.3157247089126454, time 730.0, rides 132\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 230, reward 625.0, memory_length 2000, epsilon 0.3141460853680822, time 732.0, rides 118\n",
      "Initial State is  [2, 11, 4]\n",
      "episode 231, reward 797.0, memory_length 2000, epsilon 0.3125753549412418, time 725.0, rides 133\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 232, reward 739.0, memory_length 2000, epsilon 0.31101247816653554, time 728.0, rides 117\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 233, reward 646.0, memory_length 2000, epsilon 0.30945741577570285, time 727.0, rides 130\n",
      "Initial State is  [2, 0, 2]\n",
      "episode 234, reward 565.0, memory_length 2000, epsilon 0.3079101286968243, time 735.0, rides 121\n",
      "Initial State is  [2, 15, 0]\n",
      "episode 235, reward 412.0, memory_length 2000, epsilon 0.3063705780533402, time 722.0, rides 115\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 236, reward 617.0, memory_length 2000, epsilon 0.30483872516307353, time 729.0, rides 114\n",
      "Initial State is  [0, 3, 5]\n",
      "episode 237, reward 827.0, memory_length 2000, epsilon 0.3033145315372582, time 728.0, rides 124\n",
      "Initial State is  [3, 4, 2]\n",
      "episode 238, reward 591.0, memory_length 2000, epsilon 0.3017979588795719, time 737.0, rides 122\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 239, reward 292.0, memory_length 2000, epsilon 0.30028896908517405, time 723.0, rides 137\n",
      "Initial State is  [4, 8, 4]\n",
      "episode 240, reward 645.0, memory_length 2000, epsilon 0.2987875242397482, time 724.0, rides 122\n",
      "Initial State is  [3, 1, 1]\n",
      "episode 241, reward 391.0, memory_length 2000, epsilon 0.29729358661854943, time 732.0, rides 115\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 242, reward 575.0, memory_length 2000, epsilon 0.29580711868545667, time 727.0, rides 122\n",
      "Initial State is  [1, 19, 0]\n",
      "episode 243, reward 817.0, memory_length 2000, epsilon 0.2943280830920294, time 731.0, rides 126\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 244, reward 405.0, memory_length 2000, epsilon 0.29285644267656924, time 731.0, rides 114\n",
      "Initial State is  [1, 4, 6]\n",
      "episode 245, reward 455.0, memory_length 2000, epsilon 0.2913921604631864, time 730.0, rides 117\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 246, reward 569.0, memory_length 2000, epsilon 0.28993519966087045, time 734.0, rides 116\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 247, reward 518.0, memory_length 2000, epsilon 0.2884855236625661, time 722.0, rides 112\n",
      "Initial State is  [0, 18, 5]\n",
      "episode 248, reward 614.0, memory_length 2000, epsilon 0.28704309604425327, time 731.0, rides 125\n",
      "Initial State is  [3, 1, 2]\n",
      "episode 249, reward 713.0, memory_length 2000, epsilon 0.285607880564032, time 731.0, rides 120\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 250, reward 1003.0, memory_length 2000, epsilon 0.28417984116121187, time 726.0, rides 122\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 251, reward 617.0, memory_length 2000, epsilon 0.2827589419554058, time 736.0, rides 115\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 252, reward 842.0, memory_length 2000, epsilon 0.28134514724562876, time 731.0, rides 125\n",
      "Initial State is  [1, 10, 4]\n",
      "episode 253, reward 433.0, memory_length 2000, epsilon 0.2799384215094006, time 729.0, rides 118\n",
      "Initial State is  [4, 3, 0]\n",
      "episode 254, reward 602.0, memory_length 2000, epsilon 0.27853872940185365, time 730.0, rides 119\n",
      "Initial State is  [4, 20, 2]\n",
      "episode 255, reward 839.0, memory_length 2000, epsilon 0.27714603575484437, time 726.0, rides 128\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 256, reward 996.0, memory_length 2000, epsilon 0.2757603055760701, time 725.0, rides 115\n",
      "Initial State is  [4, 19, 6]\n",
      "episode 257, reward 655.0, memory_length 2000, epsilon 0.2743815040481898, time 736.0, rides 116\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 258, reward 486.0, memory_length 2000, epsilon 0.2730095965279488, time 735.0, rides 123\n",
      "Initial State is  [1, 16, 3]\n",
      "episode 259, reward 758.0, memory_length 2000, epsilon 0.27164454854530906, time 724.0, rides 132\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 260, reward 366.0, memory_length 2000, epsilon 0.2702863258025825, time 739.0, rides 116\n",
      "Initial State is  [4, 11, 5]\n",
      "episode 261, reward 515.0, memory_length 2000, epsilon 0.2689348941735696, time 732.0, rides 124\n",
      "Initial State is  [2, 20, 1]\n",
      "episode 262, reward 751.0, memory_length 2000, epsilon 0.26759021970270175, time 725.0, rides 130\n",
      "Initial State is  [3, 20, 4]\n",
      "episode 263, reward 767.0, memory_length 2000, epsilon 0.2662522686041882, time 734.0, rides 117\n",
      "Initial State is  [1, 19, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 264, reward 755.0, memory_length 2000, epsilon 0.2649210072611673, time 733.0, rides 126\n",
      "Initial State is  [2, 20, 3]\n",
      "episode 265, reward 646.0, memory_length 2000, epsilon 0.26359640222486147, time 728.0, rides 114\n",
      "Initial State is  [4, 23, 6]\n",
      "episode 266, reward 719.0, memory_length 2000, epsilon 0.26227842021373715, time 729.0, rides 127\n",
      "Initial State is  [0, 16, 1]\n",
      "episode 267, reward 633.0, memory_length 2000, epsilon 0.2609670281126685, time 730.0, rides 120\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 268, reward 669.0, memory_length 2000, epsilon 0.25966219297210513, time 726.0, rides 121\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 269, reward 926.0, memory_length 2000, epsilon 0.2583638820072446, time 730.0, rides 131\n",
      "Initial State is  [3, 3, 0]\n",
      "episode 270, reward 495.0, memory_length 2000, epsilon 0.2570720625972084, time 723.0, rides 115\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 271, reward 730.0, memory_length 2000, epsilon 0.25578670228422234, time 721.0, rides 118\n",
      "Initial State is  [2, 2, 2]\n",
      "episode 272, reward 556.0, memory_length 2000, epsilon 0.25450776877280124, time 724.0, rides 106\n",
      "Initial State is  [4, 9, 3]\n",
      "episode 273, reward 691.0, memory_length 2000, epsilon 0.2532352299289372, time 729.0, rides 115\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 274, reward 817.0, memory_length 2000, epsilon 0.2519690537792925, time 731.0, rides 127\n",
      "Initial State is  [1, 5, 1]\n",
      "episode 275, reward 493.0, memory_length 2000, epsilon 0.2507092085103961, time 723.0, rides 124\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 276, reward 548.0, memory_length 2000, epsilon 0.2494556624678441, time 723.0, rides 122\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 277, reward 741.0, memory_length 2000, epsilon 0.24820838415550486, time 741.0, rides 126\n",
      "Initial State is  [1, 5, 4]\n",
      "episode 278, reward 555.0, memory_length 2000, epsilon 0.24696734223472733, time 736.0, rides 112\n",
      "Initial State is  [2, 23, 0]\n",
      "episode 279, reward 665.0, memory_length 2000, epsilon 0.2457325055235537, time 735.0, rides 120\n",
      "Initial State is  [1, 17, 5]\n",
      "episode 280, reward 736.0, memory_length 2000, epsilon 0.24450384299593592, time 728.0, rides 123\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 281, reward 813.0, memory_length 2000, epsilon 0.24328132378095624, time 733.0, rides 112\n",
      "Initial State is  [4, 20, 4]\n",
      "episode 282, reward 557.0, memory_length 2000, epsilon 0.24206491716205145, time 737.0, rides 124\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 283, reward 869.0, memory_length 2000, epsilon 0.2408545925762412, time 731.0, rides 121\n",
      "Initial State is  [3, 8, 0]\n",
      "episode 284, reward 728.0, memory_length 2000, epsilon 0.23965031961336, time 725.0, rides 117\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 285, reward 771.0, memory_length 2000, epsilon 0.2384520680152932, time 729.0, rides 124\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 286, reward 864.0, memory_length 2000, epsilon 0.23725980767521673, time 734.0, rides 118\n",
      "Initial State is  [3, 2, 1]\n",
      "episode 287, reward 707.0, memory_length 2000, epsilon 0.23607350863684065, time 730.0, rides 111\n",
      "Initial State is  [0, 23, 4]\n",
      "episode 288, reward 633.0, memory_length 2000, epsilon 0.23489314109365644, time 729.0, rides 126\n",
      "Initial State is  [3, 20, 0]\n",
      "episode 289, reward 541.0, memory_length 2000, epsilon 0.23371867538818816, time 733.0, rides 127\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 290, reward 367.0, memory_length 2000, epsilon 0.23255008201124722, time 732.0, rides 116\n",
      "Initial State is  [4, 20, 6]\n",
      "episode 291, reward 896.0, memory_length 2000, epsilon 0.231387331601191, time 730.0, rides 129\n",
      "Initial State is  [2, 21, 1]\n",
      "episode 292, reward 1054.0, memory_length 2000, epsilon 0.23023039494318503, time 731.0, rides 129\n",
      "Initial State is  [1, 10, 5]\n",
      "episode 293, reward 913.0, memory_length 2000, epsilon 0.2290792429684691, time 730.0, rides 125\n",
      "Initial State is  [1, 23, 5]\n",
      "episode 294, reward 802.0, memory_length 2000, epsilon 0.22793384675362674, time 725.0, rides 124\n",
      "Initial State is  [4, 10, 0]\n",
      "episode 295, reward 702.0, memory_length 2000, epsilon 0.22679417751985861, time 728.0, rides 117\n",
      "Initial State is  [2, 13, 1]\n",
      "episode 296, reward 934.0, memory_length 2000, epsilon 0.22566020663225933, time 734.0, rides 136\n",
      "Initial State is  [2, 4, 3]\n",
      "episode 297, reward 787.0, memory_length 2000, epsilon 0.22453190559909803, time 733.0, rides 133\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 298, reward 895.0, memory_length 2000, epsilon 0.22340924607110255, time 724.0, rides 123\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 299, reward 927.0, memory_length 2000, epsilon 0.22229219984074702, time 732.0, rides 128\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 300, reward 559.0, memory_length 2000, epsilon 0.2211807388415433, time 733.0, rides 118\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 301, reward 692.0, memory_length 2000, epsilon 0.22007483514733558, time 730.0, rides 125\n",
      "Initial State is  [2, 7, 6]\n",
      "episode 302, reward 939.0, memory_length 2000, epsilon 0.2189744609715989, time 731.0, rides 129\n",
      "Initial State is  [2, 23, 3]\n",
      "episode 303, reward 995.0, memory_length 2000, epsilon 0.2178795886667409, time 727.0, rides 119\n",
      "Initial State is  [4, 11, 4]\n",
      "episode 304, reward 895.0, memory_length 2000, epsilon 0.2167901907234072, time 739.0, rides 125\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 305, reward 639.0, memory_length 2000, epsilon 0.21570623976979014, time 727.0, rides 118\n",
      "Initial State is  [1, 18, 3]\n",
      "episode 306, reward 816.0, memory_length 2000, epsilon 0.21462770857094118, time 723.0, rides 121\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 307, reward 1164.0, memory_length 2000, epsilon 0.21355457002808648, time 730.0, rides 126\n",
      "Initial State is  [1, 5, 2]\n",
      "episode 308, reward 821.0, memory_length 2000, epsilon 0.21248679717794605, time 736.0, rides 120\n",
      "Initial State is  [0, 20, 2]\n",
      "episode 309, reward 796.0, memory_length 2000, epsilon 0.21142436319205632, time 721.0, rides 119\n",
      "Initial State is  [1, 8, 6]\n",
      "episode 310, reward 666.0, memory_length 2000, epsilon 0.21036724137609603, time 732.0, rides 127\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 311, reward 513.0, memory_length 2000, epsilon 0.20931540516921554, time 733.0, rides 113\n",
      "Initial State is  [4, 10, 6]\n",
      "episode 312, reward 663.0, memory_length 2000, epsilon 0.20826882814336947, time 727.0, rides 109\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 313, reward 740.0, memory_length 2000, epsilon 0.20722748400265262, time 729.0, rides 129\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 314, reward 473.0, memory_length 2000, epsilon 0.20619134658263935, time 726.0, rides 131\n",
      "Initial State is  [1, 15, 3]\n",
      "episode 315, reward 797.0, memory_length 2000, epsilon 0.20516038984972615, time 732.0, rides 117\n",
      "Initial State is  [3, 12, 3]\n",
      "episode 316, reward 671.0, memory_length 2000, epsilon 0.2041345879004775, time 724.0, rides 123\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 317, reward 1285.0, memory_length 2000, epsilon 0.2031139149609751, time 725.0, rides 127\n",
      "Initial State is  [1, 0, 5]\n",
      "episode 318, reward 946.0, memory_length 2000, epsilon 0.20209834538617025, time 727.0, rides 109\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 319, reward 683.0, memory_length 2000, epsilon 0.2010878536592394, time 726.0, rides 127\n",
      "Initial State is  [0, 12, 6]\n",
      "episode 320, reward 858.0, memory_length 2000, epsilon 0.2000824143909432, time 729.0, rides 115\n",
      "Initial State is  [3, 1, 1]\n",
      "episode 321, reward 810.0, memory_length 2000, epsilon 0.19908200231898848, time 730.0, rides 115\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 322, reward 714.0, memory_length 2000, epsilon 0.19808659230739353, time 725.0, rides 112\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 323, reward 646.0, memory_length 2000, epsilon 0.19709615934585656, time 725.0, rides 117\n",
      "Initial State is  [3, 10, 2]\n",
      "episode 324, reward 715.0, memory_length 2000, epsilon 0.19611067854912728, time 737.0, rides 118\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 325, reward 949.0, memory_length 2000, epsilon 0.19513012515638165, time 721.0, rides 118\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 326, reward 758.0, memory_length 2000, epsilon 0.19415447453059972, time 731.0, rides 123\n",
      "Initial State is  [1, 19, 0]\n",
      "episode 327, reward 621.0, memory_length 2000, epsilon 0.19318370215794672, time 737.0, rides 121\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 328, reward 814.0, memory_length 2000, epsilon 0.192217783647157, time 723.0, rides 117\n",
      "Initial State is  [1, 1, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 329, reward 525.0, memory_length 2000, epsilon 0.1912566947289212, time 735.0, rides 119\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 330, reward 720.0, memory_length 2000, epsilon 0.1903004112552766, time 728.0, rides 113\n",
      "Initial State is  [2, 16, 4]\n",
      "episode 331, reward 1065.0, memory_length 2000, epsilon 0.18934890919900021, time 731.0, rides 121\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 332, reward 809.0, memory_length 2000, epsilon 0.18840216465300522, time 731.0, rides 123\n",
      "Initial State is  [1, 12, 0]\n",
      "episode 333, reward 1185.0, memory_length 2000, epsilon 0.18746015382974018, time 724.0, rides 114\n",
      "Initial State is  [3, 0, 5]\n",
      "episode 334, reward 968.0, memory_length 2000, epsilon 0.1865228530605915, time 733.0, rides 126\n",
      "Initial State is  [3, 3, 2]\n",
      "episode 335, reward 724.0, memory_length 2000, epsilon 0.18559023879528855, time 728.0, rides 121\n",
      "Initial State is  [4, 8, 6]\n",
      "episode 336, reward 450.0, memory_length 2000, epsilon 0.1846622876013121, time 728.0, rides 130\n",
      "Initial State is  [0, 14, 0]\n",
      "episode 337, reward 746.0, memory_length 2000, epsilon 0.18373897616330553, time 723.0, rides 113\n",
      "Initial State is  [1, 19, 5]\n",
      "episode 338, reward 1050.0, memory_length 2000, epsilon 0.182820281282489, time 730.0, rides 124\n",
      "Initial State is  [0, 18, 5]\n",
      "episode 339, reward 581.0, memory_length 2000, epsilon 0.18190617987607657, time 726.0, rides 111\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 340, reward 716.0, memory_length 2000, epsilon 0.18099664897669618, time 725.0, rides 124\n",
      "Initial State is  [3, 8, 3]\n",
      "episode 341, reward 1084.0, memory_length 2000, epsilon 0.1800916657318127, time 726.0, rides 135\n",
      "Initial State is  [2, 12, 4]\n",
      "episode 342, reward 908.0, memory_length 2000, epsilon 0.17919120740315364, time 726.0, rides 113\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 343, reward 1021.0, memory_length 2000, epsilon 0.17829525136613786, time 733.0, rides 133\n",
      "Initial State is  [1, 1, 6]\n",
      "episode 344, reward 897.0, memory_length 2000, epsilon 0.17740377510930716, time 732.0, rides 108\n",
      "Initial State is  [3, 1, 6]\n",
      "episode 345, reward 742.0, memory_length 2000, epsilon 0.17651675623376062, time 724.0, rides 109\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 346, reward 590.0, memory_length 2000, epsilon 0.1756341724525918, time 735.0, rides 120\n",
      "Initial State is  [1, 3, 0]\n",
      "episode 347, reward 715.0, memory_length 2000, epsilon 0.17475600159032884, time 731.0, rides 118\n",
      "Initial State is  [3, 21, 2]\n",
      "episode 348, reward 779.0, memory_length 2000, epsilon 0.17388222158237718, time 723.0, rides 129\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 349, reward 505.0, memory_length 2000, epsilon 0.1730128104744653, time 726.0, rides 113\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 350, reward 676.0, memory_length 2000, epsilon 0.17214774642209296, time 724.0, rides 125\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 351, reward 777.0, memory_length 2000, epsilon 0.1712870076899825, time 725.0, rides 120\n",
      "Initial State is  [4, 7, 1]\n",
      "episode 352, reward 726.0, memory_length 2000, epsilon 0.17043057265153258, time 733.0, rides 114\n",
      "Initial State is  [3, 12, 1]\n",
      "episode 353, reward 714.0, memory_length 2000, epsilon 0.16957841978827493, time 730.0, rides 122\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 354, reward 896.0, memory_length 2000, epsilon 0.16873052768933355, time 724.0, rides 112\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 355, reward 669.0, memory_length 2000, epsilon 0.1678868750508869, time 729.0, rides 117\n",
      "Initial State is  [4, 17, 2]\n",
      "episode 356, reward 652.0, memory_length 2000, epsilon 0.16704744067563246, time 727.0, rides 124\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 357, reward 887.0, memory_length 2000, epsilon 0.1662122034722543, time 727.0, rides 119\n",
      "Initial State is  [3, 15, 6]\n",
      "episode 358, reward 1164.0, memory_length 2000, epsilon 0.16538114245489302, time 730.0, rides 128\n",
      "Initial State is  [1, 14, 6]\n",
      "episode 359, reward 966.0, memory_length 2000, epsilon 0.16455423674261854, time 731.0, rides 122\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 360, reward 641.0, memory_length 2000, epsilon 0.16373146555890544, time 727.0, rides 116\n",
      "Initial State is  [1, 6, 4]\n",
      "episode 361, reward 624.0, memory_length 2000, epsilon 0.16291280823111093, time 733.0, rides 119\n",
      "Initial State is  [3, 20, 1]\n",
      "episode 362, reward 951.0, memory_length 2000, epsilon 0.16209824418995536, time 723.0, rides 127\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 363, reward 911.0, memory_length 2000, epsilon 0.16128775296900558, time 734.0, rides 122\n",
      "Initial State is  [0, 13, 3]\n",
      "episode 364, reward 562.0, memory_length 2000, epsilon 0.16048131420416054, time 731.0, rides 129\n",
      "Initial State is  [1, 9, 6]\n",
      "episode 365, reward 1098.0, memory_length 2000, epsilon 0.15967890763313974, time 734.0, rides 123\n",
      "Initial State is  [2, 11, 2]\n",
      "episode 366, reward 744.0, memory_length 2000, epsilon 0.15888051309497406, time 735.0, rides 114\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 367, reward 753.0, memory_length 2000, epsilon 0.1580861105294992, time 736.0, rides 125\n",
      "Initial State is  [0, 13, 1]\n",
      "episode 368, reward 974.0, memory_length 2000, epsilon 0.1572956799768517, time 741.0, rides 115\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 369, reward 728.0, memory_length 2000, epsilon 0.15650920157696743, time 726.0, rides 114\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 370, reward 522.0, memory_length 2000, epsilon 0.1557266555690826, time 725.0, rides 115\n",
      "Initial State is  [2, 22, 5]\n",
      "episode 371, reward 636.0, memory_length 2000, epsilon 0.1549480222912372, time 726.0, rides 124\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 372, reward 925.0, memory_length 2000, epsilon 0.15417328217978102, time 727.0, rides 120\n",
      "Initial State is  [1, 12, 4]\n",
      "episode 373, reward 899.0, memory_length 2000, epsilon 0.1534024157688821, time 735.0, rides 129\n",
      "Initial State is  [4, 9, 3]\n",
      "episode 374, reward 718.0, memory_length 2000, epsilon 0.1526354036900377, time 733.0, rides 121\n",
      "Initial State is  [1, 3, 0]\n",
      "episode 375, reward 817.0, memory_length 2000, epsilon 0.1518722266715875, time 729.0, rides 118\n",
      "Initial State is  [2, 6, 6]\n",
      "episode 376, reward 1104.0, memory_length 2000, epsilon 0.15111286553822956, time 733.0, rides 121\n",
      "Initial State is  [0, 4, 5]\n",
      "episode 377, reward 1016.0, memory_length 2000, epsilon 0.15035730121053842, time 732.0, rides 129\n",
      "Initial State is  [3, 19, 0]\n",
      "episode 378, reward 918.0, memory_length 2000, epsilon 0.14960551470448571, time 730.0, rides 125\n",
      "Initial State is  [1, 1, 3]\n",
      "episode 379, reward 829.0, memory_length 2000, epsilon 0.14885748713096328, time 736.0, rides 113\n",
      "Initial State is  [0, 16, 2]\n",
      "episode 380, reward 743.0, memory_length 2000, epsilon 0.14811319969530845, time 732.0, rides 120\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 381, reward 835.0, memory_length 2000, epsilon 0.1473726336968319, time 732.0, rides 107\n",
      "Initial State is  [1, 20, 5]\n",
      "episode 382, reward 785.0, memory_length 2000, epsilon 0.14663577052834775, time 735.0, rides 120\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 383, reward 576.0, memory_length 2000, epsilon 0.14590259167570602, time 728.0, rides 127\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 384, reward 705.0, memory_length 2000, epsilon 0.1451730787173275, time 722.0, rides 117\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 385, reward 788.0, memory_length 2000, epsilon 0.14444721332374086, time 723.0, rides 117\n",
      "Initial State is  [4, 12, 0]\n",
      "episode 386, reward 556.0, memory_length 2000, epsilon 0.14372497725712216, time 733.0, rides 122\n",
      "Initial State is  [2, 0, 4]\n",
      "episode 387, reward 818.0, memory_length 2000, epsilon 0.14300635237083656, time 729.0, rides 122\n",
      "Initial State is  [2, 16, 3]\n",
      "episode 388, reward 1014.0, memory_length 2000, epsilon 0.14229132060898236, time 727.0, rides 117\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 389, reward 682.0, memory_length 2000, epsilon 0.14157986400593744, time 730.0, rides 118\n",
      "Initial State is  [0, 19, 4]\n",
      "episode 390, reward 726.0, memory_length 2000, epsilon 0.14087196468590776, time 743.0, rides 131\n",
      "Initial State is  [4, 11, 5]\n",
      "episode 391, reward 860.0, memory_length 2000, epsilon 0.14016760486247823, time 738.0, rides 118\n",
      "Initial State is  [4, 15, 6]\n",
      "episode 392, reward 1104.0, memory_length 2000, epsilon 0.13946676683816583, time 728.0, rides 121\n",
      "Initial State is  [3, 11, 4]\n",
      "episode 393, reward 1127.0, memory_length 2000, epsilon 0.138769433003975, time 724.0, rides 135\n",
      "Initial State is  [3, 1, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 394, reward 790.0, memory_length 2000, epsilon 0.13807558583895513, time 733.0, rides 125\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 395, reward 975.0, memory_length 2000, epsilon 0.13738520790976036, time 728.0, rides 123\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 396, reward 714.0, memory_length 2000, epsilon 0.13669828187021155, time 729.0, rides 113\n",
      "Initial State is  [0, 3, 1]\n",
      "episode 397, reward 984.0, memory_length 2000, epsilon 0.13601479046086049, time 730.0, rides 124\n",
      "Initial State is  [4, 13, 1]\n",
      "episode 398, reward 870.0, memory_length 2000, epsilon 0.1353347165085562, time 732.0, rides 127\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 399, reward 991.0, memory_length 2000, epsilon 0.1346580429260134, time 728.0, rides 125\n",
      "Initial State is  [3, 0, 3]\n",
      "episode 400, reward 764.0, memory_length 2000, epsilon 0.13398475271138335, time 726.0, rides 118\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 401, reward 947.0, memory_length 2000, epsilon 0.13331482894782642, time 734.0, rides 125\n",
      "Initial State is  [1, 17, 5]\n",
      "episode 402, reward 745.0, memory_length 2000, epsilon 0.13264825480308728, time 727.0, rides 107\n",
      "Initial State is  [3, 22, 6]\n",
      "episode 403, reward 846.0, memory_length 2000, epsilon 0.13198501352907185, time 722.0, rides 117\n",
      "Initial State is  [3, 18, 4]\n",
      "episode 404, reward 734.0, memory_length 2000, epsilon 0.1313250884614265, time 732.0, rides 109\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 405, reward 999.0, memory_length 2000, epsilon 0.13066846301911936, time 722.0, rides 123\n",
      "Initial State is  [2, 14, 6]\n",
      "episode 406, reward 1121.0, memory_length 2000, epsilon 0.13001512070402377, time 732.0, rides 127\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 407, reward 805.0, memory_length 2000, epsilon 0.12936504510050365, time 729.0, rides 122\n",
      "Initial State is  [2, 16, 2]\n",
      "episode 408, reward 963.0, memory_length 2000, epsilon 0.12871821987500112, time 723.0, rides 116\n",
      "Initial State is  [1, 0, 1]\n",
      "episode 409, reward 542.0, memory_length 2000, epsilon 0.12807462877562611, time 722.0, rides 116\n",
      "Initial State is  [2, 14, 2]\n",
      "episode 410, reward 1077.0, memory_length 2000, epsilon 0.12743425563174798, time 735.0, rides 122\n",
      "Initial State is  [2, 8, 6]\n",
      "episode 411, reward 678.0, memory_length 2000, epsilon 0.12679708435358925, time 723.0, rides 127\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 412, reward 874.0, memory_length 2000, epsilon 0.1261630989318213, time 728.0, rides 112\n",
      "Initial State is  [1, 10, 0]\n",
      "episode 413, reward 1158.0, memory_length 2000, epsilon 0.1255322834371622, time 732.0, rides 110\n",
      "Initial State is  [2, 23, 2]\n",
      "episode 414, reward 799.0, memory_length 2000, epsilon 0.12490462201997637, time 726.0, rides 110\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 415, reward 1153.0, memory_length 2000, epsilon 0.1242800989098765, time 733.0, rides 121\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 416, reward 995.0, memory_length 2000, epsilon 0.12365869841532712, time 735.0, rides 126\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 417, reward 901.0, memory_length 2000, epsilon 0.12304040492325048, time 728.0, rides 116\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 418, reward 841.0, memory_length 2000, epsilon 0.12242520289863423, time 730.0, rides 111\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 419, reward 893.0, memory_length 2000, epsilon 0.12181307688414106, time 736.0, rides 120\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 420, reward 671.0, memory_length 2000, epsilon 0.12120401149972035, time 728.0, rides 115\n",
      "Initial State is  [4, 4, 2]\n",
      "episode 421, reward 1080.0, memory_length 2000, epsilon 0.12059799144222175, time 736.0, rides 118\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 422, reward 1042.0, memory_length 2000, epsilon 0.11999500148501063, time 733.0, rides 120\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 423, reward 888.0, memory_length 2000, epsilon 0.11939502647758558, time 728.0, rides 121\n",
      "Initial State is  [2, 3, 6]\n",
      "episode 424, reward 535.0, memory_length 2000, epsilon 0.11879805134519765, time 727.0, rides 114\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 425, reward 661.0, memory_length 2000, epsilon 0.11820406108847166, time 734.0, rides 119\n",
      "Initial State is  [3, 6, 3]\n",
      "episode 426, reward 943.0, memory_length 2000, epsilon 0.1176130407830293, time 732.0, rides 121\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 427, reward 943.0, memory_length 2000, epsilon 0.11702497557911415, time 729.0, rides 123\n",
      "Initial State is  [2, 11, 4]\n",
      "episode 428, reward 949.0, memory_length 2000, epsilon 0.11643985070121858, time 732.0, rides 109\n",
      "Initial State is  [4, 0, 4]\n",
      "episode 429, reward 772.0, memory_length 2000, epsilon 0.11585765144771248, time 732.0, rides 126\n",
      "Initial State is  [1, 4, 1]\n",
      "episode 430, reward 943.0, memory_length 2000, epsilon 0.11527836319047392, time 737.0, rides 126\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 431, reward 851.0, memory_length 2000, epsilon 0.11470197137452155, time 737.0, rides 133\n",
      "Initial State is  [3, 7, 2]\n",
      "episode 432, reward 739.0, memory_length 2000, epsilon 0.11412846151764894, time 733.0, rides 125\n",
      "Initial State is  [0, 6, 3]\n",
      "episode 433, reward 825.0, memory_length 2000, epsilon 0.1135578192100607, time 732.0, rides 122\n",
      "Initial State is  [4, 10, 0]\n",
      "episode 434, reward 680.0, memory_length 2000, epsilon 0.11299003011401039, time 729.0, rides 119\n",
      "Initial State is  [4, 15, 1]\n",
      "episode 435, reward 812.0, memory_length 2000, epsilon 0.11242507996344034, time 730.0, rides 121\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 436, reward 1000.0, memory_length 2000, epsilon 0.11186295456362313, time 729.0, rides 134\n",
      "Initial State is  [4, 20, 3]\n",
      "episode 437, reward 854.0, memory_length 2000, epsilon 0.11130363979080501, time 723.0, rides 117\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 438, reward 658.0, memory_length 2000, epsilon 0.11074712159185099, time 732.0, rides 109\n",
      "Initial State is  [1, 16, 1]\n",
      "episode 439, reward 996.0, memory_length 2000, epsilon 0.11019338598389174, time 725.0, rides 128\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 440, reward 889.0, memory_length 2000, epsilon 0.10964241905397228, time 725.0, rides 110\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 441, reward 1086.0, memory_length 2000, epsilon 0.10909420695870241, time 723.0, rides 118\n",
      "Initial State is  [0, 18, 0]\n",
      "episode 442, reward 889.0, memory_length 2000, epsilon 0.1085487359239089, time 727.0, rides 105\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 443, reward 822.0, memory_length 2000, epsilon 0.10800599224428936, time 746.0, rides 123\n",
      "Initial State is  [1, 2, 2]\n",
      "episode 444, reward 620.0, memory_length 2000, epsilon 0.10746596228306791, time 727.0, rides 117\n",
      "Initial State is  [2, 6, 1]\n",
      "episode 445, reward 788.0, memory_length 2000, epsilon 0.10692863247165257, time 729.0, rides 112\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 446, reward 809.0, memory_length 2000, epsilon 0.1063939893092943, time 728.0, rides 117\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 447, reward 699.0, memory_length 2000, epsilon 0.10586201936274783, time 737.0, rides 116\n",
      "Initial State is  [1, 10, 4]\n",
      "episode 448, reward 685.0, memory_length 2000, epsilon 0.10533270926593409, time 728.0, rides 114\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 449, reward 918.0, memory_length 2000, epsilon 0.10480604571960442, time 726.0, rides 119\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 450, reward 942.0, memory_length 2000, epsilon 0.1042820154910064, time 725.0, rides 124\n",
      "Initial State is  [2, 6, 6]\n",
      "episode 451, reward 855.0, memory_length 2000, epsilon 0.10376060541355137, time 722.0, rides 108\n",
      "Initial State is  [0, 19, 3]\n",
      "episode 452, reward 704.0, memory_length 2000, epsilon 0.1032418023864836, time 736.0, rides 121\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 453, reward 1141.0, memory_length 2000, epsilon 0.10272559337455119, time 723.0, rides 118\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 454, reward 864.0, memory_length 2000, epsilon 0.10221196540767843, time 730.0, rides 129\n",
      "Initial State is  [3, 22, 3]\n",
      "episode 455, reward 693.0, memory_length 2000, epsilon 0.10170090558064004, time 734.0, rides 121\n",
      "Initial State is  [3, 2, 4]\n",
      "episode 456, reward 1100.0, memory_length 2000, epsilon 0.10119240105273684, time 727.0, rides 119\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 457, reward 628.0, memory_length 2000, epsilon 0.10068643904747315, time 729.0, rides 124\n",
      "Initial State is  [3, 12, 4]\n",
      "episode 458, reward 827.0, memory_length 2000, epsilon 0.10018300685223579, time 725.0, rides 133\n",
      "Initial State is  [0, 20, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 459, reward 912.0, memory_length 2000, epsilon 0.0996820918179746, time 730.0, rides 115\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 460, reward 1052.0, memory_length 2000, epsilon 0.09918368135888474, time 732.0, rides 128\n",
      "Initial State is  [2, 14, 6]\n",
      "episode 461, reward 888.0, memory_length 2000, epsilon 0.09868776295209031, time 725.0, rides 115\n",
      "Initial State is  [0, 19, 3]\n",
      "episode 462, reward 821.0, memory_length 2000, epsilon 0.09819432413732986, time 731.0, rides 117\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 463, reward 920.0, memory_length 2000, epsilon 0.09770335251664321, time 732.0, rides 114\n",
      "Initial State is  [1, 12, 4]\n",
      "episode 464, reward 1039.0, memory_length 2000, epsilon 0.09721483575406, time 730.0, rides 115\n",
      "Initial State is  [2, 23, 0]\n",
      "episode 465, reward 1064.0, memory_length 2000, epsilon 0.09672876157528969, time 732.0, rides 122\n",
      "Initial State is  [2, 0, 0]\n",
      "episode 466, reward 1002.0, memory_length 2000, epsilon 0.09624511776741324, time 734.0, rides 122\n",
      "Initial State is  [0, 20, 0]\n",
      "episode 467, reward 997.0, memory_length 2000, epsilon 0.09576389217857617, time 727.0, rides 119\n",
      "Initial State is  [1, 6, 0]\n",
      "episode 468, reward 1172.0, memory_length 2000, epsilon 0.09528507271768329, time 731.0, rides 122\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 469, reward 826.0, memory_length 2000, epsilon 0.09480864735409487, time 729.0, rides 118\n",
      "Initial State is  [4, 8, 0]\n",
      "episode 470, reward 1060.0, memory_length 2000, epsilon 0.0943346041173244, time 730.0, rides 120\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 471, reward 618.0, memory_length 2000, epsilon 0.09386293109673778, time 726.0, rides 110\n",
      "Initial State is  [2, 4, 1]\n",
      "episode 472, reward 1015.0, memory_length 2000, epsilon 0.09339361644125409, time 731.0, rides 123\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 473, reward 1003.0, memory_length 2000, epsilon 0.09292664835904782, time 731.0, rides 112\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 474, reward 1000.0, memory_length 2000, epsilon 0.09246201511725258, time 729.0, rides 126\n",
      "Initial State is  [4, 9, 5]\n",
      "episode 475, reward 766.0, memory_length 2000, epsilon 0.09199970504166631, time 739.0, rides 120\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 476, reward 999.0, memory_length 2000, epsilon 0.09153970651645797, time 732.0, rides 114\n",
      "Initial State is  [2, 11, 0]\n",
      "episode 477, reward 816.0, memory_length 2000, epsilon 0.09108200798387568, time 733.0, rides 115\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 478, reward 919.0, memory_length 2000, epsilon 0.0906265979439563, time 728.0, rides 113\n",
      "Initial State is  [0, 2, 6]\n",
      "episode 479, reward 1379.0, memory_length 2000, epsilon 0.09017346495423652, time 729.0, rides 125\n",
      "Initial State is  [1, 6, 4]\n",
      "episode 480, reward 1078.0, memory_length 2000, epsilon 0.08972259762946533, time 725.0, rides 131\n",
      "Initial State is  [1, 17, 0]\n",
      "episode 481, reward 969.0, memory_length 2000, epsilon 0.089273984641318, time 725.0, rides 113\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 482, reward 925.0, memory_length 2000, epsilon 0.0888276147181114, time 730.0, rides 118\n",
      "Initial State is  [3, 9, 3]\n",
      "episode 483, reward 1086.0, memory_length 2000, epsilon 0.08838347664452084, time 728.0, rides 129\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 484, reward 677.0, memory_length 2000, epsilon 0.08794155926129824, time 728.0, rides 115\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 485, reward 1185.0, memory_length 2000, epsilon 0.08750185146499175, time 732.0, rides 125\n",
      "Initial State is  [2, 0, 4]\n",
      "episode 486, reward 991.0, memory_length 2000, epsilon 0.08706434220766679, time 729.0, rides 109\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 487, reward 1225.0, memory_length 2000, epsilon 0.08662902049662846, time 740.0, rides 127\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 488, reward 1041.0, memory_length 2000, epsilon 0.08619587539414532, time 738.0, rides 114\n",
      "Initial State is  [4, 5, 3]\n",
      "episode 489, reward 746.0, memory_length 2000, epsilon 0.08576489601717459, time 722.0, rides 127\n",
      "Initial State is  [4, 18, 2]\n",
      "episode 490, reward 998.0, memory_length 2000, epsilon 0.08533607153708872, time 738.0, rides 128\n",
      "Initial State is  [2, 18, 5]\n",
      "episode 491, reward 938.0, memory_length 2000, epsilon 0.08490939117940327, time 730.0, rides 119\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 492, reward 638.0, memory_length 2000, epsilon 0.08448484422350626, time 735.0, rides 115\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 493, reward 1283.0, memory_length 2000, epsilon 0.08406242000238873, time 728.0, rides 116\n",
      "Initial State is  [2, 19, 4]\n",
      "episode 494, reward 798.0, memory_length 2000, epsilon 0.08364210790237678, time 725.0, rides 109\n",
      "Initial State is  [1, 6, 1]\n",
      "episode 495, reward 1155.0, memory_length 2000, epsilon 0.0832238973628649, time 734.0, rides 120\n",
      "Initial State is  [3, 11, 3]\n",
      "episode 496, reward 946.0, memory_length 2000, epsilon 0.08280777787605056, time 730.0, rides 114\n",
      "Initial State is  [2, 14, 5]\n",
      "episode 497, reward 1027.0, memory_length 2000, epsilon 0.08239373898667031, time 740.0, rides 119\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 498, reward 797.0, memory_length 2000, epsilon 0.08198177029173696, time 728.0, rides 123\n",
      "Initial State is  [1, 18, 4]\n",
      "episode 499, reward 959.0, memory_length 2000, epsilon 0.08157186144027828, time 729.0, rides 122\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 500, reward 904.0, memory_length 2000, epsilon 0.0811640021330769, time 739.0, rides 114\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 501, reward 810.0, memory_length 2000, epsilon 0.08075818212241151, time 729.0, rides 114\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 502, reward 877.0, memory_length 2000, epsilon 0.08035439121179945, time 726.0, rides 115\n",
      "Initial State is  [3, 10, 6]\n",
      "episode 503, reward 930.0, memory_length 2000, epsilon 0.07995261925574046, time 735.0, rides 115\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 504, reward 1026.0, memory_length 2000, epsilon 0.07955285615946175, time 722.0, rides 120\n",
      "Initial State is  [2, 12, 2]\n",
      "episode 505, reward 1236.0, memory_length 2000, epsilon 0.07915509187866444, time 729.0, rides 119\n",
      "Initial State is  [0, 19, 6]\n",
      "episode 506, reward 745.0, memory_length 2000, epsilon 0.07875931641927113, time 727.0, rides 116\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 507, reward 1237.0, memory_length 2000, epsilon 0.07836551983717477, time 728.0, rides 122\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 508, reward 1388.0, memory_length 2000, epsilon 0.07797369223798889, time 726.0, rides 118\n",
      "Initial State is  [3, 5, 6]\n",
      "episode 509, reward 1093.0, memory_length 2000, epsilon 0.07758382377679894, time 733.0, rides 124\n",
      "Initial State is  [0, 14, 0]\n",
      "episode 510, reward 1213.0, memory_length 2000, epsilon 0.07719590465791494, time 739.0, rides 114\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 511, reward 1223.0, memory_length 2000, epsilon 0.07680992513462537, time 740.0, rides 122\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 512, reward 1062.0, memory_length 2000, epsilon 0.07642587550895225, time 731.0, rides 127\n",
      "Initial State is  [2, 13, 2]\n",
      "episode 513, reward 1047.0, memory_length 2000, epsilon 0.07604374613140748, time 732.0, rides 122\n",
      "Initial State is  [1, 1, 3]\n",
      "episode 514, reward 1041.0, memory_length 2000, epsilon 0.07566352740075044, time 733.0, rides 120\n",
      "Initial State is  [2, 14, 1]\n",
      "episode 515, reward 1147.0, memory_length 2000, epsilon 0.07528520976374668, time 730.0, rides 119\n",
      "Initial State is  [1, 13, 6]\n",
      "episode 516, reward 783.0, memory_length 2000, epsilon 0.07490878371492794, time 727.0, rides 112\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 517, reward 1402.0, memory_length 2000, epsilon 0.0745342397963533, time 726.0, rides 118\n",
      "Initial State is  [4, 0, 0]\n",
      "episode 518, reward 1173.0, memory_length 2000, epsilon 0.07416156859737154, time 727.0, rides 119\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 519, reward 822.0, memory_length 2000, epsilon 0.07379076075438468, time 729.0, rides 113\n",
      "Initial State is  [4, 16, 1]\n",
      "episode 520, reward 592.0, memory_length 2000, epsilon 0.07342180695061275, time 733.0, rides 116\n",
      "Initial State is  [4, 1, 6]\n",
      "episode 521, reward 1026.0, memory_length 2000, epsilon 0.07305469791585968, time 732.0, rides 119\n",
      "Initial State is  [4, 22, 3]\n",
      "episode 522, reward 1134.0, memory_length 2000, epsilon 0.07268942442628039, time 729.0, rides 112\n",
      "Initial State is  [0, 6, 2]\n",
      "episode 523, reward 1194.0, memory_length 2000, epsilon 0.07232597730414898, time 730.0, rides 127\n",
      "Initial State is  [0, 20, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 524, reward 929.0, memory_length 2000, epsilon 0.07196434741762824, time 732.0, rides 132\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 525, reward 1056.0, memory_length 2000, epsilon 0.0716045256805401, time 732.0, rides 114\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 526, reward 838.0, memory_length 2000, epsilon 0.0712465030521374, time 729.0, rides 110\n",
      "Initial State is  [4, 3, 6]\n",
      "episode 527, reward 772.0, memory_length 2000, epsilon 0.0708902705368767, time 731.0, rides 115\n",
      "Initial State is  [2, 7, 6]\n",
      "episode 528, reward 1172.0, memory_length 2000, epsilon 0.07053581918419231, time 736.0, rides 117\n",
      "Initial State is  [3, 12, 2]\n",
      "episode 529, reward 1185.0, memory_length 2000, epsilon 0.07018314008827135, time 726.0, rides 120\n",
      "Initial State is  [1, 15, 0]\n",
      "episode 530, reward 969.0, memory_length 2000, epsilon 0.06983222438783, time 727.0, rides 124\n",
      "Initial State is  [3, 22, 3]\n",
      "episode 531, reward 879.0, memory_length 2000, epsilon 0.06948306326589085, time 726.0, rides 116\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 532, reward 724.0, memory_length 2000, epsilon 0.0691356479495614, time 728.0, rides 104\n",
      "Initial State is  [0, 23, 2]\n",
      "episode 533, reward 979.0, memory_length 2000, epsilon 0.06878996970981359, time 730.0, rides 112\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 534, reward 1072.0, memory_length 2000, epsilon 0.06844601986126451, time 726.0, rides 125\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 535, reward 1082.0, memory_length 2000, epsilon 0.06810378976195819, time 736.0, rides 116\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 536, reward 824.0, memory_length 2000, epsilon 0.0677632708131484, time 736.0, rides 118\n",
      "Initial State is  [2, 11, 6]\n",
      "episode 537, reward 814.0, memory_length 2000, epsilon 0.06742445445908266, time 729.0, rides 117\n",
      "Initial State is  [4, 11, 0]\n",
      "episode 538, reward 1237.0, memory_length 2000, epsilon 0.06708733218678724, time 729.0, rides 118\n",
      "Initial State is  [4, 9, 3]\n",
      "episode 539, reward 1005.0, memory_length 2000, epsilon 0.0667518955258533, time 728.0, rides 127\n",
      "Initial State is  [0, 23, 0]\n",
      "episode 540, reward 861.0, memory_length 2000, epsilon 0.06641813604822402, time 729.0, rides 123\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 541, reward 1120.0, memory_length 2000, epsilon 0.0660860453679829, time 728.0, rides 127\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 542, reward 864.0, memory_length 2000, epsilon 0.06575561514114299, time 728.0, rides 122\n",
      "Initial State is  [0, 21, 2]\n",
      "episode 543, reward 808.0, memory_length 2000, epsilon 0.06542683706543727, time 726.0, rides 117\n",
      "Initial State is  [1, 17, 1]\n",
      "episode 544, reward 1299.0, memory_length 2000, epsilon 0.06509970288011008, time 731.0, rides 122\n",
      "Initial State is  [4, 7, 0]\n",
      "episode 545, reward 1053.0, memory_length 2000, epsilon 0.06477420436570952, time 729.0, rides 114\n",
      "Initial State is  [3, 10, 4]\n",
      "episode 546, reward 897.0, memory_length 2000, epsilon 0.06445033334388098, time 732.0, rides 108\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 547, reward 729.0, memory_length 2000, epsilon 0.06412808167716157, time 731.0, rides 117\n",
      "Initial State is  [0, 17, 5]\n",
      "episode 548, reward 931.0, memory_length 2000, epsilon 0.06380744126877576, time 727.0, rides 115\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 549, reward 1175.0, memory_length 2000, epsilon 0.06348840406243188, time 724.0, rides 122\n",
      "Initial State is  [1, 0, 5]\n",
      "episode 550, reward 1142.0, memory_length 2000, epsilon 0.06317096204211972, time 730.0, rides 122\n",
      "Initial State is  [1, 20, 5]\n",
      "episode 551, reward 722.0, memory_length 2000, epsilon 0.06285510723190912, time 733.0, rides 117\n",
      "Initial State is  [1, 2, 0]\n",
      "episode 552, reward 695.0, memory_length 2000, epsilon 0.06254083169574957, time 733.0, rides 111\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 553, reward 871.0, memory_length 2000, epsilon 0.062228127537270826, time 725.0, rides 117\n",
      "Initial State is  [1, 15, 4]\n",
      "episode 554, reward 1170.0, memory_length 2000, epsilon 0.06191698689958447, time 731.0, rides 110\n",
      "Initial State is  [3, 7, 3]\n",
      "episode 555, reward 742.0, memory_length 2000, epsilon 0.061607401965086545, time 729.0, rides 113\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 556, reward 1002.0, memory_length 2000, epsilon 0.06129936495526111, time 742.0, rides 129\n",
      "Initial State is  [4, 18, 5]\n",
      "episode 557, reward 973.0, memory_length 2000, epsilon 0.0609928681304848, time 728.0, rides 117\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 558, reward 741.0, memory_length 2000, epsilon 0.060687903789832374, time 724.0, rides 117\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 559, reward 1133.0, memory_length 2000, epsilon 0.06038446427088321, time 727.0, rides 131\n",
      "Initial State is  [1, 0, 3]\n",
      "episode 560, reward 970.0, memory_length 2000, epsilon 0.06008254194952879, time 734.0, rides 116\n",
      "Initial State is  [4, 6, 2]\n",
      "episode 561, reward 1197.0, memory_length 2000, epsilon 0.05978212923978115, time 727.0, rides 126\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 562, reward 1009.0, memory_length 2000, epsilon 0.05948321859358224, time 729.0, rides 126\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 563, reward 978.0, memory_length 2000, epsilon 0.05918580250061433, time 741.0, rides 116\n",
      "Initial State is  [4, 21, 3]\n",
      "episode 564, reward 964.0, memory_length 2000, epsilon 0.058889873488111255, time 736.0, rides 112\n",
      "Initial State is  [1, 5, 3]\n",
      "episode 565, reward 897.0, memory_length 2000, epsilon 0.058595424120670696, time 734.0, rides 115\n",
      "Initial State is  [0, 10, 5]\n",
      "episode 566, reward 1156.0, memory_length 2000, epsilon 0.05830244700006734, time 727.0, rides 116\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 567, reward 865.0, memory_length 2000, epsilon 0.058010934765067, time 728.0, rides 118\n",
      "Initial State is  [2, 7, 2]\n",
      "episode 568, reward 771.0, memory_length 2000, epsilon 0.05772088009124167, time 734.0, rides 122\n",
      "Initial State is  [2, 12, 5]\n",
      "episode 569, reward 1199.0, memory_length 2000, epsilon 0.05743227569078546, time 732.0, rides 125\n",
      "Initial State is  [4, 21, 6]\n",
      "episode 570, reward 1167.0, memory_length 2000, epsilon 0.05714511431233153, time 726.0, rides 122\n",
      "Initial State is  [2, 4, 4]\n",
      "episode 571, reward 637.0, memory_length 2000, epsilon 0.05685938874076987, time 722.0, rides 113\n",
      "Initial State is  [4, 5, 1]\n",
      "episode 572, reward 589.0, memory_length 2000, epsilon 0.056575091797066025, time 721.0, rides 117\n",
      "Initial State is  [0, 20, 2]\n",
      "episode 573, reward 1108.0, memory_length 2000, epsilon 0.056292216338080694, time 724.0, rides 107\n",
      "Initial State is  [4, 22, 5]\n",
      "episode 574, reward 898.0, memory_length 2000, epsilon 0.05601075525639029, time 740.0, rides 119\n",
      "Initial State is  [2, 8, 6]\n",
      "episode 575, reward 990.0, memory_length 2000, epsilon 0.05573070148010834, time 732.0, rides 118\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 576, reward 723.0, memory_length 2000, epsilon 0.0554520479727078, time 728.0, rides 120\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 577, reward 906.0, memory_length 2000, epsilon 0.05517478773284426, time 727.0, rides 125\n",
      "Initial State is  [4, 5, 2]\n",
      "episode 578, reward 958.0, memory_length 2000, epsilon 0.05489891379418004, time 736.0, rides 120\n",
      "Initial State is  [2, 4, 3]\n",
      "episode 579, reward 1076.0, memory_length 2000, epsilon 0.05462441922520914, time 729.0, rides 118\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 580, reward 1125.0, memory_length 2000, epsilon 0.0543512971290831, time 729.0, rides 111\n",
      "Initial State is  [2, 9, 1]\n",
      "episode 581, reward 983.0, memory_length 2000, epsilon 0.05407954064343768, time 738.0, rides 113\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 582, reward 570.0, memory_length 2000, epsilon 0.05380914294022049, time 727.0, rides 104\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 583, reward 994.0, memory_length 2000, epsilon 0.05354009722551939, time 743.0, rides 119\n",
      "Initial State is  [2, 8, 4]\n",
      "episode 584, reward 845.0, memory_length 2000, epsilon 0.05327239673939179, time 730.0, rides 116\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 585, reward 1089.0, memory_length 2000, epsilon 0.053006034755694834, time 728.0, rides 124\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 586, reward 1158.0, memory_length 2000, epsilon 0.052741004581916356, time 737.0, rides 121\n",
      "Initial State is  [4, 18, 5]\n",
      "episode 587, reward 1121.0, memory_length 2000, epsilon 0.052477299559006776, time 732.0, rides 116\n",
      "Initial State is  [2, 4, 1]\n",
      "episode 588, reward 1218.0, memory_length 2000, epsilon 0.052214913061211746, time 735.0, rides 122\n",
      "Initial State is  [0, 23, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 589, reward 1051.0, memory_length 2000, epsilon 0.05195383849590569, time 737.0, rides 114\n",
      "Initial State is  [1, 11, 6]\n",
      "episode 590, reward 1053.0, memory_length 2000, epsilon 0.05169406930342616, time 731.0, rides 116\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 591, reward 856.0, memory_length 2000, epsilon 0.05143559895690903, time 734.0, rides 125\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 592, reward 1117.0, memory_length 2000, epsilon 0.051178420962124486, time 725.0, rides 131\n",
      "Initial State is  [3, 22, 1]\n",
      "episode 593, reward 1140.0, memory_length 2000, epsilon 0.05092252885731386, time 725.0, rides 122\n",
      "Initial State is  [2, 20, 0]\n",
      "episode 594, reward 880.0, memory_length 2000, epsilon 0.05066791621302729, time 722.0, rides 125\n",
      "Initial State is  [1, 3, 3]\n",
      "episode 595, reward 1021.0, memory_length 2000, epsilon 0.05041457663196215, time 724.0, rides 123\n",
      "Initial State is  [1, 19, 5]\n",
      "episode 596, reward 1089.0, memory_length 2000, epsilon 0.050162503748802344, time 726.0, rides 118\n",
      "Initial State is  [4, 1, 2]\n",
      "episode 597, reward 958.0, memory_length 2000, epsilon 0.049911691230058335, time 736.0, rides 115\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 598, reward 1030.0, memory_length 2000, epsilon 0.04966213277390804, time 725.0, rides 119\n",
      "Initial State is  [4, 20, 4]\n",
      "episode 599, reward 1027.0, memory_length 2000, epsilon 0.0494138221100385, time 728.0, rides 110\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 600, reward 1101.0, memory_length 2000, epsilon 0.04916675299948831, time 732.0, rides 112\n",
      "Initial State is  [2, 12, 2]\n",
      "episode 601, reward 842.0, memory_length 2000, epsilon 0.04892091923449087, time 739.0, rides 118\n",
      "Initial State is  [2, 13, 3]\n",
      "episode 602, reward 853.0, memory_length 2000, epsilon 0.04867631463831842, time 737.0, rides 115\n",
      "Initial State is  [2, 19, 0]\n",
      "episode 603, reward 1205.0, memory_length 2000, epsilon 0.048432933065126825, time 725.0, rides 121\n",
      "Initial State is  [0, 23, 5]\n",
      "episode 604, reward 875.0, memory_length 2000, epsilon 0.048190768399801194, time 737.0, rides 126\n",
      "Initial State is  [3, 21, 5]\n",
      "episode 605, reward 1211.0, memory_length 2000, epsilon 0.04794981455780219, time 736.0, rides 115\n",
      "Initial State is  [2, 2, 0]\n",
      "episode 606, reward 1106.0, memory_length 2000, epsilon 0.04771006548501318, time 731.0, rides 120\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 607, reward 1299.0, memory_length 2000, epsilon 0.047471515157588115, time 731.0, rides 114\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 608, reward 861.0, memory_length 2000, epsilon 0.047234157581800176, time 725.0, rides 116\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 609, reward 1012.0, memory_length 2000, epsilon 0.046997986793891174, time 730.0, rides 119\n",
      "Initial State is  [1, 0, 0]\n",
      "episode 610, reward 1314.0, memory_length 2000, epsilon 0.04676299685992172, time 735.0, rides 116\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 611, reward 1206.0, memory_length 2000, epsilon 0.04652918187562211, time 726.0, rides 116\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 612, reward 1011.0, memory_length 2000, epsilon 0.046296535966244, time 734.0, rides 111\n",
      "Initial State is  [4, 10, 4]\n",
      "episode 613, reward 1048.0, memory_length 2000, epsilon 0.046065053286412784, time 728.0, rides 121\n",
      "Initial State is  [1, 21, 3]\n",
      "episode 614, reward 1120.0, memory_length 2000, epsilon 0.04583472801998072, time 724.0, rides 115\n",
      "Initial State is  [0, 9, 3]\n",
      "episode 615, reward 985.0, memory_length 2000, epsilon 0.045605554379880814, time 725.0, rides 120\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 616, reward 915.0, memory_length 2000, epsilon 0.04537752660798141, time 726.0, rides 121\n",
      "Initial State is  [4, 19, 5]\n",
      "episode 617, reward 1115.0, memory_length 2000, epsilon 0.0451506389749415, time 729.0, rides 113\n",
      "Initial State is  [1, 19, 3]\n",
      "episode 618, reward 736.0, memory_length 2000, epsilon 0.044924885780066794, time 721.0, rides 125\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 619, reward 1158.0, memory_length 2000, epsilon 0.04470026135116646, time 735.0, rides 118\n",
      "Initial State is  [2, 15, 2]\n",
      "episode 620, reward 1008.0, memory_length 2000, epsilon 0.04447676004441063, time 728.0, rides 116\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 621, reward 931.0, memory_length 2000, epsilon 0.04425437624418858, time 723.0, rides 109\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 622, reward 841.0, memory_length 2000, epsilon 0.04403310436296763, time 729.0, rides 120\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 623, reward 1259.0, memory_length 2000, epsilon 0.043812938841152796, time 729.0, rides 124\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 624, reward 1387.0, memory_length 2000, epsilon 0.04359387414694703, time 727.0, rides 129\n",
      "Initial State is  [3, 4, 3]\n",
      "episode 625, reward 832.0, memory_length 2000, epsilon 0.043375904776212296, time 730.0, rides 105\n",
      "Initial State is  [1, 12, 4]\n",
      "episode 626, reward 1050.0, memory_length 2000, epsilon 0.043159025252331236, time 727.0, rides 120\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 627, reward 1173.0, memory_length 2000, epsilon 0.04294323012606958, time 734.0, rides 126\n",
      "Initial State is  [4, 15, 2]\n",
      "episode 628, reward 1095.0, memory_length 2000, epsilon 0.04272851397543923, time 735.0, rides 116\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 629, reward 1164.0, memory_length 2000, epsilon 0.04251487140556204, time 727.0, rides 126\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 630, reward 1204.0, memory_length 2000, epsilon 0.04230229704853423, time 728.0, rides 118\n",
      "Initial State is  [1, 21, 1]\n",
      "episode 631, reward 695.0, memory_length 2000, epsilon 0.04209078556329156, time 729.0, rides 110\n",
      "Initial State is  [3, 0, 5]\n",
      "episode 632, reward 1109.0, memory_length 2000, epsilon 0.0418803316354751, time 724.0, rides 119\n",
      "Initial State is  [2, 19, 4]\n",
      "episode 633, reward 1245.0, memory_length 2000, epsilon 0.041670929977297724, time 724.0, rides 114\n",
      "Initial State is  [3, 8, 2]\n",
      "episode 634, reward 737.0, memory_length 2000, epsilon 0.04146257532741124, time 733.0, rides 116\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 635, reward 1046.0, memory_length 2000, epsilon 0.04125526245077418, time 733.0, rides 117\n",
      "Initial State is  [4, 14, 2]\n",
      "episode 636, reward 971.0, memory_length 2000, epsilon 0.04104898613852031, time 728.0, rides 112\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 637, reward 1246.0, memory_length 2000, epsilon 0.04084374120782771, time 733.0, rides 118\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 638, reward 1022.0, memory_length 2000, epsilon 0.04063952250178857, time 727.0, rides 125\n",
      "Initial State is  [4, 17, 3]\n",
      "episode 639, reward 827.0, memory_length 2000, epsilon 0.04043632488927963, time 736.0, rides 110\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 640, reward 824.0, memory_length 2000, epsilon 0.04023414326483323, time 729.0, rides 114\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 641, reward 824.0, memory_length 2000, epsilon 0.040032972548509065, time 726.0, rides 126\n",
      "Initial State is  [2, 18, 5]\n",
      "episode 642, reward 1095.0, memory_length 2000, epsilon 0.03983280768576652, time 735.0, rides 121\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 643, reward 1298.0, memory_length 2000, epsilon 0.03963364364733769, time 727.0, rides 120\n",
      "Initial State is  [3, 3, 0]\n",
      "episode 644, reward 939.0, memory_length 2000, epsilon 0.039435475429100995, time 722.0, rides 125\n",
      "Initial State is  [2, 10, 4]\n",
      "episode 645, reward 927.0, memory_length 2000, epsilon 0.03923829805195549, time 730.0, rides 130\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 646, reward 1028.0, memory_length 2000, epsilon 0.03904210656169572, time 732.0, rides 128\n",
      "Initial State is  [1, 3, 5]\n",
      "episode 647, reward 1228.0, memory_length 2000, epsilon 0.03884689602888724, time 724.0, rides 127\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 648, reward 944.0, memory_length 2000, epsilon 0.0386526615487428, time 726.0, rides 115\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 649, reward 847.0, memory_length 2000, epsilon 0.03845939824099909, time 733.0, rides 123\n",
      "Initial State is  [0, 1, 4]\n",
      "episode 650, reward 729.0, memory_length 2000, epsilon 0.03826710124979409, time 736.0, rides 118\n",
      "Initial State is  [0, 2, 1]\n",
      "episode 651, reward 1226.0, memory_length 2000, epsilon 0.038075765743545126, time 729.0, rides 117\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 652, reward 988.0, memory_length 2000, epsilon 0.0378853869148274, time 739.0, rides 116\n",
      "Initial State is  [0, 3, 1]\n",
      "episode 653, reward 957.0, memory_length 2000, epsilon 0.03769595998025326, time 724.0, rides 120\n",
      "Initial State is  [1, 13, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 654, reward 1053.0, memory_length 2000, epsilon 0.03750748018035199, time 731.0, rides 123\n",
      "Initial State is  [3, 12, 1]\n",
      "episode 655, reward 768.0, memory_length 2000, epsilon 0.037319942779450235, time 735.0, rides 114\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 656, reward 1229.0, memory_length 2000, epsilon 0.037133343065552986, time 725.0, rides 117\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 657, reward 1327.0, memory_length 2000, epsilon 0.03694767635022522, time 730.0, rides 134\n",
      "Initial State is  [0, 11, 2]\n",
      "episode 658, reward 1059.0, memory_length 2000, epsilon 0.036762937968474095, time 728.0, rides 110\n",
      "Initial State is  [3, 0, 1]\n",
      "episode 659, reward 838.0, memory_length 2000, epsilon 0.03657912327863173, time 724.0, rides 115\n",
      "Initial State is  [4, 15, 0]\n",
      "episode 660, reward 625.0, memory_length 2000, epsilon 0.036396227662238566, time 727.0, rides 114\n",
      "Initial State is  [2, 5, 1]\n",
      "episode 661, reward 1452.0, memory_length 2000, epsilon 0.03621424652392737, time 739.0, rides 133\n",
      "Initial State is  [0, 19, 6]\n",
      "episode 662, reward 1161.0, memory_length 2000, epsilon 0.036033175291307735, time 731.0, rides 125\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 663, reward 1288.0, memory_length 2000, epsilon 0.03585300941485119, time 728.0, rides 120\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 664, reward 1001.0, memory_length 2000, epsilon 0.035673744367776934, time 728.0, rides 129\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 665, reward 1085.0, memory_length 2000, epsilon 0.03549537564593805, time 727.0, rides 111\n",
      "Initial State is  [3, 12, 1]\n",
      "episode 666, reward 1156.0, memory_length 2000, epsilon 0.035317898767708356, time 727.0, rides 119\n",
      "Initial State is  [4, 7, 2]\n",
      "episode 667, reward 1234.0, memory_length 2000, epsilon 0.03514130927386981, time 734.0, rides 115\n",
      "Initial State is  [1, 3, 0]\n",
      "episode 668, reward 1143.0, memory_length 2000, epsilon 0.03496560272750046, time 725.0, rides 106\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 669, reward 1097.0, memory_length 2000, epsilon 0.03479077471386296, time 732.0, rides 109\n",
      "Initial State is  [4, 23, 6]\n",
      "episode 670, reward 806.0, memory_length 2000, epsilon 0.03461682084029365, time 729.0, rides 105\n",
      "Initial State is  [2, 16, 0]\n",
      "episode 671, reward 1133.0, memory_length 2000, epsilon 0.034443736736092176, time 726.0, rides 117\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 672, reward 1091.0, memory_length 2000, epsilon 0.034271518052411715, time 728.0, rides 117\n",
      "Initial State is  [0, 18, 4]\n",
      "episode 673, reward 955.0, memory_length 2000, epsilon 0.034100160462149656, time 732.0, rides 119\n",
      "Initial State is  [0, 19, 6]\n",
      "episode 674, reward 887.0, memory_length 2000, epsilon 0.03392965965983891, time 731.0, rides 113\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 675, reward 862.0, memory_length 2000, epsilon 0.033760011361539714, time 731.0, rides 118\n",
      "Initial State is  [3, 6, 2]\n",
      "episode 676, reward 1038.0, memory_length 2000, epsilon 0.03359121130473201, time 727.0, rides 120\n",
      "Initial State is  [3, 16, 2]\n",
      "episode 677, reward 1216.0, memory_length 2000, epsilon 0.033423255248208356, time 729.0, rides 128\n",
      "Initial State is  [4, 9, 4]\n",
      "episode 678, reward 885.0, memory_length 2000, epsilon 0.03325613897196732, time 724.0, rides 124\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 679, reward 849.0, memory_length 2000, epsilon 0.03308985827710748, time 731.0, rides 115\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 680, reward 987.0, memory_length 2000, epsilon 0.032924408985721944, time 725.0, rides 121\n",
      "Initial State is  [2, 8, 4]\n",
      "episode 681, reward 1263.0, memory_length 2000, epsilon 0.03275978694079333, time 728.0, rides 127\n",
      "Initial State is  [0, 11, 2]\n",
      "episode 682, reward 1255.0, memory_length 2000, epsilon 0.032595988006089364, time 729.0, rides 116\n",
      "Initial State is  [4, 5, 5]\n",
      "episode 683, reward 907.0, memory_length 2000, epsilon 0.032433008066058915, time 728.0, rides 118\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 684, reward 1070.0, memory_length 2000, epsilon 0.03227084302572862, time 730.0, rides 119\n",
      "Initial State is  [1, 3, 2]\n",
      "episode 685, reward 964.0, memory_length 2000, epsilon 0.032109488810599975, time 723.0, rides 118\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 686, reward 1096.0, memory_length 2000, epsilon 0.031948941366546975, time 727.0, rides 120\n",
      "Initial State is  [4, 22, 3]\n",
      "episode 687, reward 979.0, memory_length 2000, epsilon 0.03178919665971424, time 728.0, rides 116\n",
      "Initial State is  [1, 16, 1]\n",
      "episode 688, reward 1412.0, memory_length 2000, epsilon 0.03163025067641567, time 733.0, rides 133\n",
      "Initial State is  [4, 12, 0]\n",
      "episode 689, reward 839.0, memory_length 2000, epsilon 0.03147209942303359, time 727.0, rides 125\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 690, reward 1190.0, memory_length 2000, epsilon 0.03131473892591842, time 724.0, rides 121\n",
      "Initial State is  [1, 8, 0]\n",
      "episode 691, reward 733.0, memory_length 2000, epsilon 0.031158165231288826, time 728.0, rides 108\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 692, reward 871.0, memory_length 2000, epsilon 0.03100237440513238, time 727.0, rides 122\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 693, reward 1188.0, memory_length 2000, epsilon 0.030847362533106718, time 732.0, rides 114\n",
      "Initial State is  [2, 16, 5]\n",
      "episode 694, reward 1260.0, memory_length 2000, epsilon 0.030693125720441184, time 730.0, rides 117\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 695, reward 1071.0, memory_length 2000, epsilon 0.030539660091838977, time 731.0, rides 107\n",
      "Initial State is  [1, 9, 2]\n",
      "episode 696, reward 887.0, memory_length 2000, epsilon 0.03038696179137978, time 731.0, rides 120\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 697, reward 851.0, memory_length 2000, epsilon 0.030235026982422884, time 724.0, rides 110\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 698, reward 1026.0, memory_length 2000, epsilon 0.030083851847510768, time 732.0, rides 115\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 699, reward 1156.0, memory_length 2000, epsilon 0.029933432588273214, time 731.0, rides 115\n",
      "Initial State is  [4, 7, 6]\n",
      "episode 700, reward 1050.0, memory_length 2000, epsilon 0.029783765425331846, time 733.0, rides 116\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 701, reward 1134.0, memory_length 2000, epsilon 0.029634846598205186, time 722.0, rides 119\n",
      "Initial State is  [0, 10, 2]\n",
      "episode 702, reward 1006.0, memory_length 2000, epsilon 0.02948667236521416, time 728.0, rides 113\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 703, reward 1189.0, memory_length 2000, epsilon 0.029339239003388088, time 731.0, rides 124\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 704, reward 1098.0, memory_length 2000, epsilon 0.029192542808371146, time 730.0, rides 109\n",
      "Initial State is  [3, 10, 3]\n",
      "episode 705, reward 1110.0, memory_length 2000, epsilon 0.02904658009432929, time 724.0, rides 115\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 706, reward 1200.0, memory_length 2000, epsilon 0.028901347193857643, time 730.0, rides 120\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 707, reward 993.0, memory_length 2000, epsilon 0.028756840457888354, time 730.0, rides 115\n",
      "Initial State is  [4, 8, 5]\n",
      "episode 708, reward 807.0, memory_length 2000, epsilon 0.02861305625559891, time 731.0, rides 113\n",
      "Initial State is  [4, 18, 5]\n",
      "episode 709, reward 1228.0, memory_length 2000, epsilon 0.028469990974320916, time 721.0, rides 115\n",
      "Initial State is  [3, 19, 4]\n",
      "episode 710, reward 1306.0, memory_length 2000, epsilon 0.02832764101944931, time 745.0, rides 119\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 711, reward 1164.0, memory_length 2000, epsilon 0.028186002814352063, time 727.0, rides 113\n",
      "Initial State is  [1, 18, 0]\n",
      "episode 712, reward 968.0, memory_length 2000, epsilon 0.0280450728002803, time 725.0, rides 126\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 713, reward 1050.0, memory_length 2000, epsilon 0.0279048474362789, time 738.0, rides 117\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 714, reward 1119.0, memory_length 2000, epsilon 0.027765323199097504, time 728.0, rides 115\n",
      "Initial State is  [4, 17, 5]\n",
      "episode 715, reward 1054.0, memory_length 2000, epsilon 0.027626496583102015, time 737.0, rides 122\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 716, reward 1232.0, memory_length 2000, epsilon 0.027488364100186506, time 734.0, rides 121\n",
      "Initial State is  [3, 10, 2]\n",
      "episode 717, reward 1239.0, memory_length 2000, epsilon 0.027350922279685573, time 721.0, rides 112\n",
      "Initial State is  [4, 3, 4]\n",
      "episode 718, reward 741.0, memory_length 2000, epsilon 0.027214167668287145, time 726.0, rides 113\n",
      "Initial State is  [1, 16, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 719, reward 1119.0, memory_length 2000, epsilon 0.02707809682994571, time 723.0, rides 124\n",
      "Initial State is  [0, 9, 2]\n",
      "episode 720, reward 1079.0, memory_length 2000, epsilon 0.02694270634579598, time 736.0, rides 128\n",
      "Initial State is  [1, 13, 0]\n",
      "episode 721, reward 1072.0, memory_length 2000, epsilon 0.026807992814067, time 729.0, rides 119\n",
      "Initial State is  [4, 18, 2]\n",
      "episode 722, reward 895.0, memory_length 2000, epsilon 0.026673952849996664, time 731.0, rides 114\n",
      "Initial State is  [0, 22, 2]\n",
      "episode 723, reward 1387.0, memory_length 2000, epsilon 0.02654058308574668, time 733.0, rides 124\n",
      "Initial State is  [4, 19, 5]\n",
      "episode 724, reward 973.0, memory_length 2000, epsilon 0.026407880170317945, time 738.0, rides 123\n",
      "Initial State is  [1, 8, 2]\n",
      "episode 725, reward 1105.0, memory_length 2000, epsilon 0.026275840769466357, time 729.0, rides 116\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 726, reward 1121.0, memory_length 2000, epsilon 0.026144461565619025, time 734.0, rides 119\n",
      "Initial State is  [1, 18, 4]\n",
      "episode 727, reward 979.0, memory_length 2000, epsilon 0.02601373925779093, time 725.0, rides 117\n",
      "Initial State is  [0, 9, 2]\n",
      "episode 728, reward 901.0, memory_length 2000, epsilon 0.025883670561501974, time 744.0, rides 111\n",
      "Initial State is  [0, 23, 4]\n",
      "episode 729, reward 978.0, memory_length 2000, epsilon 0.025754252208694463, time 730.0, rides 130\n",
      "Initial State is  [3, 9, 6]\n",
      "episode 730, reward 874.0, memory_length 2000, epsilon 0.02562548094765099, time 725.0, rides 115\n",
      "Initial State is  [2, 9, 3]\n",
      "episode 731, reward 917.0, memory_length 2000, epsilon 0.025497353542912736, time 726.0, rides 103\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 732, reward 1196.0, memory_length 2000, epsilon 0.02536986677519817, time 732.0, rides 122\n",
      "Initial State is  [3, 17, 4]\n",
      "episode 733, reward 898.0, memory_length 2000, epsilon 0.02524301744132218, time 735.0, rides 113\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 734, reward 1286.0, memory_length 2000, epsilon 0.025116802354115567, time 734.0, rides 125\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 735, reward 1010.0, memory_length 2000, epsilon 0.024991218342344988, time 724.0, rides 113\n",
      "Initial State is  [1, 10, 5]\n",
      "episode 736, reward 773.0, memory_length 2000, epsilon 0.024866262250633264, time 722.0, rides 116\n",
      "Initial State is  [3, 15, 6]\n",
      "episode 737, reward 1155.0, memory_length 2000, epsilon 0.024741930939380097, time 731.0, rides 118\n",
      "Initial State is  [3, 9, 3]\n",
      "episode 738, reward 1197.0, memory_length 2000, epsilon 0.024618221284683196, time 727.0, rides 125\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 739, reward 975.0, memory_length 2000, epsilon 0.02449513017825978, time 736.0, rides 122\n",
      "Initial State is  [4, 8, 1]\n",
      "episode 740, reward 619.0, memory_length 2000, epsilon 0.02437265452736848, time 736.0, rides 121\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 741, reward 1223.0, memory_length 2000, epsilon 0.024250791254731636, time 729.0, rides 114\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 742, reward 1155.0, memory_length 2000, epsilon 0.024129537298457977, time 741.0, rides 115\n",
      "Initial State is  [3, 4, 0]\n",
      "episode 743, reward 1028.0, memory_length 2000, epsilon 0.024008889611965685, time 728.0, rides 123\n",
      "Initial State is  [1, 21, 5]\n",
      "episode 744, reward 1013.0, memory_length 2000, epsilon 0.023888845163905856, time 723.0, rides 124\n",
      "Initial State is  [0, 14, 1]\n",
      "episode 745, reward 998.0, memory_length 2000, epsilon 0.023769400938086327, time 730.0, rides 119\n",
      "Initial State is  [1, 21, 0]\n",
      "episode 746, reward 1263.0, memory_length 2000, epsilon 0.023650553933395897, time 722.0, rides 119\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 747, reward 927.0, memory_length 2000, epsilon 0.023532301163728918, time 728.0, rides 110\n",
      "Initial State is  [2, 16, 4]\n",
      "episode 748, reward 828.0, memory_length 2000, epsilon 0.023414639657910272, time 726.0, rides 115\n",
      "Initial State is  [2, 18, 6]\n",
      "episode 749, reward 1099.0, memory_length 2000, epsilon 0.023297566459620722, time 732.0, rides 117\n",
      "Initial State is  [0, 1, 1]\n",
      "episode 750, reward 1171.0, memory_length 2000, epsilon 0.023181078627322618, time 733.0, rides 118\n",
      "Initial State is  [1, 17, 0]\n",
      "episode 751, reward 1167.0, memory_length 2000, epsilon 0.023065173234186005, time 729.0, rides 132\n",
      "Initial State is  [0, 14, 5]\n",
      "episode 752, reward 942.0, memory_length 2000, epsilon 0.022949847368015076, time 724.0, rides 109\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 753, reward 1219.0, memory_length 2000, epsilon 0.022835098131175, time 729.0, rides 117\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 754, reward 1163.0, memory_length 2000, epsilon 0.022720922640519125, time 730.0, rides 122\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 755, reward 1262.0, memory_length 2000, epsilon 0.02260731802731653, time 726.0, rides 120\n",
      "Initial State is  [1, 3, 6]\n",
      "episode 756, reward 1039.0, memory_length 2000, epsilon 0.022494281437179946, time 730.0, rides 116\n",
      "Initial State is  [2, 6, 4]\n",
      "episode 757, reward 1229.0, memory_length 2000, epsilon 0.022381810029994047, time 726.0, rides 123\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 758, reward 1041.0, memory_length 2000, epsilon 0.022269900979844076, time 736.0, rides 112\n",
      "Initial State is  [2, 18, 4]\n",
      "episode 759, reward 1108.0, memory_length 2000, epsilon 0.022158551474944856, time 729.0, rides 131\n",
      "Initial State is  [2, 21, 1]\n",
      "episode 760, reward 899.0, memory_length 2000, epsilon 0.022047758717570132, time 727.0, rides 122\n",
      "Initial State is  [0, 12, 5]\n",
      "episode 761, reward 1140.0, memory_length 2000, epsilon 0.021937519923982282, time 724.0, rides 119\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 762, reward 1140.0, memory_length 2000, epsilon 0.021827832324362372, time 726.0, rides 121\n",
      "Initial State is  [3, 10, 3]\n",
      "episode 763, reward 1018.0, memory_length 2000, epsilon 0.02171869316274056, time 727.0, rides 124\n",
      "Initial State is  [3, 16, 3]\n",
      "episode 764, reward 1134.0, memory_length 2000, epsilon 0.021610099696926857, time 727.0, rides 119\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 765, reward 1143.0, memory_length 2000, epsilon 0.021502049198442223, time 728.0, rides 125\n",
      "Initial State is  [1, 2, 0]\n",
      "episode 766, reward 1096.0, memory_length 2000, epsilon 0.021394538952450012, time 739.0, rides 122\n",
      "Initial State is  [1, 3, 6]\n",
      "episode 767, reward 969.0, memory_length 2000, epsilon 0.02128756625768776, time 730.0, rides 120\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 768, reward 1274.0, memory_length 2000, epsilon 0.021181128426399323, time 724.0, rides 114\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 769, reward 1171.0, memory_length 2000, epsilon 0.021075222784267326, time 723.0, rides 119\n",
      "Initial State is  [1, 5, 6]\n",
      "episode 770, reward 790.0, memory_length 2000, epsilon 0.020969846670345987, time 729.0, rides 122\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 771, reward 901.0, memory_length 2000, epsilon 0.020864997436994256, time 735.0, rides 113\n",
      "Initial State is  [3, 22, 0]\n",
      "episode 772, reward 1108.0, memory_length 2000, epsilon 0.020760672449809284, time 730.0, rides 124\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 773, reward 880.0, memory_length 2000, epsilon 0.020656869087560238, time 733.0, rides 108\n",
      "Initial State is  [4, 8, 4]\n",
      "episode 774, reward 1207.0, memory_length 2000, epsilon 0.020553584742122436, time 738.0, rides 113\n",
      "Initial State is  [4, 17, 6]\n",
      "episode 775, reward 1084.0, memory_length 2000, epsilon 0.020450816818411825, time 731.0, rides 116\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 776, reward 1105.0, memory_length 2000, epsilon 0.020348562734319765, time 731.0, rides 119\n",
      "Initial State is  [4, 3, 0]\n",
      "episode 777, reward 1107.0, memory_length 2000, epsilon 0.020246819920648168, time 733.0, rides 123\n",
      "Initial State is  [2, 20, 5]\n",
      "episode 778, reward 832.0, memory_length 2000, epsilon 0.020145585821044927, time 724.0, rides 118\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 779, reward 1351.0, memory_length 2000, epsilon 0.020044857891939702, time 728.0, rides 127\n",
      "Initial State is  [4, 17, 2]\n",
      "episode 780, reward 1040.0, memory_length 2000, epsilon 0.019944633602480003, time 734.0, rides 126\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 781, reward 1053.0, memory_length 2000, epsilon 0.019844910434467605, time 729.0, rides 114\n",
      "Initial State is  [0, 12, 3]\n",
      "episode 782, reward 1303.0, memory_length 2000, epsilon 0.019745685882295267, time 723.0, rides 114\n",
      "Initial State is  [0, 8, 1]\n",
      "episode 783, reward 1353.0, memory_length 2000, epsilon 0.01964695745288379, time 729.0, rides 123\n",
      "Initial State is  [1, 21, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 784, reward 1052.0, memory_length 2000, epsilon 0.01954872266561937, time 730.0, rides 113\n",
      "Initial State is  [2, 3, 6]\n",
      "episode 785, reward 1022.0, memory_length 2000, epsilon 0.019450979052291272, time 738.0, rides 111\n",
      "Initial State is  [4, 14, 1]\n",
      "episode 786, reward 1159.0, memory_length 2000, epsilon 0.019353724157029815, time 726.0, rides 120\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 787, reward 946.0, memory_length 2000, epsilon 0.019256955536244666, time 733.0, rides 112\n",
      "Initial State is  [0, 9, 3]\n",
      "episode 788, reward 1098.0, memory_length 2000, epsilon 0.019160670758563442, time 731.0, rides 117\n",
      "Initial State is  [1, 22, 4]\n",
      "episode 789, reward 1080.0, memory_length 2000, epsilon 0.019064867404770626, time 731.0, rides 130\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 790, reward 1232.0, memory_length 2000, epsilon 0.018969543067746772, time 725.0, rides 120\n",
      "Initial State is  [0, 20, 2]\n",
      "episode 791, reward 976.0, memory_length 2000, epsilon 0.018874695352408037, time 729.0, rides 120\n",
      "Initial State is  [0, 6, 2]\n",
      "episode 792, reward 832.0, memory_length 2000, epsilon 0.018780321875645996, time 734.0, rides 112\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 793, reward 1163.0, memory_length 2000, epsilon 0.018686420266267767, time 732.0, rides 115\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 794, reward 773.0, memory_length 2000, epsilon 0.018592988164936427, time 720.0, rides 107\n",
      "Initial State is  [3, 8, 0]\n",
      "episode 795, reward 1359.0, memory_length 2000, epsilon 0.018500023224111744, time 730.0, rides 127\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 796, reward 1076.0, memory_length 2000, epsilon 0.018407523107991184, time 724.0, rides 118\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 797, reward 1086.0, memory_length 2000, epsilon 0.01831548549245123, time 728.0, rides 108\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 798, reward 1174.0, memory_length 2000, epsilon 0.018223908064988973, time 732.0, rides 130\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 799, reward 822.0, memory_length 2000, epsilon 0.018132788524664028, time 729.0, rides 106\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 800, reward 1187.0, memory_length 2000, epsilon 0.018042124582040707, time 724.0, rides 126\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 801, reward 772.0, memory_length 2000, epsilon 0.017951913959130504, time 738.0, rides 103\n",
      "Initial State is  [3, 15, 2]\n",
      "episode 802, reward 1409.0, memory_length 2000, epsilon 0.01786215438933485, time 725.0, rides 123\n",
      "Initial State is  [0, 19, 6]\n",
      "episode 803, reward 1423.0, memory_length 2000, epsilon 0.017772843617388175, time 739.0, rides 119\n",
      "Initial State is  [1, 0, 5]\n",
      "episode 804, reward 1248.0, memory_length 2000, epsilon 0.017683979399301233, time 727.0, rides 113\n",
      "Initial State is  [4, 19, 4]\n",
      "episode 805, reward 1087.0, memory_length 2000, epsilon 0.017595559502304726, time 729.0, rides 114\n",
      "Initial State is  [0, 8, 1]\n",
      "episode 806, reward 962.0, memory_length 2000, epsilon 0.0175075817047932, time 728.0, rides 113\n",
      "Initial State is  [4, 23, 3]\n",
      "episode 807, reward 1337.0, memory_length 2000, epsilon 0.017420043796269234, time 726.0, rides 118\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 808, reward 1069.0, memory_length 2000, epsilon 0.017332943577287888, time 727.0, rides 118\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 809, reward 1105.0, memory_length 2000, epsilon 0.01724627885940145, time 728.0, rides 103\n",
      "Initial State is  [4, 19, 1]\n",
      "episode 810, reward 1284.0, memory_length 2000, epsilon 0.017160047465104442, time 734.0, rides 110\n",
      "Initial State is  [2, 16, 2]\n",
      "episode 811, reward 954.0, memory_length 2000, epsilon 0.01707424722777892, time 723.0, rides 118\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 812, reward 1104.0, memory_length 2000, epsilon 0.016988875991640028, time 730.0, rides 118\n",
      "Initial State is  [0, 5, 3]\n",
      "episode 813, reward 1216.0, memory_length 2000, epsilon 0.016903931611681827, time 735.0, rides 122\n",
      "Initial State is  [3, 2, 2]\n",
      "episode 814, reward 1061.0, memory_length 2000, epsilon 0.01681941195362342, time 731.0, rides 121\n",
      "Initial State is  [4, 20, 3]\n",
      "episode 815, reward 1444.0, memory_length 2000, epsilon 0.016735314893855303, time 727.0, rides 129\n",
      "Initial State is  [3, 2, 4]\n",
      "episode 816, reward 1158.0, memory_length 2000, epsilon 0.016651638319386028, time 732.0, rides 117\n",
      "Initial State is  [3, 10, 1]\n",
      "episode 817, reward 988.0, memory_length 2000, epsilon 0.0165683801277891, time 729.0, rides 128\n",
      "Initial State is  [1, 10, 0]\n",
      "episode 818, reward 1335.0, memory_length 2000, epsilon 0.016485538227150154, time 730.0, rides 122\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 819, reward 765.0, memory_length 2000, epsilon 0.0164031105360144, time 727.0, rides 124\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 820, reward 1052.0, memory_length 2000, epsilon 0.01632109498333433, time 731.0, rides 115\n",
      "Initial State is  [2, 23, 3]\n",
      "episode 821, reward 860.0, memory_length 2000, epsilon 0.016239489508417658, time 724.0, rides 106\n",
      "Initial State is  [2, 18, 0]\n",
      "episode 822, reward 1093.0, memory_length 2000, epsilon 0.01615829206087557, time 729.0, rides 114\n",
      "Initial State is  [0, 12, 3]\n",
      "episode 823, reward 1073.0, memory_length 2000, epsilon 0.01607750060057119, time 726.0, rides 110\n",
      "Initial State is  [3, 2, 3]\n",
      "episode 824, reward 1502.0, memory_length 2000, epsilon 0.015997113097568336, time 728.0, rides 121\n",
      "Initial State is  [2, 6, 0]\n",
      "episode 825, reward 1068.0, memory_length 2000, epsilon 0.015917127532080494, time 728.0, rides 128\n",
      "Initial State is  [1, 8, 6]\n",
      "episode 826, reward 1005.0, memory_length 2000, epsilon 0.01583754189442009, time 726.0, rides 114\n",
      "Initial State is  [1, 7, 5]\n",
      "episode 827, reward 1001.0, memory_length 2000, epsilon 0.01575835418494799, time 734.0, rides 115\n",
      "Initial State is  [4, 23, 1]\n",
      "episode 828, reward 978.0, memory_length 2000, epsilon 0.01567956241402325, time 727.0, rides 113\n",
      "Initial State is  [2, 10, 4]\n",
      "episode 829, reward 1018.0, memory_length 2000, epsilon 0.015601164601953134, time 722.0, rides 122\n",
      "Initial State is  [0, 20, 2]\n",
      "episode 830, reward 968.0, memory_length 2000, epsilon 0.015523158778943369, time 725.0, rides 117\n",
      "Initial State is  [4, 19, 1]\n",
      "episode 831, reward 1219.0, memory_length 2000, epsilon 0.015445542985048652, time 727.0, rides 113\n",
      "Initial State is  [0, 22, 1]\n",
      "episode 832, reward 1029.0, memory_length 2000, epsilon 0.015368315270123408, time 728.0, rides 116\n",
      "Initial State is  [2, 7, 6]\n",
      "episode 833, reward 1186.0, memory_length 2000, epsilon 0.01529147369377279, time 731.0, rides 113\n",
      "Initial State is  [0, 13, 4]\n",
      "episode 834, reward 1223.0, memory_length 2000, epsilon 0.015215016325303928, time 722.0, rides 121\n",
      "Initial State is  [0, 22, 2]\n",
      "episode 835, reward 1293.0, memory_length 2000, epsilon 0.015138941243677408, time 727.0, rides 115\n",
      "Initial State is  [3, 22, 3]\n",
      "episode 836, reward 1176.0, memory_length 2000, epsilon 0.01506324653745902, time 728.0, rides 123\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 837, reward 1233.0, memory_length 2000, epsilon 0.014987930304771725, time 725.0, rides 129\n",
      "Initial State is  [3, 15, 6]\n",
      "episode 838, reward 1364.0, memory_length 2000, epsilon 0.014912990653247866, time 729.0, rides 117\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 839, reward 906.0, memory_length 2000, epsilon 0.014838425699981627, time 728.0, rides 108\n",
      "Initial State is  [2, 12, 4]\n",
      "episode 840, reward 962.0, memory_length 2000, epsilon 0.01476423357148172, time 726.0, rides 117\n",
      "Initial State is  [4, 15, 4]\n",
      "episode 841, reward 1033.0, memory_length 2000, epsilon 0.014690412403624311, time 735.0, rides 106\n",
      "Initial State is  [1, 0, 3]\n",
      "episode 842, reward 768.0, memory_length 2000, epsilon 0.01461696034160619, time 733.0, rides 115\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 843, reward 986.0, memory_length 2000, epsilon 0.014543875539898159, time 732.0, rides 118\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 844, reward 1172.0, memory_length 2000, epsilon 0.014471156162198668, time 732.0, rides 114\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 845, reward 1151.0, memory_length 2000, epsilon 0.014398800381387675, time 726.0, rides 116\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 846, reward 1013.0, memory_length 2000, epsilon 0.014326806379480736, time 732.0, rides 121\n",
      "Initial State is  [2, 20, 3]\n",
      "episode 847, reward 1218.0, memory_length 2000, epsilon 0.014255172347583332, time 731.0, rides 123\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 848, reward 858.0, memory_length 2000, epsilon 0.014183896485845416, time 724.0, rides 103\n",
      "Initial State is  [1, 7, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 849, reward 1218.0, memory_length 2000, epsilon 0.014112977003416188, time 735.0, rides 120\n",
      "Initial State is  [1, 15, 0]\n",
      "episode 850, reward 1179.0, memory_length 2000, epsilon 0.014042412118399107, time 728.0, rides 111\n",
      "Initial State is  [3, 7, 0]\n",
      "episode 851, reward 807.0, memory_length 2000, epsilon 0.013972200057807112, time 728.0, rides 118\n",
      "Initial State is  [1, 4, 6]\n",
      "episode 852, reward 1166.0, memory_length 2000, epsilon 0.013902339057518077, time 730.0, rides 113\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 853, reward 1102.0, memory_length 2000, epsilon 0.013832827362230486, time 731.0, rides 119\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 854, reward 1033.0, memory_length 2000, epsilon 0.013763663225419333, time 731.0, rides 122\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 855, reward 1148.0, memory_length 2000, epsilon 0.013694844909292236, time 736.0, rides 118\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 856, reward 1036.0, memory_length 2000, epsilon 0.013626370684745774, time 730.0, rides 122\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 857, reward 995.0, memory_length 2000, epsilon 0.013558238831322046, time 724.0, rides 111\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 858, reward 935.0, memory_length 2000, epsilon 0.013490447637165436, time 730.0, rides 119\n",
      "Initial State is  [1, 8, 6]\n",
      "episode 859, reward 1055.0, memory_length 2000, epsilon 0.013422995398979608, time 728.0, rides 116\n",
      "Initial State is  [1, 3, 1]\n",
      "episode 860, reward 979.0, memory_length 2000, epsilon 0.01335588042198471, time 725.0, rides 115\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 861, reward 1335.0, memory_length 2000, epsilon 0.013289101019874787, time 727.0, rides 122\n",
      "Initial State is  [0, 14, 0]\n",
      "episode 862, reward 913.0, memory_length 2000, epsilon 0.013222655514775413, time 722.0, rides 114\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 863, reward 1013.0, memory_length 2000, epsilon 0.013156542237201536, time 738.0, rides 116\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 864, reward 1056.0, memory_length 2000, epsilon 0.013090759526015528, time 732.0, rides 111\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 865, reward 895.0, memory_length 2000, epsilon 0.01302530572838545, time 740.0, rides 109\n",
      "Initial State is  [1, 0, 0]\n",
      "episode 866, reward 1123.0, memory_length 2000, epsilon 0.012960179199743523, time 725.0, rides 124\n",
      "Initial State is  [3, 2, 4]\n",
      "episode 867, reward 1207.0, memory_length 2000, epsilon 0.012895378303744804, time 737.0, rides 121\n",
      "Initial State is  [4, 8, 1]\n",
      "episode 868, reward 1260.0, memory_length 2000, epsilon 0.01283090141222608, time 732.0, rides 116\n",
      "Initial State is  [1, 16, 0]\n",
      "episode 869, reward 1190.0, memory_length 2000, epsilon 0.012766746905164949, time 734.0, rides 118\n",
      "Initial State is  [3, 13, 0]\n",
      "episode 870, reward 998.0, memory_length 2000, epsilon 0.012702913170639124, time 726.0, rides 123\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 871, reward 1303.0, memory_length 2000, epsilon 0.012639398604785928, time 720.0, rides 125\n",
      "Initial State is  [1, 8, 1]\n",
      "episode 872, reward 934.0, memory_length 2000, epsilon 0.012576201611761997, time 734.0, rides 118\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 873, reward 1497.0, memory_length 2000, epsilon 0.012513320603703188, time 731.0, rides 119\n",
      "Initial State is  [0, 1, 4]\n",
      "episode 874, reward 1356.0, memory_length 2000, epsilon 0.012450754000684672, time 729.0, rides 124\n",
      "Initial State is  [2, 22, 1]\n",
      "episode 875, reward 994.0, memory_length 2000, epsilon 0.012388500230681249, time 730.0, rides 119\n",
      "Initial State is  [2, 19, 6]\n",
      "episode 876, reward 1009.0, memory_length 2000, epsilon 0.012326557729527843, time 730.0, rides 111\n",
      "Initial State is  [2, 6, 6]\n",
      "episode 877, reward 1223.0, memory_length 2000, epsilon 0.012264924940880204, time 729.0, rides 111\n",
      "Initial State is  [3, 4, 0]\n",
      "episode 878, reward 739.0, memory_length 2000, epsilon 0.012203600316175803, time 726.0, rides 104\n",
      "Initial State is  [2, 11, 1]\n",
      "episode 879, reward 1159.0, memory_length 2000, epsilon 0.012142582314594924, time 728.0, rides 126\n",
      "Initial State is  [3, 11, 2]\n",
      "episode 880, reward 1068.0, memory_length 2000, epsilon 0.01208186940302195, time 728.0, rides 111\n",
      "Initial State is  [1, 17, 0]\n",
      "episode 881, reward 1103.0, memory_length 2000, epsilon 0.01202146005600684, time 730.0, rides 117\n",
      "Initial State is  [3, 13, 0]\n",
      "episode 882, reward 1155.0, memory_length 2000, epsilon 0.011961352755726806, time 728.0, rides 120\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 883, reward 804.0, memory_length 2000, epsilon 0.01190154599194817, time 731.0, rides 112\n",
      "Initial State is  [3, 7, 6]\n",
      "episode 884, reward 967.0, memory_length 2000, epsilon 0.01184203826198843, time 732.0, rides 110\n",
      "Initial State is  [4, 19, 4]\n",
      "episode 885, reward 1007.0, memory_length 2000, epsilon 0.011782828070678488, time 724.0, rides 106\n",
      "Initial State is  [3, 2, 6]\n",
      "episode 886, reward 962.0, memory_length 2000, epsilon 0.011723913930325095, time 730.0, rides 114\n",
      "Initial State is  [3, 8, 1]\n",
      "episode 887, reward 1223.0, memory_length 2000, epsilon 0.01166529436067347, time 728.0, rides 113\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 888, reward 805.0, memory_length 2000, epsilon 0.011606967888870102, time 725.0, rides 115\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 889, reward 1269.0, memory_length 2000, epsilon 0.01154893304942575, time 725.0, rides 120\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 890, reward 1160.0, memory_length 2000, epsilon 0.011491188384178622, time 724.0, rides 119\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 891, reward 964.0, memory_length 2000, epsilon 0.011433732442257729, time 734.0, rides 109\n",
      "Initial State is  [3, 6, 0]\n",
      "episode 892, reward 1039.0, memory_length 2000, epsilon 0.01137656378004644, time 734.0, rides 110\n",
      "Initial State is  [4, 4, 5]\n",
      "episode 893, reward 1288.0, memory_length 2000, epsilon 0.011319680961146208, time 728.0, rides 114\n",
      "Initial State is  [3, 5, 5]\n",
      "episode 894, reward 984.0, memory_length 2000, epsilon 0.011263082556340478, time 732.0, rides 110\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 895, reward 1118.0, memory_length 2000, epsilon 0.011206767143558775, time 728.0, rides 118\n",
      "Initial State is  [1, 2, 0]\n",
      "episode 896, reward 1200.0, memory_length 2000, epsilon 0.011150733307840981, time 726.0, rides 132\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 897, reward 739.0, memory_length 2000, epsilon 0.011094979641301777, time 743.0, rides 112\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 898, reward 1125.0, memory_length 2000, epsilon 0.011039504743095268, time 734.0, rides 122\n",
      "Initial State is  [1, 2, 2]\n",
      "episode 899, reward 905.0, memory_length 2000, epsilon 0.01098430721937979, time 725.0, rides 115\n",
      "Initial State is  [0, 2, 5]\n",
      "episode 900, reward 1029.0, memory_length 2000, epsilon 0.010929385683282892, time 725.0, rides 122\n",
      "Initial State is  [3, 8, 3]\n",
      "episode 901, reward 1073.0, memory_length 2000, epsilon 0.010874738754866477, time 724.0, rides 110\n",
      "Initial State is  [0, 15, 0]\n",
      "episode 902, reward 1152.0, memory_length 2000, epsilon 0.010820365061092144, time 722.0, rides 127\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 903, reward 1046.0, memory_length 2000, epsilon 0.010766263235786683, time 728.0, rides 118\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 904, reward 732.0, memory_length 2000, epsilon 0.01071243191960775, time 739.0, rides 110\n",
      "Initial State is  [3, 15, 2]\n",
      "episode 905, reward 1164.0, memory_length 2000, epsilon 0.010658869760009713, time 738.0, rides 129\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 906, reward 927.0, memory_length 2000, epsilon 0.010605575411209664, time 728.0, rides 111\n",
      "Initial State is  [0, 16, 3]\n",
      "episode 907, reward 1186.0, memory_length 2000, epsilon 0.010552547534153616, time 727.0, rides 122\n",
      "Initial State is  [4, 12, 2]\n",
      "episode 908, reward 917.0, memory_length 2000, epsilon 0.010499784796482848, time 722.0, rides 108\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 909, reward 954.0, memory_length 2000, epsilon 0.010447285872500434, time 727.0, rides 114\n",
      "Initial State is  [1, 6, 3]\n",
      "episode 910, reward 986.0, memory_length 2000, epsilon 0.01039504944313793, time 729.0, rides 116\n",
      "Initial State is  [2, 18, 0]\n",
      "episode 911, reward 1222.0, memory_length 2000, epsilon 0.010343074195922241, time 724.0, rides 110\n",
      "Initial State is  [2, 6, 5]\n",
      "episode 912, reward 1041.0, memory_length 2000, epsilon 0.01029135882494263, time 724.0, rides 121\n",
      "Initial State is  [1, 8, 6]\n",
      "episode 913, reward 720.0, memory_length 2000, epsilon 0.010239902030817916, time 727.0, rides 112\n",
      "Initial State is  [3, 22, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 914, reward 1104.0, memory_length 2000, epsilon 0.010188702520663827, time 722.0, rides 120\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 915, reward 1103.0, memory_length 2000, epsilon 0.010137759008060509, time 734.0, rides 129\n",
      "Initial State is  [4, 21, 6]\n",
      "episode 916, reward 1082.0, memory_length 2000, epsilon 0.010087070213020206, time 731.0, rides 113\n",
      "Initial State is  [1, 0, 3]\n",
      "episode 917, reward 867.0, memory_length 2000, epsilon 0.010036634861955105, time 724.0, rides 114\n",
      "Initial State is  [3, 8, 3]\n",
      "episode 918, reward 1157.0, memory_length 2000, epsilon 0.00998645168764533, time 726.0, rides 133\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 919, reward 913.0, memory_length 2000, epsilon 0.009936519429207103, time 725.0, rides 114\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 920, reward 844.0, memory_length 2000, epsilon 0.009886836832061067, time 737.0, rides 115\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 921, reward 1182.0, memory_length 2000, epsilon 0.009837402647900761, time 730.0, rides 116\n",
      "Initial State is  [1, 9, 3]\n",
      "episode 922, reward 1177.0, memory_length 2000, epsilon 0.009788215634661257, time 724.0, rides 114\n",
      "Initial State is  [0, 19, 3]\n",
      "episode 923, reward 1063.0, memory_length 2000, epsilon 0.00973927455648795, time 722.0, rides 115\n",
      "Initial State is  [4, 13, 6]\n",
      "episode 924, reward 1141.0, memory_length 2000, epsilon 0.009690578183705511, time 730.0, rides 124\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 925, reward 848.0, memory_length 2000, epsilon 0.009642125292786984, time 722.0, rides 107\n",
      "Initial State is  [2, 6, 6]\n",
      "episode 926, reward 977.0, memory_length 2000, epsilon 0.009593914666323049, time 734.0, rides 121\n",
      "Initial State is  [1, 19, 3]\n",
      "episode 927, reward 1085.0, memory_length 2000, epsilon 0.009545945092991434, time 733.0, rides 119\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 928, reward 774.0, memory_length 2000, epsilon 0.009498215367526477, time 734.0, rides 107\n",
      "Initial State is  [4, 12, 6]\n",
      "episode 929, reward 1089.0, memory_length 2000, epsilon 0.009450724290688845, time 733.0, rides 111\n",
      "Initial State is  [2, 18, 3]\n",
      "episode 930, reward 1282.0, memory_length 2000, epsilon 0.0094034706692354, time 730.0, rides 126\n",
      "Initial State is  [0, 17, 1]\n",
      "episode 931, reward 932.0, memory_length 2000, epsilon 0.009356453315889223, time 724.0, rides 109\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 932, reward 775.0, memory_length 2000, epsilon 0.009309671049309777, time 732.0, rides 109\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 933, reward 932.0, memory_length 2000, epsilon 0.009263122694063227, time 732.0, rides 111\n",
      "Initial State is  [1, 6, 4]\n",
      "episode 934, reward 1071.0, memory_length 2000, epsilon 0.009216807080592911, time 727.0, rides 118\n",
      "Initial State is  [2, 23, 6]\n",
      "episode 935, reward 1260.0, memory_length 2000, epsilon 0.009170723045189946, time 741.0, rides 117\n",
      "Initial State is  [1, 1, 3]\n",
      "episode 936, reward 1197.0, memory_length 2000, epsilon 0.009124869429963996, time 735.0, rides 125\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 937, reward 987.0, memory_length 2000, epsilon 0.009079245082814175, time 728.0, rides 117\n",
      "Initial State is  [3, 5, 4]\n",
      "episode 938, reward 1092.0, memory_length 2000, epsilon 0.009033848857400105, time 736.0, rides 112\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 939, reward 919.0, memory_length 2000, epsilon 0.008988679613113105, time 726.0, rides 111\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 940, reward 915.0, memory_length 2000, epsilon 0.00894373621504754, time 728.0, rides 100\n",
      "Initial State is  [3, 7, 3]\n",
      "episode 941, reward 729.0, memory_length 2000, epsilon 0.008899017533972303, time 723.0, rides 106\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 942, reward 916.0, memory_length 2000, epsilon 0.008854522446302441, time 724.0, rides 129\n",
      "Initial State is  [1, 18, 0]\n",
      "episode 943, reward 1301.0, memory_length 2000, epsilon 0.00881024983407093, time 739.0, rides 123\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 944, reward 1345.0, memory_length 2000, epsilon 0.008766198584900575, time 737.0, rides 119\n",
      "Initial State is  [4, 1, 0]\n",
      "episode 945, reward 979.0, memory_length 2000, epsilon 0.008722367591976072, time 734.0, rides 117\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 946, reward 861.0, memory_length 2000, epsilon 0.008678755754016191, time 734.0, rides 111\n",
      "Initial State is  [3, 9, 6]\n",
      "episode 947, reward 993.0, memory_length 2000, epsilon 0.00863536197524611, time 723.0, rides 114\n",
      "Initial State is  [0, 7, 4]\n",
      "episode 948, reward 1251.0, memory_length 2000, epsilon 0.00859218516536988, time 727.0, rides 115\n",
      "Initial State is  [2, 2, 4]\n",
      "episode 949, reward 1043.0, memory_length 2000, epsilon 0.00854922423954303, time 721.0, rides 113\n",
      "Initial State is  [4, 14, 3]\n",
      "episode 950, reward 1045.0, memory_length 2000, epsilon 0.008506478118345316, time 736.0, rides 104\n",
      "Initial State is  [0, 3, 2]\n",
      "episode 951, reward 1204.0, memory_length 2000, epsilon 0.008463945727753589, time 727.0, rides 127\n",
      "Initial State is  [2, 19, 4]\n",
      "episode 952, reward 1134.0, memory_length 2000, epsilon 0.00842162599911482, time 730.0, rides 114\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 953, reward 1409.0, memory_length 2000, epsilon 0.008379517869119247, time 728.0, rides 129\n",
      "Initial State is  [4, 22, 2]\n",
      "episode 954, reward 1403.0, memory_length 2000, epsilon 0.008337620279773651, time 735.0, rides 122\n",
      "Initial State is  [1, 18, 1]\n",
      "episode 955, reward 1000.0, memory_length 2000, epsilon 0.008295932178374783, time 722.0, rides 111\n",
      "Initial State is  [0, 11, 6]\n",
      "episode 956, reward 1469.0, memory_length 2000, epsilon 0.008254452517482908, time 728.0, rides 123\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 957, reward 1250.0, memory_length 2000, epsilon 0.008213180254895494, time 729.0, rides 121\n",
      "Initial State is  [4, 7, 4]\n",
      "episode 958, reward 1105.0, memory_length 2000, epsilon 0.008172114353621017, time 734.0, rides 122\n",
      "Initial State is  [3, 7, 2]\n",
      "episode 959, reward 1393.0, memory_length 2000, epsilon 0.008131253781852912, time 724.0, rides 114\n",
      "Initial State is  [4, 13, 5]\n",
      "episode 960, reward 1287.0, memory_length 2000, epsilon 0.008090597512943647, time 730.0, rides 112\n",
      "Initial State is  [2, 23, 4]\n",
      "episode 961, reward 885.0, memory_length 2000, epsilon 0.008050144525378928, time 727.0, rides 107\n",
      "Initial State is  [0, 7, 2]\n",
      "episode 962, reward 909.0, memory_length 2000, epsilon 0.008009893802752034, time 727.0, rides 113\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 963, reward 1200.0, memory_length 2000, epsilon 0.007969844333738273, time 728.0, rides 127\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 964, reward 1038.0, memory_length 2000, epsilon 0.007929995112069581, time 731.0, rides 118\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 965, reward 1101.0, memory_length 2000, epsilon 0.007890345136509233, time 724.0, rides 113\n",
      "Initial State is  [1, 5, 3]\n",
      "episode 966, reward 806.0, memory_length 2000, epsilon 0.007850893410826686, time 728.0, rides 121\n",
      "Initial State is  [2, 16, 6]\n",
      "episode 967, reward 889.0, memory_length 2000, epsilon 0.007811638943772553, time 738.0, rides 116\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 968, reward 1039.0, memory_length 2000, epsilon 0.00777258074905369, time 734.0, rides 117\n",
      "Initial State is  [0, 10, 6]\n",
      "episode 969, reward 1256.0, memory_length 2000, epsilon 0.0077337178453084215, time 723.0, rides 122\n",
      "Initial State is  [3, 16, 5]\n",
      "episode 970, reward 1019.0, memory_length 2000, epsilon 0.0076950492560818795, time 728.0, rides 117\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 971, reward 1025.0, memory_length 2000, epsilon 0.00765657400980147, time 723.0, rides 119\n",
      "Initial State is  [2, 6, 0]\n",
      "episode 972, reward 1314.0, memory_length 2000, epsilon 0.007618291139752462, time 725.0, rides 130\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 973, reward 910.0, memory_length 2000, epsilon 0.0075801996840537, time 724.0, rides 116\n",
      "Initial State is  [2, 23, 5]\n",
      "episode 974, reward 793.0, memory_length 2000, epsilon 0.007542298685633431, time 727.0, rides 115\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 975, reward 1128.0, memory_length 2000, epsilon 0.007504587192205264, time 724.0, rides 122\n",
      "Initial State is  [4, 5, 0]\n",
      "episode 976, reward 976.0, memory_length 2000, epsilon 0.0074670642562442375, time 724.0, rides 118\n",
      "Initial State is  [0, 6, 1]\n",
      "episode 977, reward 1268.0, memory_length 2000, epsilon 0.007429728934963016, time 722.0, rides 113\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 978, reward 1153.0, memory_length 2000, epsilon 0.007392580290288201, time 734.0, rides 116\n",
      "Initial State is  [0, 3, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 979, reward 944.0, memory_length 2000, epsilon 0.00735561738883676, time 725.0, rides 116\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 980, reward 1226.0, memory_length 2000, epsilon 0.007318839301892576, time 729.0, rides 119\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 981, reward 1034.0, memory_length 2000, epsilon 0.0072822451053831125, time 728.0, rides 122\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 982, reward 947.0, memory_length 2000, epsilon 0.007245833879856197, time 730.0, rides 111\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 983, reward 919.0, memory_length 2000, epsilon 0.007209604710456916, time 729.0, rides 122\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 984, reward 1122.0, memory_length 2000, epsilon 0.0071735566869046315, time 732.0, rides 119\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 985, reward 1055.0, memory_length 2000, epsilon 0.007137688903470108, time 730.0, rides 125\n",
      "Initial State is  [1, 1, 1]\n",
      "episode 986, reward 1235.0, memory_length 2000, epsilon 0.0071020004589527575, time 730.0, rides 116\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 987, reward 1002.0, memory_length 2000, epsilon 0.0070664904566579935, time 727.0, rides 112\n",
      "Initial State is  [2, 16, 4]\n",
      "episode 988, reward 1229.0, memory_length 2000, epsilon 0.007031158004374704, time 732.0, rides 121\n",
      "Initial State is  [0, 22, 1]\n",
      "episode 989, reward 1310.0, memory_length 2000, epsilon 0.00699600221435283, time 724.0, rides 129\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 990, reward 1179.0, memory_length 2000, epsilon 0.0069610222032810655, time 728.0, rides 119\n",
      "Initial State is  [0, 14, 1]\n",
      "episode 991, reward 1170.0, memory_length 2000, epsilon 0.0069262170922646605, time 728.0, rides 127\n",
      "Initial State is  [0, 23, 0]\n",
      "episode 992, reward 1042.0, memory_length 2000, epsilon 0.006891586006803337, time 723.0, rides 117\n",
      "Initial State is  [3, 6, 0]\n",
      "episode 993, reward 1211.0, memory_length 2000, epsilon 0.006857128076769321, time 734.0, rides 126\n",
      "Initial State is  [3, 8, 6]\n",
      "episode 994, reward 1306.0, memory_length 2000, epsilon 0.006822842436385474, time 722.0, rides 124\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 995, reward 1023.0, memory_length 2000, epsilon 0.006788728224203546, time 727.0, rides 116\n",
      "Initial State is  [3, 10, 3]\n",
      "episode 996, reward 1327.0, memory_length 2000, epsilon 0.006754784583082528, time 732.0, rides 123\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 997, reward 1157.0, memory_length 2000, epsilon 0.006721010660167116, time 731.0, rides 109\n",
      "Initial State is  [2, 19, 4]\n",
      "episode 998, reward 1088.0, memory_length 2000, epsilon 0.00668740560686628, time 727.0, rides 128\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 999, reward 979.0, memory_length 2000, epsilon 0.006653968578831948, time 733.0, rides 114\n",
      "Initial State is  [3, 4, 5]\n",
      "episode 1000, reward 1056.0, memory_length 2000, epsilon 0.0066206987359377885, time 731.0, rides 108\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 1001, reward 1074.0, memory_length 2000, epsilon 0.0065875952422581, time 744.0, rides 115\n",
      "Initial State is  [1, 1, 1]\n",
      "episode 1002, reward 1112.0, memory_length 2000, epsilon 0.006554657266046809, time 734.0, rides 128\n",
      "Initial State is  [2, 9, 3]\n",
      "episode 1003, reward 1050.0, memory_length 2000, epsilon 0.006521883979716575, time 722.0, rides 120\n",
      "Initial State is  [2, 15, 1]\n",
      "episode 1004, reward 1338.0, memory_length 2000, epsilon 0.006489274559817992, time 746.0, rides 113\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 1005, reward 1049.0, memory_length 2000, epsilon 0.006456828187018902, time 723.0, rides 112\n",
      "Initial State is  [0, 16, 1]\n",
      "episode 1006, reward 1026.0, memory_length 2000, epsilon 0.006424544046083807, time 731.0, rides 109\n",
      "Initial State is  [3, 14, 2]\n",
      "episode 1007, reward 1204.0, memory_length 2000, epsilon 0.006392421325853387, time 727.0, rides 114\n",
      "Initial State is  [4, 17, 2]\n",
      "episode 1008, reward 929.0, memory_length 2000, epsilon 0.00636045921922412, time 731.0, rides 120\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 1009, reward 1113.0, memory_length 2000, epsilon 0.006328656923128, time 729.0, rides 119\n",
      "Initial State is  [3, 11, 1]\n",
      "episode 1010, reward 818.0, memory_length 2000, epsilon 0.00629701363851236, time 736.0, rides 116\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 1011, reward 1233.0, memory_length 2000, epsilon 0.006265528570319798, time 727.0, rides 129\n",
      "Initial State is  [2, 4, 1]\n",
      "episode 1012, reward 1122.0, memory_length 2000, epsilon 0.006234200927468199, time 732.0, rides 117\n",
      "Initial State is  [2, 11, 6]\n",
      "episode 1013, reward 984.0, memory_length 2000, epsilon 0.006203029922830858, time 738.0, rides 112\n",
      "Initial State is  [0, 21, 2]\n",
      "episode 1014, reward 1139.0, memory_length 2000, epsilon 0.006172014773216704, time 723.0, rides 119\n",
      "Initial State is  [1, 14, 5]\n",
      "episode 1015, reward 1148.0, memory_length 2000, epsilon 0.006141154699350621, time 727.0, rides 114\n",
      "Initial State is  [3, 12, 0]\n",
      "episode 1016, reward 1177.0, memory_length 2000, epsilon 0.006110448925853868, time 727.0, rides 116\n",
      "Initial State is  [4, 9, 0]\n",
      "episode 1017, reward 1197.0, memory_length 2000, epsilon 0.006079896681224598, time 728.0, rides 124\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 1018, reward 821.0, memory_length 2000, epsilon 0.006049497197818475, time 728.0, rides 115\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 1019, reward 1050.0, memory_length 2000, epsilon 0.006019249711829383, time 731.0, rides 116\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 1020, reward 1331.0, memory_length 2000, epsilon 0.005989153463270236, time 727.0, rides 117\n",
      "Initial State is  [4, 21, 2]\n",
      "episode 1021, reward 1129.0, memory_length 2000, epsilon 0.005959207695953885, time 728.0, rides 119\n",
      "Initial State is  [0, 17, 2]\n",
      "episode 1022, reward 1334.0, memory_length 2000, epsilon 0.005929411657474116, time 734.0, rides 127\n",
      "Initial State is  [4, 19, 6]\n",
      "episode 1023, reward 957.0, memory_length 2000, epsilon 0.005899764599186745, time 721.0, rides 120\n",
      "Initial State is  [4, 4, 1]\n",
      "episode 1024, reward 1326.0, memory_length 2000, epsilon 0.005870265776190812, time 724.0, rides 117\n",
      "Initial State is  [2, 19, 1]\n",
      "episode 1025, reward 1439.0, memory_length 2000, epsilon 0.0058409144473098576, time 730.0, rides 120\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 1026, reward 1281.0, memory_length 2000, epsilon 0.005811709875073308, time 729.0, rides 122\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 1027, reward 1117.0, memory_length 2000, epsilon 0.0057826513256979415, time 724.0, rides 125\n",
      "Initial State is  [4, 0, 5]\n",
      "episode 1028, reward 1080.0, memory_length 2000, epsilon 0.005753738069069452, time 728.0, rides 121\n",
      "Initial State is  [4, 15, 5]\n",
      "episode 1029, reward 1139.0, memory_length 2000, epsilon 0.005724969378724105, time 729.0, rides 119\n",
      "Initial State is  [1, 10, 6]\n",
      "episode 1030, reward 1059.0, memory_length 2000, epsilon 0.005696344531830484, time 726.0, rides 132\n",
      "Initial State is  [2, 12, 5]\n",
      "episode 1031, reward 1213.0, memory_length 2000, epsilon 0.005667862809171332, time 721.0, rides 131\n",
      "Initial State is  [0, 15, 0]\n",
      "episode 1032, reward 1158.0, memory_length 2000, epsilon 0.005639523495125475, time 730.0, rides 125\n",
      "Initial State is  [3, 6, 0]\n",
      "episode 1033, reward 1123.0, memory_length 2000, epsilon 0.005611325877649847, time 734.0, rides 111\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 1034, reward 1291.0, memory_length 2000, epsilon 0.005583269248261598, time 730.0, rides 124\n",
      "Initial State is  [2, 4, 2]\n",
      "episode 1035, reward 746.0, memory_length 2000, epsilon 0.00555535290202029, time 731.0, rides 108\n",
      "Initial State is  [0, 6, 5]\n",
      "episode 1036, reward 1072.0, memory_length 2000, epsilon 0.005527576137510188, time 722.0, rides 112\n",
      "Initial State is  [0, 5, 0]\n",
      "episode 1037, reward 969.0, memory_length 2000, epsilon 0.005499938256822637, time 732.0, rides 119\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 1038, reward 883.0, memory_length 2000, epsilon 0.005472438565538524, time 729.0, rides 103\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 1039, reward 1061.0, memory_length 2000, epsilon 0.005445076372710831, time 736.0, rides 106\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 1040, reward 1331.0, memory_length 2000, epsilon 0.005417850990847277, time 727.0, rides 119\n",
      "Initial State is  [1, 2, 0]\n",
      "episode 1041, reward 1210.0, memory_length 2000, epsilon 0.00539076173589304, time 731.0, rides 119\n",
      "Initial State is  [4, 17, 6]\n",
      "episode 1042, reward 730.0, memory_length 2000, epsilon 0.005363807927213575, time 734.0, rides 106\n",
      "Initial State is  [1, 15, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1043, reward 946.0, memory_length 2000, epsilon 0.005336988887577507, time 729.0, rides 127\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 1044, reward 908.0, memory_length 2000, epsilon 0.00531030394313962, time 730.0, rides 112\n",
      "Initial State is  [3, 23, 5]\n",
      "episode 1045, reward 1180.0, memory_length 2000, epsilon 0.005283752423423922, time 725.0, rides 110\n",
      "Initial State is  [0, 3, 6]\n",
      "episode 1046, reward 1210.0, memory_length 2000, epsilon 0.005257333661306802, time 733.0, rides 120\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 1047, reward 888.0, memory_length 2000, epsilon 0.005231046993000268, time 728.0, rides 119\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 1048, reward 905.0, memory_length 2000, epsilon 0.005204891758035267, time 725.0, rides 114\n",
      "Initial State is  [3, 23, 3]\n",
      "episode 1049, reward 1088.0, memory_length 2000, epsilon 0.005178867299245091, time 728.0, rides 113\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 1050, reward 1226.0, memory_length 2000, epsilon 0.0051529729627488655, time 724.0, rides 117\n",
      "Initial State is  [3, 5, 2]\n",
      "episode 1051, reward 868.0, memory_length 2000, epsilon 0.005127208097935121, time 738.0, rides 119\n",
      "Initial State is  [0, 12, 2]\n",
      "episode 1052, reward 1321.0, memory_length 2000, epsilon 0.0051015720574454455, time 731.0, rides 121\n",
      "Initial State is  [2, 4, 2]\n",
      "episode 1053, reward 1181.0, memory_length 2000, epsilon 0.0050760641971582185, time 723.0, rides 131\n",
      "Initial State is  [0, 23, 0]\n",
      "episode 1054, reward 929.0, memory_length 2000, epsilon 0.005050683876172427, time 727.0, rides 114\n",
      "Initial State is  [3, 15, 6]\n",
      "episode 1055, reward 1133.0, memory_length 2000, epsilon 0.0050254304567915655, time 727.0, rides 119\n",
      "Initial State is  [0, 22, 3]\n",
      "episode 1056, reward 1316.0, memory_length 2000, epsilon 0.005000303304507608, time 731.0, rides 115\n",
      "Initial State is  [3, 22, 1]\n",
      "episode 1057, reward 851.0, memory_length 2000, epsilon 0.004975301787985069, time 721.0, rides 117\n",
      "Initial State is  [4, 22, 3]\n",
      "episode 1058, reward 911.0, memory_length 2000, epsilon 0.004950425279045144, time 727.0, rides 115\n",
      "Initial State is  [3, 15, 0]\n",
      "episode 1059, reward 807.0, memory_length 2000, epsilon 0.004925673152649918, time 730.0, rides 125\n",
      "Initial State is  [0, 13, 1]\n",
      "episode 1060, reward 1126.0, memory_length 2000, epsilon 0.004901044786886668, time 725.0, rides 115\n",
      "Initial State is  [0, 5, 0]\n",
      "episode 1061, reward 1176.0, memory_length 2000, epsilon 0.004876539562952234, time 739.0, rides 129\n",
      "Initial State is  [4, 7, 4]\n",
      "episode 1062, reward 937.0, memory_length 2000, epsilon 0.004852156865137473, time 724.0, rides 116\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 1063, reward 1160.0, memory_length 2000, epsilon 0.004827896080811785, time 729.0, rides 133\n",
      "Initial State is  [4, 13, 3]\n",
      "episode 1064, reward 976.0, memory_length 2000, epsilon 0.004803756600407726, time 723.0, rides 127\n",
      "Initial State is  [1, 8, 0]\n",
      "episode 1065, reward 1142.0, memory_length 2000, epsilon 0.0047797378174056875, time 737.0, rides 114\n",
      "Initial State is  [1, 14, 2]\n",
      "episode 1066, reward 1131.0, memory_length 2000, epsilon 0.004755839128318659, time 732.0, rides 117\n",
      "Initial State is  [4, 3, 3]\n",
      "episode 1067, reward 1210.0, memory_length 2000, epsilon 0.004732059932677066, time 721.0, rides 127\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 1068, reward 951.0, memory_length 2000, epsilon 0.004708399633013681, time 728.0, rides 121\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 1069, reward 942.0, memory_length 2000, epsilon 0.004684857634848613, time 727.0, rides 116\n",
      "Initial State is  [0, 12, 6]\n",
      "episode 1070, reward 1001.0, memory_length 2000, epsilon 0.004661433346674369, time 735.0, rides 129\n",
      "Initial State is  [1, 4, 5]\n",
      "episode 1071, reward 927.0, memory_length 2000, epsilon 0.004638126179940997, time 725.0, rides 105\n",
      "Initial State is  [4, 12, 1]\n",
      "episode 1072, reward 1162.0, memory_length 2000, epsilon 0.004614935549041292, time 731.0, rides 125\n",
      "Initial State is  [0, 3, 6]\n",
      "episode 1073, reward 1033.0, memory_length 2000, epsilon 0.004591860871296085, time 728.0, rides 119\n",
      "Initial State is  [1, 5, 6]\n",
      "episode 1074, reward 973.0, memory_length 2000, epsilon 0.004568901566939605, time 726.0, rides 122\n",
      "Initial State is  [4, 15, 0]\n",
      "episode 1075, reward 807.0, memory_length 2000, epsilon 0.004546057059104907, time 724.0, rides 113\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 1076, reward 1022.0, memory_length 2000, epsilon 0.004523326773809382, time 731.0, rides 119\n",
      "Initial State is  [0, 8, 5]\n",
      "episode 1077, reward 1135.0, memory_length 2000, epsilon 0.004500710139940335, time 733.0, rides 110\n",
      "Initial State is  [2, 21, 4]\n",
      "episode 1078, reward 1325.0, memory_length 2000, epsilon 0.004478206589240633, time 727.0, rides 120\n",
      "Initial State is  [3, 3, 3]\n",
      "episode 1079, reward 929.0, memory_length 2000, epsilon 0.00445581555629443, time 731.0, rides 105\n",
      "Initial State is  [4, 3, 5]\n",
      "episode 1080, reward 1379.0, memory_length 2000, epsilon 0.004433536478512958, time 728.0, rides 121\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 1081, reward 1181.0, memory_length 2000, epsilon 0.004411368796120392, time 731.0, rides 113\n",
      "Initial State is  [1, 22, 4]\n",
      "episode 1082, reward 1178.0, memory_length 2000, epsilon 0.0043893119521397905, time 728.0, rides 109\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 1083, reward 1273.0, memory_length 2000, epsilon 0.004367365392379092, time 728.0, rides 127\n",
      "Initial State is  [4, 7, 2]\n",
      "episode 1084, reward 993.0, memory_length 2000, epsilon 0.004345528565417196, time 734.0, rides 117\n",
      "Initial State is  [0, 20, 1]\n",
      "episode 1085, reward 1133.0, memory_length 2000, epsilon 0.00432380092259011, time 726.0, rides 117\n",
      "Initial State is  [4, 9, 5]\n",
      "episode 1086, reward 954.0, memory_length 2000, epsilon 0.00430218191797716, time 727.0, rides 107\n",
      "Initial State is  [2, 16, 4]\n",
      "episode 1087, reward 1166.0, memory_length 2000, epsilon 0.004280671008387274, time 732.0, rides 113\n",
      "Initial State is  [3, 22, 4]\n",
      "episode 1088, reward 1128.0, memory_length 2000, epsilon 0.004259267653345338, time 731.0, rides 114\n",
      "Initial State is  [2, 21, 0]\n",
      "episode 1089, reward 1150.0, memory_length 2000, epsilon 0.004237971315078611, time 732.0, rides 129\n",
      "Initial State is  [2, 23, 0]\n",
      "episode 1090, reward 1312.0, memory_length 2000, epsilon 0.004216781458503218, time 731.0, rides 117\n",
      "Initial State is  [1, 1, 2]\n",
      "episode 1091, reward 1448.0, memory_length 2000, epsilon 0.004195697551210702, time 728.0, rides 111\n",
      "Initial State is  [1, 9, 3]\n",
      "episode 1092, reward 715.0, memory_length 2000, epsilon 0.004174719063454648, time 730.0, rides 113\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 1093, reward 1068.0, memory_length 2000, epsilon 0.004153845468137375, time 725.0, rides 121\n",
      "Initial State is  [3, 10, 3]\n",
      "episode 1094, reward 1065.0, memory_length 2000, epsilon 0.004133076240796688, time 730.0, rides 110\n",
      "Initial State is  [3, 12, 0]\n",
      "episode 1095, reward 891.0, memory_length 2000, epsilon 0.004112410859592705, time 729.0, rides 125\n",
      "Initial State is  [3, 14, 5]\n",
      "episode 1096, reward 997.0, memory_length 2000, epsilon 0.004091848805294741, time 723.0, rides 116\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 1097, reward 1146.0, memory_length 2000, epsilon 0.004071389561268267, time 727.0, rides 114\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 1098, reward 1206.0, memory_length 2000, epsilon 0.004051032613461926, time 734.0, rides 113\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 1099, reward 1071.0, memory_length 2000, epsilon 0.004030777450394616, time 723.0, rides 112\n",
      "Initial State is  [0, 19, 0]\n",
      "episode 1100, reward 908.0, memory_length 2000, epsilon 0.004010623563142642, time 740.0, rides 111\n",
      "Initial State is  [2, 19, 1]\n",
      "episode 1101, reward 1464.0, memory_length 2000, epsilon 0.0039905704453269296, time 723.0, rides 119\n",
      "Initial State is  [3, 19, 2]\n",
      "episode 1102, reward 1305.0, memory_length 2000, epsilon 0.003970617593100295, time 738.0, rides 117\n",
      "Initial State is  [3, 10, 4]\n",
      "episode 1103, reward 1003.0, memory_length 2000, epsilon 0.003950764505134793, time 728.0, rides 126\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 1104, reward 732.0, memory_length 2000, epsilon 0.003931010682609119, time 734.0, rides 115\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 1105, reward 1164.0, memory_length 2000, epsilon 0.003911355629196074, time 735.0, rides 115\n",
      "Initial State is  [2, 3, 6]\n",
      "episode 1106, reward 1387.0, memory_length 2000, epsilon 0.0038917988510500934, time 728.0, rides 123\n",
      "Initial State is  [2, 5, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1107, reward 991.0, memory_length 2000, epsilon 0.003872339856794843, time 731.0, rides 115\n",
      "Initial State is  [1, 5, 6]\n",
      "episode 1108, reward 1057.0, memory_length 2000, epsilon 0.0038529781575108685, time 731.0, rides 116\n",
      "Initial State is  [2, 18, 4]\n",
      "episode 1109, reward 1083.0, memory_length 2000, epsilon 0.003833713266723314, time 724.0, rides 114\n",
      "Initial State is  [2, 13, 3]\n",
      "episode 1110, reward 1133.0, memory_length 2000, epsilon 0.003814544700389697, time 729.0, rides 111\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 1111, reward 1216.0, memory_length 2000, epsilon 0.0037954719768877486, time 727.0, rides 112\n",
      "Initial State is  [2, 1, 1]\n",
      "episode 1112, reward 1280.0, memory_length 2000, epsilon 0.0037764946170033096, time 734.0, rides 128\n",
      "Initial State is  [2, 1, 0]\n",
      "episode 1113, reward 1123.0, memory_length 2000, epsilon 0.003757612143918293, time 727.0, rides 122\n",
      "Initial State is  [1, 0, 3]\n",
      "episode 1114, reward 1187.0, memory_length 2000, epsilon 0.003738824083198702, time 727.0, rides 124\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 1115, reward 1134.0, memory_length 2000, epsilon 0.003720129962782708, time 734.0, rides 113\n",
      "Initial State is  [3, 14, 5]\n",
      "episode 1116, reward 1171.0, memory_length 2000, epsilon 0.0037015293129687944, time 730.0, rides 112\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 1117, reward 1286.0, memory_length 2000, epsilon 0.0036830216664039505, time 730.0, rides 115\n",
      "Initial State is  [0, 12, 6]\n",
      "episode 1118, reward 1356.0, memory_length 2000, epsilon 0.0036646065580719306, time 731.0, rides 127\n",
      "Initial State is  [3, 5, 0]\n",
      "episode 1119, reward 1128.0, memory_length 2000, epsilon 0.003646283525281571, time 731.0, rides 116\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 1120, reward 1102.0, memory_length 2000, epsilon 0.003628052107655163, time 729.0, rides 119\n",
      "Initial State is  [0, 8, 2]\n",
      "episode 1121, reward 949.0, memory_length 2000, epsilon 0.0036099118471168874, time 727.0, rides 113\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 1122, reward 1067.0, memory_length 2000, epsilon 0.003591862287881303, time 727.0, rides 124\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 1123, reward 1175.0, memory_length 2000, epsilon 0.0035739029764418964, time 726.0, rides 112\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 1124, reward 1055.0, memory_length 2000, epsilon 0.003556033461559687, time 735.0, rides 118\n",
      "Initial State is  [0, 23, 4]\n",
      "episode 1125, reward 896.0, memory_length 2000, epsilon 0.0035382532942518884, time 727.0, rides 110\n",
      "Initial State is  [0, 17, 2]\n",
      "episode 1126, reward 1305.0, memory_length 2000, epsilon 0.003520562027780629, time 732.0, rides 111\n",
      "Initial State is  [2, 21, 4]\n",
      "episode 1127, reward 1047.0, memory_length 2000, epsilon 0.0035029592176417258, time 732.0, rides 114\n",
      "Initial State is  [1, 0, 1]\n",
      "episode 1128, reward 956.0, memory_length 2000, epsilon 0.0034854444215535172, time 725.0, rides 116\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 1129, reward 1333.0, memory_length 2000, epsilon 0.0034680171994457497, time 730.0, rides 122\n",
      "Initial State is  [2, 4, 4]\n",
      "episode 1130, reward 1147.0, memory_length 2000, epsilon 0.003450677113448521, time 732.0, rides 115\n",
      "Initial State is  [4, 5, 6]\n",
      "episode 1131, reward 952.0, memory_length 2000, epsilon 0.0034334237278812784, time 729.0, rides 108\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 1132, reward 840.0, memory_length 2000, epsilon 0.003416256609241872, time 735.0, rides 107\n",
      "Initial State is  [2, 20, 3]\n",
      "episode 1133, reward 1170.0, memory_length 2000, epsilon 0.003399175326195663, time 726.0, rides 114\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 1134, reward 1208.0, memory_length 2000, epsilon 0.0033821794495646844, time 731.0, rides 119\n",
      "Initial State is  [4, 4, 0]\n",
      "episode 1135, reward 1143.0, memory_length 2000, epsilon 0.003365268552316861, time 727.0, rides 119\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 1136, reward 925.0, memory_length 2000, epsilon 0.0033484422095552764, time 729.0, rides 114\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 1137, reward 1076.0, memory_length 2000, epsilon 0.0033316999985075002, time 722.0, rides 116\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 1138, reward 1161.0, memory_length 2000, epsilon 0.0033150414985149627, time 725.0, rides 125\n",
      "Initial State is  [3, 15, 2]\n",
      "episode 1139, reward 1248.0, memory_length 2000, epsilon 0.003298466291022388, time 728.0, rides 125\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 1140, reward 1207.0, memory_length 2000, epsilon 0.003281973959567276, time 733.0, rides 116\n",
      "Initial State is  [0, 23, 2]\n",
      "episode 1141, reward 813.0, memory_length 2000, epsilon 0.0032655640897694396, time 730.0, rides 122\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 1142, reward 1069.0, memory_length 2000, epsilon 0.0032492362693205923, time 727.0, rides 115\n",
      "Initial State is  [3, 20, 6]\n",
      "episode 1143, reward 1022.0, memory_length 2000, epsilon 0.0032329900879739895, time 722.0, rides 121\n",
      "Initial State is  [4, 4, 6]\n",
      "episode 1144, reward 1100.0, memory_length 2000, epsilon 0.0032168251375341195, time 726.0, rides 115\n",
      "Initial State is  [0, 2, 5]\n",
      "episode 1145, reward 985.0, memory_length 2000, epsilon 0.0032007410118464487, time 730.0, rides 109\n",
      "Initial State is  [0, 0, 2]\n",
      "episode 1146, reward 924.0, memory_length 2000, epsilon 0.0031847373067872163, time 730.0, rides 125\n",
      "Initial State is  [3, 22, 6]\n",
      "episode 1147, reward 1264.0, memory_length 2000, epsilon 0.00316881362025328, time 729.0, rides 117\n",
      "Initial State is  [1, 9, 6]\n",
      "episode 1148, reward 1348.0, memory_length 2000, epsilon 0.0031529695521520136, time 734.0, rides 128\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 1149, reward 1562.0, memory_length 2000, epsilon 0.0031372047043912534, time 731.0, rides 120\n",
      "Initial State is  [1, 16, 5]\n",
      "episode 1150, reward 842.0, memory_length 2000, epsilon 0.0031215186808692973, time 730.0, rides 120\n",
      "Initial State is  [4, 10, 3]\n",
      "episode 1151, reward 902.0, memory_length 2000, epsilon 0.003105911087464951, time 722.0, rides 110\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 1152, reward 1159.0, memory_length 2000, epsilon 0.003090381532027626, time 732.0, rides 131\n",
      "Initial State is  [1, 10, 3]\n",
      "episode 1153, reward 1235.0, memory_length 2000, epsilon 0.003074929624367488, time 724.0, rides 113\n",
      "Initial State is  [2, 22, 1]\n",
      "episode 1154, reward 1122.0, memory_length 2000, epsilon 0.0030595549762456506, time 731.0, rides 119\n",
      "Initial State is  [3, 16, 5]\n",
      "episode 1155, reward 1233.0, memory_length 2000, epsilon 0.0030442572013644224, time 737.0, rides 122\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 1156, reward 1258.0, memory_length 2000, epsilon 0.0030290359153576003, time 730.0, rides 123\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 1157, reward 1029.0, memory_length 2000, epsilon 0.003013890735780812, time 729.0, rides 121\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 1158, reward 906.0, memory_length 2000, epsilon 0.002998821282101908, time 733.0, rides 108\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 1159, reward 990.0, memory_length 2000, epsilon 0.002983827175691398, time 721.0, rides 117\n",
      "Initial State is  [3, 18, 1]\n",
      "episode 1160, reward 1179.0, memory_length 2000, epsilon 0.0029689080398129413, time 730.0, rides 112\n",
      "Initial State is  [3, 20, 6]\n",
      "episode 1161, reward 1249.0, memory_length 2000, epsilon 0.0029540634996138766, time 727.0, rides 122\n",
      "Initial State is  [2, 13, 1]\n",
      "episode 1162, reward 1278.0, memory_length 2000, epsilon 0.002939293182115807, time 730.0, rides 121\n",
      "Initial State is  [0, 20, 1]\n",
      "episode 1163, reward 1220.0, memory_length 2000, epsilon 0.002924596716205228, time 733.0, rides 121\n",
      "Initial State is  [1, 10, 3]\n",
      "episode 1164, reward 990.0, memory_length 2000, epsilon 0.002909973732624202, time 734.0, rides 120\n",
      "Initial State is  [1, 14, 2]\n",
      "episode 1165, reward 1358.0, memory_length 2000, epsilon 0.002895423863961081, time 728.0, rides 132\n",
      "Initial State is  [1, 2, 2]\n",
      "episode 1166, reward 1198.0, memory_length 2000, epsilon 0.0028809467446412754, time 732.0, rides 131\n",
      "Initial State is  [3, 5, 2]\n",
      "episode 1167, reward 1113.0, memory_length 2000, epsilon 0.002866542010918069, time 741.0, rides 105\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 1168, reward 1132.0, memory_length 2000, epsilon 0.0028522093008634787, time 730.0, rides 117\n",
      "Initial State is  [4, 11, 5]\n",
      "episode 1169, reward 1051.0, memory_length 2000, epsilon 0.002837948254359161, time 723.0, rides 122\n",
      "Initial State is  [3, 17, 1]\n",
      "episode 1170, reward 1118.0, memory_length 2000, epsilon 0.0028237585130873656, time 735.0, rides 122\n",
      "Initial State is  [3, 1, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1171, reward 1073.0, memory_length 2000, epsilon 0.0028096397205219286, time 730.0, rides 113\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 1172, reward 1228.0, memory_length 2000, epsilon 0.002795591521919319, time 729.0, rides 123\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 1173, reward 961.0, memory_length 2000, epsilon 0.0027816135643097223, time 725.0, rides 119\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 1174, reward 885.0, memory_length 2000, epsilon 0.0027677054964881736, time 732.0, rides 123\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 1175, reward 1036.0, memory_length 2000, epsilon 0.0027538669690057326, time 733.0, rides 108\n",
      "Initial State is  [4, 14, 3]\n",
      "episode 1176, reward 1314.0, memory_length 2000, epsilon 0.002740097634160704, time 733.0, rides 125\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 1177, reward 860.0, memory_length 2000, epsilon 0.0027263971459899005, time 735.0, rides 123\n",
      "Initial State is  [3, 21, 2]\n",
      "episode 1178, reward 1287.0, memory_length 2000, epsilon 0.002712765160259951, time 720.0, rides 110\n",
      "Initial State is  [2, 22, 5]\n",
      "episode 1179, reward 1130.0, memory_length 2000, epsilon 0.0026992013344586513, time 735.0, rides 121\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 1180, reward 982.0, memory_length 2000, epsilon 0.002685705327786358, time 728.0, rides 105\n",
      "Initial State is  [3, 22, 0]\n",
      "episode 1181, reward 927.0, memory_length 2000, epsilon 0.002672276801147426, time 733.0, rides 114\n",
      "Initial State is  [3, 0, 0]\n",
      "episode 1182, reward 1232.0, memory_length 2000, epsilon 0.002658915417141689, time 733.0, rides 119\n",
      "Initial State is  [2, 18, 5]\n",
      "episode 1183, reward 1054.0, memory_length 2000, epsilon 0.0026456208400559805, time 725.0, rides 126\n",
      "Initial State is  [3, 14, 2]\n",
      "episode 1184, reward 1143.0, memory_length 2000, epsilon 0.0026323927358557005, time 727.0, rides 130\n",
      "Initial State is  [2, 9, 0]\n",
      "episode 1185, reward 1144.0, memory_length 2000, epsilon 0.002619230772176422, time 721.0, rides 124\n",
      "Initial State is  [2, 16, 3]\n",
      "episode 1186, reward 1112.0, memory_length 2000, epsilon 0.00260613461831554, time 728.0, rides 122\n",
      "Initial State is  [2, 12, 5]\n",
      "episode 1187, reward 1228.0, memory_length 2000, epsilon 0.0025931039452239623, time 739.0, rides 122\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 1188, reward 1003.0, memory_length 2000, epsilon 0.0025801384254978423, time 736.0, rides 114\n",
      "Initial State is  [2, 11, 6]\n",
      "episode 1189, reward 946.0, memory_length 2000, epsilon 0.002567237733370353, time 732.0, rides 128\n",
      "Initial State is  [3, 20, 0]\n",
      "episode 1190, reward 1022.0, memory_length 2000, epsilon 0.0025544015447035015, time 734.0, rides 126\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 1191, reward 1304.0, memory_length 2000, epsilon 0.002541629536979984, time 727.0, rides 114\n",
      "Initial State is  [4, 6, 5]\n",
      "episode 1192, reward 1366.0, memory_length 2000, epsilon 0.002528921389295084, time 728.0, rides 121\n",
      "Initial State is  [1, 4, 1]\n",
      "episode 1193, reward 1164.0, memory_length 2000, epsilon 0.0025162767823486087, time 735.0, rides 112\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 1194, reward 898.0, memory_length 2000, epsilon 0.0025036953984368658, time 730.0, rides 126\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 1195, reward 1197.0, memory_length 2000, epsilon 0.0024911769214446813, time 732.0, rides 113\n",
      "Initial State is  [2, 12, 2]\n",
      "episode 1196, reward 1414.0, memory_length 2000, epsilon 0.0024787210368374577, time 735.0, rides 122\n",
      "Initial State is  [3, 9, 1]\n",
      "episode 1197, reward 1309.0, memory_length 2000, epsilon 0.0024663274316532703, time 737.0, rides 126\n",
      "Initial State is  [1, 9, 0]\n",
      "episode 1198, reward 1043.0, memory_length 2000, epsilon 0.002453995794495004, time 727.0, rides 116\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 1199, reward 1150.0, memory_length 2000, epsilon 0.002441725815522529, time 735.0, rides 124\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 1200, reward 1265.0, memory_length 2000, epsilon 0.0024295171864449163, time 723.0, rides 126\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 1201, reward 1267.0, memory_length 2000, epsilon 0.0024173696005126916, time 728.0, rides 120\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 1202, reward 1184.0, memory_length 2000, epsilon 0.0024052827525101283, time 734.0, rides 121\n",
      "Initial State is  [1, 13, 0]\n",
      "episode 1203, reward 1514.0, memory_length 2000, epsilon 0.0023932563387475776, time 730.0, rides 118\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 1204, reward 827.0, memory_length 2000, epsilon 0.00238129005705384, time 727.0, rides 115\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 1205, reward 1181.0, memory_length 2000, epsilon 0.002369383606768571, time 727.0, rides 113\n",
      "Initial State is  [2, 21, 1]\n",
      "episode 1206, reward 1066.0, memory_length 2000, epsilon 0.002357536688734728, time 730.0, rides 113\n",
      "Initial State is  [2, 7, 5]\n",
      "episode 1207, reward 748.0, memory_length 2000, epsilon 0.0023457490052910543, time 728.0, rides 112\n",
      "Initial State is  [2, 22, 3]\n",
      "episode 1208, reward 856.0, memory_length 2000, epsilon 0.002334020260264599, time 732.0, rides 116\n",
      "Initial State is  [0, 13, 1]\n",
      "episode 1209, reward 1075.0, memory_length 2000, epsilon 0.002322350158963276, time 735.0, rides 110\n",
      "Initial State is  [0, 23, 2]\n",
      "episode 1210, reward 1096.0, memory_length 2000, epsilon 0.0023107384081684596, time 730.0, rides 114\n",
      "Initial State is  [4, 21, 0]\n",
      "episode 1211, reward 1212.0, memory_length 2000, epsilon 0.0022991847161276174, time 721.0, rides 123\n",
      "Initial State is  [4, 20, 2]\n",
      "episode 1212, reward 1271.0, memory_length 2000, epsilon 0.0022876887925469794, time 728.0, rides 114\n",
      "Initial State is  [2, 19, 6]\n",
      "episode 1213, reward 1077.0, memory_length 2000, epsilon 0.0022762503485842444, time 741.0, rides 108\n",
      "Initial State is  [3, 0, 5]\n",
      "episode 1214, reward 796.0, memory_length 2000, epsilon 0.0022648690968413232, time 733.0, rides 126\n",
      "Initial State is  [4, 22, 4]\n",
      "episode 1215, reward 1013.0, memory_length 2000, epsilon 0.0022535447513571164, time 733.0, rides 124\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 1216, reward 1061.0, memory_length 2000, epsilon 0.002242277027600331, time 727.0, rides 109\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 1217, reward 1216.0, memory_length 2000, epsilon 0.0022310656424623294, time 733.0, rides 126\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 1218, reward 1191.0, memory_length 2000, epsilon 0.002219910314250018, time 725.0, rides 111\n",
      "Initial State is  [3, 20, 6]\n",
      "episode 1219, reward 1037.0, memory_length 2000, epsilon 0.0022088107626787677, time 732.0, rides 116\n",
      "Initial State is  [1, 16, 5]\n",
      "episode 1220, reward 872.0, memory_length 2000, epsilon 0.002197766708865374, time 725.0, rides 116\n",
      "Initial State is  [4, 3, 5]\n",
      "episode 1221, reward 1500.0, memory_length 2000, epsilon 0.002186777875321047, time 732.0, rides 122\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 1222, reward 1173.0, memory_length 2000, epsilon 0.0021758439859444418, time 729.0, rides 123\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 1223, reward 1062.0, memory_length 2000, epsilon 0.0021649647660147197, time 725.0, rides 119\n",
      "Initial State is  [2, 11, 5]\n",
      "episode 1224, reward 1303.0, memory_length 2000, epsilon 0.002154139942184646, time 733.0, rides 117\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 1225, reward 854.0, memory_length 2000, epsilon 0.002143369242473723, time 721.0, rides 113\n",
      "Initial State is  [3, 15, 6]\n",
      "episode 1226, reward 1005.0, memory_length 2000, epsilon 0.0021326523962613545, time 731.0, rides 109\n",
      "Initial State is  [1, 4, 5]\n",
      "episode 1227, reward 883.0, memory_length 2000, epsilon 0.002121989134280048, time 739.0, rides 128\n",
      "Initial State is  [4, 22, 6]\n",
      "episode 1228, reward 1149.0, memory_length 2000, epsilon 0.002111379188608648, time 735.0, rides 114\n",
      "Initial State is  [3, 19, 2]\n",
      "episode 1229, reward 1236.0, memory_length 2000, epsilon 0.0021008222926656044, time 740.0, rides 118\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 1230, reward 1218.0, memory_length 2000, epsilon 0.0020903181812022766, time 728.0, rides 120\n",
      "Initial State is  [2, 17, 3]\n",
      "episode 1231, reward 1395.0, memory_length 2000, epsilon 0.002079866590296265, time 732.0, rides 113\n",
      "Initial State is  [2, 16, 0]\n",
      "episode 1232, reward 1144.0, memory_length 2000, epsilon 0.0020694672573447837, time 729.0, rides 126\n",
      "Initial State is  [3, 23, 1]\n",
      "episode 1233, reward 1100.0, memory_length 2000, epsilon 0.0020591199210580596, time 730.0, rides 123\n",
      "Initial State is  [1, 3, 6]\n",
      "episode 1234, reward 1304.0, memory_length 2000, epsilon 0.0020488243214527692, time 734.0, rides 119\n",
      "Initial State is  [4, 18, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1235, reward 1161.0, memory_length 2000, epsilon 0.0020385801998455055, time 728.0, rides 116\n",
      "Initial State is  [3, 21, 2]\n",
      "episode 1236, reward 956.0, memory_length 2000, epsilon 0.002028387298846278, time 734.0, rides 113\n",
      "Initial State is  [3, 7, 3]\n",
      "episode 1237, reward 794.0, memory_length 2000, epsilon 0.0020182453623520465, time 738.0, rides 110\n",
      "Initial State is  [2, 16, 6]\n",
      "episode 1238, reward 1082.0, memory_length 2000, epsilon 0.0020081541355402863, time 728.0, rides 107\n",
      "Initial State is  [0, 0, 5]\n",
      "episode 1239, reward 1234.0, memory_length 2000, epsilon 0.0019981133648625847, time 742.0, rides 124\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 1240, reward 1074.0, memory_length 2000, epsilon 0.001988122798038272, time 733.0, rides 118\n",
      "Initial State is  [4, 9, 5]\n",
      "episode 1241, reward 1244.0, memory_length 2000, epsilon 0.0019781821840480804, time 724.0, rides 118\n",
      "Initial State is  [3, 21, 3]\n",
      "episode 1242, reward 1274.0, memory_length 2000, epsilon 0.00196829127312784, time 721.0, rides 112\n",
      "Initial State is  [1, 1, 5]\n",
      "episode 1243, reward 1120.0, memory_length 2000, epsilon 0.0019584498167622005, time 730.0, rides 124\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 1244, reward 946.0, memory_length 2000, epsilon 0.0019486575676783894, time 735.0, rides 118\n",
      "Initial State is  [1, 7, 3]\n",
      "episode 1245, reward 1457.0, memory_length 2000, epsilon 0.0019389142798399974, time 726.0, rides 140\n",
      "Initial State is  [3, 7, 3]\n",
      "episode 1246, reward 978.0, memory_length 2000, epsilon 0.0019292197084407974, time 730.0, rides 119\n",
      "Initial State is  [3, 15, 4]\n",
      "episode 1247, reward 878.0, memory_length 2000, epsilon 0.0019195736098985934, time 732.0, rides 118\n",
      "Initial State is  [4, 4, 2]\n",
      "episode 1248, reward 1083.0, memory_length 2000, epsilon 0.0019099757418491003, time 729.0, rides 121\n",
      "Initial State is  [1, 10, 2]\n",
      "episode 1249, reward 820.0, memory_length 2000, epsilon 0.0019004258631398548, time 724.0, rides 114\n",
      "Initial State is  [3, 13, 4]\n",
      "episode 1250, reward 1421.0, memory_length 2000, epsilon 0.0018909237338241556, time 729.0, rides 121\n",
      "Initial State is  [4, 19, 1]\n",
      "episode 1251, reward 1065.0, memory_length 2000, epsilon 0.0018814691151550348, time 731.0, rides 121\n",
      "Initial State is  [3, 10, 3]\n",
      "episode 1252, reward 1126.0, memory_length 2000, epsilon 0.0018720617695792596, time 736.0, rides 121\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 1253, reward 1167.0, memory_length 2000, epsilon 0.0018627014607313632, time 735.0, rides 123\n",
      "Initial State is  [1, 22, 2]\n",
      "episode 1254, reward 1301.0, memory_length 2000, epsilon 0.0018533879534277063, time 728.0, rides 115\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 1255, reward 952.0, memory_length 2000, epsilon 0.0018441210136605677, time 725.0, rides 117\n",
      "Initial State is  [4, 12, 4]\n",
      "episode 1256, reward 1210.0, memory_length 2000, epsilon 0.0018349004085922648, time 726.0, rides 121\n",
      "Initial State is  [4, 20, 3]\n",
      "episode 1257, reward 1055.0, memory_length 2000, epsilon 0.0018257259065493036, time 724.0, rides 113\n",
      "Initial State is  [3, 15, 6]\n",
      "episode 1258, reward 1204.0, memory_length 2000, epsilon 0.0018165972770165571, time 731.0, rides 115\n",
      "Initial State is  [3, 22, 3]\n",
      "episode 1259, reward 1052.0, memory_length 2000, epsilon 0.0018075142906314743, time 728.0, rides 117\n",
      "Initial State is  [1, 13, 2]\n",
      "episode 1260, reward 1139.0, memory_length 2000, epsilon 0.001798476719178317, time 728.0, rides 122\n",
      "Initial State is  [2, 13, 2]\n",
      "episode 1261, reward 1101.0, memory_length 2000, epsilon 0.0017894843355824254, time 728.0, rides 113\n",
      "Initial State is  [1, 9, 6]\n",
      "episode 1262, reward 1154.0, memory_length 2000, epsilon 0.0017805369139045131, time 726.0, rides 116\n",
      "Initial State is  [1, 0, 1]\n",
      "episode 1263, reward 1257.0, memory_length 2000, epsilon 0.0017716342293349905, time 726.0, rides 120\n",
      "Initial State is  [3, 11, 3]\n",
      "episode 1264, reward 1051.0, memory_length 2000, epsilon 0.0017627760581883155, time 736.0, rides 122\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 1265, reward 1478.0, memory_length 2000, epsilon 0.0017539621778973739, time 734.0, rides 120\n",
      "Initial State is  [4, 20, 5]\n",
      "episode 1266, reward 997.0, memory_length 2000, epsilon 0.001745192367007887, time 729.0, rides 121\n",
      "Initial State is  [2, 15, 5]\n",
      "episode 1267, reward 817.0, memory_length 2000, epsilon 0.0017364664051728476, time 724.0, rides 118\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 1268, reward 1118.0, memory_length 2000, epsilon 0.0017277840731469835, time 726.0, rides 120\n",
      "Initial State is  [0, 3, 6]\n",
      "episode 1269, reward 1307.0, memory_length 2000, epsilon 0.0017191451527812486, time 737.0, rides 117\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 1270, reward 1397.0, memory_length 2000, epsilon 0.0017105494270173423, time 735.0, rides 115\n",
      "Initial State is  [1, 18, 0]\n",
      "episode 1271, reward 774.0, memory_length 2000, epsilon 0.0017019966798822556, time 725.0, rides 124\n",
      "Initial State is  [1, 6, 0]\n",
      "episode 1272, reward 1493.0, memory_length 2000, epsilon 0.0016934866964828444, time 726.0, rides 123\n",
      "Initial State is  [0, 9, 3]\n",
      "episode 1273, reward 1017.0, memory_length 2000, epsilon 0.00168501926300043, time 728.0, rides 110\n",
      "Initial State is  [4, 19, 1]\n",
      "episode 1274, reward 1134.0, memory_length 2000, epsilon 0.0016765941666854278, time 728.0, rides 127\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 1275, reward 1378.0, memory_length 2000, epsilon 0.0016682111958520006, time 742.0, rides 113\n",
      "Initial State is  [4, 13, 1]\n",
      "episode 1276, reward 1192.0, memory_length 2000, epsilon 0.0016598701398727405, time 739.0, rides 127\n",
      "Initial State is  [2, 16, 6]\n",
      "episode 1277, reward 1069.0, memory_length 2000, epsilon 0.0016515707891733768, time 731.0, rides 121\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 1278, reward 1116.0, memory_length 2000, epsilon 0.0016433129352275099, time 731.0, rides 107\n",
      "Initial State is  [0, 3, 2]\n",
      "episode 1279, reward 1336.0, memory_length 2000, epsilon 0.0016350963705513723, time 729.0, rides 121\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 1280, reward 1141.0, memory_length 2000, epsilon 0.0016269208886986154, time 735.0, rides 120\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 1281, reward 1234.0, memory_length 2000, epsilon 0.0016187862842551224, time 727.0, rides 122\n",
      "Initial State is  [3, 8, 2]\n",
      "episode 1282, reward 993.0, memory_length 2000, epsilon 0.0016106923528338467, time 728.0, rides 118\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 1283, reward 1166.0, memory_length 2000, epsilon 0.0016026388910696774, time 731.0, rides 129\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 1284, reward 1284.0, memory_length 2000, epsilon 0.001594625696614329, time 730.0, rides 117\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 1285, reward 1049.0, memory_length 2000, epsilon 0.0015866525681312573, time 734.0, rides 115\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 1286, reward 908.0, memory_length 2000, epsilon 0.001578719305290601, time 726.0, rides 111\n",
      "Initial State is  [0, 6, 3]\n",
      "episode 1287, reward 1116.0, memory_length 2000, epsilon 0.0015708257087641482, time 722.0, rides 113\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 1288, reward 1160.0, memory_length 2000, epsilon 0.0015629715802203273, time 723.0, rides 120\n",
      "Initial State is  [2, 16, 6]\n",
      "episode 1289, reward 1309.0, memory_length 2000, epsilon 0.0015551567223192257, time 736.0, rides 113\n",
      "Initial State is  [0, 5, 3]\n",
      "episode 1290, reward 966.0, memory_length 2000, epsilon 0.0015473809387076295, time 728.0, rides 125\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 1291, reward 1135.0, memory_length 2000, epsilon 0.0015396440340140914, time 726.0, rides 114\n",
      "Initial State is  [0, 6, 2]\n",
      "episode 1292, reward 1210.0, memory_length 2000, epsilon 0.0015319458138440209, time 727.0, rides 122\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 1293, reward 1305.0, memory_length 2000, epsilon 0.0015242860847748008, time 733.0, rides 125\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 1294, reward 900.0, memory_length 2000, epsilon 0.0015166646543509268, time 726.0, rides 111\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 1295, reward 1309.0, memory_length 2000, epsilon 0.0015090813310791721, time 730.0, rides 120\n",
      "Initial State is  [1, 14, 6]\n",
      "episode 1296, reward 1246.0, memory_length 2000, epsilon 0.0015015359244237763, time 728.0, rides 118\n",
      "Initial State is  [3, 10, 4]\n",
      "episode 1297, reward 1049.0, memory_length 2000, epsilon 0.0014940282448016574, time 728.0, rides 121\n",
      "Initial State is  [4, 16, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1298, reward 1273.0, memory_length 2000, epsilon 0.001486558103577649, time 732.0, rides 118\n",
      "Initial State is  [0, 6, 3]\n",
      "episode 1299, reward 1137.0, memory_length 2000, epsilon 0.0014791253130597608, time 735.0, rides 111\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 1300, reward 1109.0, memory_length 2000, epsilon 0.001471729686494462, time 726.0, rides 119\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 1301, reward 1078.0, memory_length 2000, epsilon 0.0014643710380619897, time 734.0, rides 120\n",
      "Initial State is  [2, 8, 0]\n",
      "episode 1302, reward 873.0, memory_length 2000, epsilon 0.0014570491828716798, time 733.0, rides 119\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 1303, reward 1203.0, memory_length 2000, epsilon 0.0014497639369573214, time 728.0, rides 113\n",
      "Initial State is  [1, 13, 6]\n",
      "episode 1304, reward 1241.0, memory_length 2000, epsilon 0.0014425151172725347, time 726.0, rides 125\n",
      "Initial State is  [4, 19, 3]\n",
      "episode 1305, reward 1061.0, memory_length 2000, epsilon 0.001435302541686172, time 730.0, rides 112\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 1306, reward 907.0, memory_length 2000, epsilon 0.0014281260289777413, time 731.0, rides 120\n",
      "Initial State is  [3, 11, 5]\n",
      "episode 1307, reward 937.0, memory_length 2000, epsilon 0.0014209853988328526, time 730.0, rides 119\n",
      "Initial State is  [3, 23, 5]\n",
      "episode 1308, reward 975.0, memory_length 2000, epsilon 0.0014138804718386883, time 727.0, rides 106\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 1309, reward 1124.0, memory_length 2000, epsilon 0.0014068110694794949, time 730.0, rides 125\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 1310, reward 1289.0, memory_length 2000, epsilon 0.0013997770141320975, time 736.0, rides 121\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 1311, reward 1036.0, memory_length 2000, epsilon 0.001392778129061437, time 738.0, rides 115\n",
      "Initial State is  [2, 18, 6]\n",
      "episode 1312, reward 1219.0, memory_length 2000, epsilon 0.00138581423841613, time 736.0, rides 118\n",
      "Initial State is  [4, 14, 3]\n",
      "episode 1313, reward 877.0, memory_length 2000, epsilon 0.0013788851672240493, time 727.0, rides 114\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 1314, reward 931.0, memory_length 2000, epsilon 0.0013719907413879291, time 731.0, rides 117\n",
      "Initial State is  [4, 18, 4]\n",
      "episode 1315, reward 1272.0, memory_length 2000, epsilon 0.0013651307876809894, time 733.0, rides 129\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 1316, reward 1258.0, memory_length 2000, epsilon 0.0013583051337425845, time 731.0, rides 112\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 1317, reward 1184.0, memory_length 2000, epsilon 0.0013515136080738716, time 729.0, rides 123\n",
      "Initial State is  [3, 8, 2]\n",
      "episode 1318, reward 1326.0, memory_length 2000, epsilon 0.0013447560400335022, time 731.0, rides 120\n",
      "Initial State is  [3, 13, 0]\n",
      "episode 1319, reward 870.0, memory_length 2000, epsilon 0.0013380322598333348, time 723.0, rides 109\n",
      "Initial State is  [1, 4, 4]\n",
      "episode 1320, reward 1160.0, memory_length 2000, epsilon 0.0013313420985341681, time 728.0, rides 123\n",
      "Initial State is  [1, 19, 3]\n",
      "episode 1321, reward 1252.0, memory_length 2000, epsilon 0.0013246853880414973, time 730.0, rides 121\n",
      "Initial State is  [4, 5, 3]\n",
      "episode 1322, reward 1316.0, memory_length 2000, epsilon 0.0013180619611012898, time 727.0, rides 122\n",
      "Initial State is  [4, 12, 0]\n",
      "episode 1323, reward 1156.0, memory_length 2000, epsilon 0.0013114716512957834, time 733.0, rides 125\n",
      "Initial State is  [0, 0, 2]\n",
      "episode 1324, reward 1187.0, memory_length 2000, epsilon 0.0013049142930393045, time 732.0, rides 119\n",
      "Initial State is  [1, 21, 6]\n",
      "episode 1325, reward 1069.0, memory_length 2000, epsilon 0.001298389721574108, time 734.0, rides 115\n",
      "Initial State is  [0, 21, 1]\n",
      "episode 1326, reward 907.0, memory_length 2000, epsilon 0.0012918977729662374, time 730.0, rides 107\n",
      "Initial State is  [1, 1, 1]\n",
      "episode 1327, reward 911.0, memory_length 2000, epsilon 0.0012854382841014063, time 740.0, rides 129\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 1328, reward 939.0, memory_length 2000, epsilon 0.0012790110926808992, time 729.0, rides 114\n",
      "Initial State is  [4, 7, 6]\n",
      "episode 1329, reward 909.0, memory_length 2000, epsilon 0.0012726160372174948, time 738.0, rides 116\n",
      "Initial State is  [2, 8, 6]\n",
      "episode 1330, reward 1132.0, memory_length 2000, epsilon 0.0012662529570314073, time 737.0, rides 114\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 1331, reward 1111.0, memory_length 2000, epsilon 0.0012599216922462503, time 729.0, rides 103\n",
      "Initial State is  [4, 0, 0]\n",
      "episode 1332, reward 1349.0, memory_length 2000, epsilon 0.001253622083785019, time 733.0, rides 116\n",
      "Initial State is  [3, 20, 4]\n",
      "episode 1333, reward 968.0, memory_length 2000, epsilon 0.001247353973366094, time 726.0, rides 117\n",
      "Initial State is  [3, 17, 5]\n",
      "episode 1334, reward 1154.0, memory_length 2000, epsilon 0.0012411172034992636, time 736.0, rides 121\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 1335, reward 1326.0, memory_length 2000, epsilon 0.0012349116174817673, time 726.0, rides 116\n",
      "Initial State is  [1, 22, 4]\n",
      "episode 1336, reward 1396.0, memory_length 2000, epsilon 0.0012287370593943585, time 728.0, rides 117\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 1337, reward 1027.0, memory_length 2000, epsilon 0.0012225933740973866, time 726.0, rides 113\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 1338, reward 1351.0, memory_length 2000, epsilon 0.0012164804072268998, time 731.0, rides 129\n",
      "Initial State is  [4, 15, 6]\n",
      "episode 1339, reward 1220.0, memory_length 2000, epsilon 0.0012103980051907653, time 731.0, rides 119\n",
      "Initial State is  [0, 15, 0]\n",
      "episode 1340, reward 980.0, memory_length 2000, epsilon 0.0012043460151648115, time 730.0, rides 122\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 1341, reward 1157.0, memory_length 2000, epsilon 0.0011983242850889873, time 735.0, rides 104\n",
      "Initial State is  [3, 11, 4]\n",
      "episode 1342, reward 1162.0, memory_length 2000, epsilon 0.0011923326636635425, time 731.0, rides 115\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 1343, reward 1084.0, memory_length 2000, epsilon 0.0011863710003452248, time 732.0, rides 124\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 1344, reward 898.0, memory_length 2000, epsilon 0.0011804391453434987, time 722.0, rides 121\n",
      "Initial State is  [1, 4, 5]\n",
      "episode 1345, reward 1007.0, memory_length 2000, epsilon 0.0011745369496167812, time 738.0, rides 119\n",
      "Initial State is  [4, 9, 0]\n",
      "episode 1346, reward 1048.0, memory_length 2000, epsilon 0.0011686642648686973, time 735.0, rides 121\n",
      "Initial State is  [2, 19, 6]\n",
      "episode 1347, reward 1041.0, memory_length 2000, epsilon 0.001162820943544354, time 725.0, rides 118\n",
      "Initial State is  [0, 9, 4]\n",
      "episode 1348, reward 972.0, memory_length 2000, epsilon 0.0011570068388266322, time 735.0, rides 113\n",
      "Initial State is  [3, 14, 1]\n",
      "episode 1349, reward 1009.0, memory_length 2000, epsilon 0.001151221804632499, time 728.0, rides 125\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 1350, reward 1073.0, memory_length 2000, epsilon 0.0011454656956093366, time 730.0, rides 111\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 1351, reward 948.0, memory_length 2000, epsilon 0.00113973836713129, time 731.0, rides 114\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 1352, reward 1136.0, memory_length 2000, epsilon 0.0011340396752956335, time 736.0, rides 126\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 1353, reward 1255.0, memory_length 2000, epsilon 0.0011283694769191554, time 736.0, rides 121\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 1354, reward 1051.0, memory_length 2000, epsilon 0.0011227276295345597, time 731.0, rides 129\n",
      "Initial State is  [3, 12, 4]\n",
      "episode 1355, reward 1077.0, memory_length 2000, epsilon 0.001117113991386887, time 722.0, rides 117\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 1356, reward 1477.0, memory_length 2000, epsilon 0.0011115284214299524, time 724.0, rides 118\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 1357, reward 1173.0, memory_length 2000, epsilon 0.0011059707793228026, time 731.0, rides 125\n",
      "Initial State is  [3, 9, 1]\n",
      "episode 1358, reward 1432.0, memory_length 2000, epsilon 0.0011004409254261886, time 728.0, rides 117\n",
      "Initial State is  [3, 4, 0]\n",
      "episode 1359, reward 1158.0, memory_length 2000, epsilon 0.0010949387207990578, time 726.0, rides 119\n",
      "Initial State is  [1, 16, 4]\n",
      "episode 1360, reward 1308.0, memory_length 2000, epsilon 0.0010894640271950624, time 733.0, rides 116\n",
      "Initial State is  [3, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1361, reward 972.0, memory_length 2000, epsilon 0.0010840167070590872, time 739.0, rides 121\n",
      "Initial State is  [4, 1, 0]\n",
      "episode 1362, reward 852.0, memory_length 2000, epsilon 0.0010785966235237919, time 734.0, rides 108\n",
      "Initial State is  [0, 1, 2]\n",
      "episode 1363, reward 890.0, memory_length 2000, epsilon 0.001073203640406173, time 725.0, rides 121\n",
      "Initial State is  [4, 15, 0]\n",
      "episode 1364, reward 854.0, memory_length 2000, epsilon 0.001067837622204142, time 731.0, rides 110\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 1365, reward 1211.0, memory_length 2000, epsilon 0.0010624984340931213, time 728.0, rides 111\n",
      "Initial State is  [0, 4, 5]\n",
      "episode 1366, reward 1187.0, memory_length 2000, epsilon 0.0010571859419226557, time 727.0, rides 121\n",
      "Initial State is  [3, 12, 3]\n",
      "episode 1367, reward 1063.0, memory_length 2000, epsilon 0.0010519000122130424, time 735.0, rides 113\n",
      "Initial State is  [4, 15, 0]\n",
      "episode 1368, reward 1242.0, memory_length 2000, epsilon 0.0010466405121519772, time 726.0, rides 121\n",
      "Initial State is  [1, 11, 1]\n",
      "episode 1369, reward 938.0, memory_length 2000, epsilon 0.0010414073095912173, time 728.0, rides 115\n",
      "Initial State is  [3, 20, 2]\n",
      "episode 1370, reward 1208.0, memory_length 2000, epsilon 0.001036200273043261, time 727.0, rides 136\n",
      "Initial State is  [0, 8, 6]\n",
      "episode 1371, reward 1104.0, memory_length 2000, epsilon 0.0010310192716780449, time 727.0, rides 112\n",
      "Initial State is  [3, 16, 0]\n",
      "episode 1372, reward 1230.0, memory_length 2000, epsilon 0.0010258641753196547, time 731.0, rides 116\n",
      "Initial State is  [4, 8, 4]\n",
      "episode 1373, reward 1072.0, memory_length 2000, epsilon 0.0010207348544430564, time 739.0, rides 114\n",
      "Initial State is  [3, 13, 4]\n",
      "episode 1374, reward 994.0, memory_length 2000, epsilon 0.001015631180170841, time 733.0, rides 123\n",
      "Initial State is  [4, 12, 1]\n",
      "episode 1375, reward 1096.0, memory_length 2000, epsilon 0.0010105530242699868, time 731.0, rides 118\n",
      "Initial State is  [4, 18, 2]\n",
      "episode 1376, reward 850.0, memory_length 2000, epsilon 0.001005500259148637, time 732.0, rides 116\n",
      "Initial State is  [1, 21, 5]\n",
      "episode 1377, reward 864.0, memory_length 2000, epsilon 0.0010004727578528938, time 726.0, rides 119\n",
      "Initial State is  [0, 5, 0]\n",
      "episode 1378, reward 867.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 110\n",
      "Initial State is  [2, 23, 1]\n",
      "episode 1379, reward 1292.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 132\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 1380, reward 1195.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 109\n",
      "Initial State is  [1, 3, 6]\n",
      "episode 1381, reward 1053.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 112\n",
      "Initial State is  [0, 22, 1]\n",
      "episode 1382, reward 940.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 113\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 1383, reward 1158.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 129\n",
      "Initial State is  [3, 21, 5]\n",
      "episode 1384, reward 976.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 121\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 1385, reward 1170.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 118\n",
      "Initial State is  [1, 16, 0]\n",
      "episode 1386, reward 1164.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 122\n",
      "Initial State is  [1, 22, 5]\n",
      "episode 1387, reward 1000.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 121\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 1388, reward 1439.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 133\n",
      "Initial State is  [4, 10, 3]\n",
      "episode 1389, reward 1296.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 125\n",
      "Initial State is  [2, 22, 5]\n",
      "episode 1390, reward 1185.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 111\n",
      "Initial State is  [1, 16, 1]\n",
      "episode 1391, reward 1181.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 111\n",
      "Initial State is  [0, 12, 3]\n",
      "episode 1392, reward 1190.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 120\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 1393, reward 1134.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 117\n",
      "Initial State is  [3, 11, 2]\n",
      "episode 1394, reward 1262.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 119\n",
      "Initial State is  [3, 12, 6]\n",
      "episode 1395, reward 1228.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 105\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 1396, reward 1109.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 127\n",
      "Initial State is  [2, 0, 5]\n",
      "episode 1397, reward 1150.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 115\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 1398, reward 922.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 116\n",
      "Initial State is  [2, 15, 0]\n",
      "episode 1399, reward 1247.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 125\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 1400, reward 1083.0, memory_length 2000, epsilon 0.0009954703940636294, time 741.0, rides 123\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 1401, reward 1065.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 116\n",
      "Initial State is  [4, 23, 1]\n",
      "episode 1402, reward 1222.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 123\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 1403, reward 1185.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 127\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 1404, reward 1178.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 117\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 1405, reward 1321.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 126\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 1406, reward 1134.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 119\n",
      "Initial State is  [4, 17, 5]\n",
      "episode 1407, reward 1193.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 120\n",
      "Initial State is  [2, 17, 0]\n",
      "episode 1408, reward 1225.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 130\n",
      "Initial State is  [2, 4, 1]\n",
      "episode 1409, reward 907.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 116\n",
      "Initial State is  [2, 21, 0]\n",
      "episode 1410, reward 852.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 105\n",
      "Initial State is  [0, 4, 1]\n",
      "episode 1411, reward 1000.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 113\n",
      "Initial State is  [4, 13, 6]\n",
      "episode 1412, reward 978.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 122\n",
      "Initial State is  [2, 5, 1]\n",
      "episode 1413, reward 919.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 122\n",
      "Initial State is  [1, 14, 5]\n",
      "episode 1414, reward 1053.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 124\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 1415, reward 688.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 119\n",
      "Initial State is  [4, 17, 6]\n",
      "episode 1416, reward 1109.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 115\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 1417, reward 1167.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 113\n",
      "Initial State is  [0, 4, 3]\n",
      "episode 1418, reward 1104.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 118\n",
      "Initial State is  [0, 19, 6]\n",
      "episode 1419, reward 1237.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 118\n",
      "Initial State is  [1, 12, 4]\n",
      "episode 1420, reward 1196.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 121\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 1421, reward 960.0, memory_length 2000, epsilon 0.0009954703940636294, time 741.0, rides 112\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 1422, reward 1195.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 119\n",
      "Initial State is  [4, 16, 1]\n",
      "episode 1423, reward 807.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 121\n",
      "Initial State is  [4, 1, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1424, reward 1630.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 117\n",
      "Initial State is  [0, 22, 6]\n",
      "episode 1425, reward 1221.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 112\n",
      "Initial State is  [2, 3, 5]\n",
      "episode 1426, reward 1027.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 123\n",
      "Initial State is  [4, 5, 3]\n",
      "episode 1427, reward 1209.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 125\n",
      "Initial State is  [2, 14, 0]\n",
      "episode 1428, reward 998.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 110\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 1429, reward 1087.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 122\n",
      "Initial State is  [3, 19, 6]\n",
      "episode 1430, reward 1333.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 130\n",
      "Initial State is  [3, 1, 6]\n",
      "episode 1431, reward 1015.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 117\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 1432, reward 971.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 111\n",
      "Initial State is  [1, 21, 1]\n",
      "episode 1433, reward 1569.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 127\n",
      "Initial State is  [3, 12, 0]\n",
      "episode 1434, reward 1545.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 118\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 1435, reward 1290.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 120\n",
      "Initial State is  [4, 5, 0]\n",
      "episode 1436, reward 1162.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 118\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 1437, reward 1047.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 115\n",
      "Initial State is  [2, 16, 3]\n",
      "episode 1438, reward 1146.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 117\n",
      "Initial State is  [0, 4, 0]\n",
      "episode 1439, reward 1175.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 119\n",
      "Initial State is  [0, 10, 2]\n",
      "episode 1440, reward 1369.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 118\n",
      "Initial State is  [2, 23, 1]\n",
      "episode 1441, reward 1240.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 118\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 1442, reward 1236.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 126\n",
      "Initial State is  [3, 10, 6]\n",
      "episode 1443, reward 1193.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 113\n",
      "Initial State is  [3, 16, 1]\n",
      "episode 1444, reward 974.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 129\n",
      "Initial State is  [2, 1, 1]\n",
      "episode 1445, reward 1143.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 116\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 1446, reward 1471.0, memory_length 2000, epsilon 0.0009954703940636294, time 739.0, rides 115\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 1447, reward 1202.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 112\n",
      "Initial State is  [1, 8, 1]\n",
      "episode 1448, reward 1219.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 125\n",
      "Initial State is  [4, 1, 0]\n",
      "episode 1449, reward 1132.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 125\n",
      "Initial State is  [4, 19, 5]\n",
      "episode 1450, reward 912.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 124\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 1451, reward 1020.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 117\n",
      "Initial State is  [4, 14, 5]\n",
      "episode 1452, reward 1117.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 116\n",
      "Initial State is  [2, 19, 6]\n",
      "episode 1453, reward 1102.0, memory_length 2000, epsilon 0.0009954703940636294, time 740.0, rides 113\n",
      "Initial State is  [4, 10, 6]\n",
      "episode 1454, reward 896.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 113\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 1455, reward 1224.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 117\n",
      "Initial State is  [2, 14, 2]\n",
      "episode 1456, reward 1250.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 121\n",
      "Initial State is  [2, 23, 4]\n",
      "episode 1457, reward 1142.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 118\n",
      "Initial State is  [1, 13, 4]\n",
      "episode 1458, reward 1109.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 128\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 1459, reward 846.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 119\n",
      "Initial State is  [1, 1, 1]\n",
      "episode 1460, reward 1371.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 121\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 1461, reward 1226.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 114\n",
      "Initial State is  [3, 14, 5]\n",
      "episode 1462, reward 1044.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 115\n",
      "Initial State is  [4, 19, 6]\n",
      "episode 1463, reward 1175.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 111\n",
      "Initial State is  [3, 20, 0]\n",
      "episode 1464, reward 1182.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 114\n",
      "Initial State is  [1, 21, 4]\n",
      "episode 1465, reward 1145.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 121\n",
      "Initial State is  [3, 3, 3]\n",
      "episode 1466, reward 1094.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 120\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 1467, reward 1022.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 132\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 1468, reward 1204.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 119\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 1469, reward 1045.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 128\n",
      "Initial State is  [0, 22, 3]\n",
      "episode 1470, reward 1112.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 113\n",
      "Initial State is  [2, 13, 3]\n",
      "episode 1471, reward 1200.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 121\n",
      "Initial State is  [4, 13, 5]\n",
      "episode 1472, reward 1063.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 117\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 1473, reward 1041.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 110\n",
      "Initial State is  [0, 21, 6]\n",
      "episode 1474, reward 1098.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 111\n",
      "Initial State is  [1, 4, 1]\n",
      "episode 1475, reward 1185.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 109\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 1476, reward 1154.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 117\n",
      "Initial State is  [1, 13, 4]\n",
      "episode 1477, reward 722.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 109\n",
      "Initial State is  [0, 11, 6]\n",
      "episode 1478, reward 1267.0, memory_length 2000, epsilon 0.0009954703940636294, time 741.0, rides 118\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 1479, reward 1183.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 125\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 1480, reward 1082.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 122\n",
      "Initial State is  [0, 18, 0]\n",
      "episode 1481, reward 911.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 124\n",
      "Initial State is  [4, 20, 5]\n",
      "episode 1482, reward 1310.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 115\n",
      "Initial State is  [2, 8, 0]\n",
      "episode 1483, reward 1052.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 122\n",
      "Initial State is  [3, 13, 0]\n",
      "episode 1484, reward 1074.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 119\n",
      "Initial State is  [2, 2, 4]\n",
      "episode 1485, reward 1277.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 124\n",
      "Initial State is  [3, 6, 3]\n",
      "episode 1486, reward 1088.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 115\n",
      "Initial State is  [2, 10, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1487, reward 1141.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 122\n",
      "Initial State is  [3, 5, 0]\n",
      "episode 1488, reward 837.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 108\n",
      "Initial State is  [4, 2, 3]\n",
      "episode 1489, reward 1107.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 108\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 1490, reward 1384.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 123\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 1491, reward 1206.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 124\n",
      "Initial State is  [2, 15, 6]\n",
      "episode 1492, reward 1038.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 115\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 1493, reward 785.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 108\n",
      "Initial State is  [1, 10, 0]\n",
      "episode 1494, reward 1102.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 114\n",
      "Initial State is  [1, 10, 1]\n",
      "episode 1495, reward 1148.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 110\n",
      "Initial State is  [0, 6, 5]\n",
      "episode 1496, reward 1259.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 122\n",
      "Initial State is  [0, 6, 5]\n",
      "episode 1497, reward 1127.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 109\n",
      "Initial State is  [0, 23, 2]\n",
      "episode 1498, reward 1037.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 117\n",
      "Initial State is  [1, 4, 5]\n",
      "episode 1499, reward 1115.0, memory_length 2000, epsilon 0.0009954703940636294, time 744.0, rides 110\n",
      "Initial State is  [0, 1, 6]\n",
      "episode 1500, reward 1457.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 114\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 1501, reward 1192.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 114\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 1502, reward 1004.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 122\n",
      "Initial State is  [4, 7, 6]\n",
      "episode 1503, reward 1220.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 123\n",
      "Initial State is  [3, 4, 5]\n",
      "episode 1504, reward 1084.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 124\n",
      "Initial State is  [0, 5, 3]\n",
      "episode 1505, reward 1036.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 117\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 1506, reward 1125.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 120\n",
      "Initial State is  [2, 23, 6]\n",
      "episode 1507, reward 1409.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 131\n",
      "Initial State is  [4, 1, 0]\n",
      "episode 1508, reward 1103.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 113\n",
      "Initial State is  [0, 11, 6]\n",
      "episode 1509, reward 1327.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 122\n",
      "Initial State is  [3, 10, 2]\n",
      "episode 1510, reward 1266.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 128\n",
      "Initial State is  [4, 23, 0]\n",
      "episode 1511, reward 1316.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 113\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 1512, reward 996.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 126\n",
      "Initial State is  [1, 20, 5]\n",
      "episode 1513, reward 1073.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 113\n",
      "Initial State is  [0, 9, 0]\n",
      "episode 1514, reward 1193.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 121\n",
      "Initial State is  [3, 19, 4]\n",
      "episode 1515, reward 1152.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 128\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 1516, reward 1221.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 118\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 1517, reward 1075.0, memory_length 2000, epsilon 0.0009954703940636294, time 740.0, rides 117\n",
      "Initial State is  [2, 4, 4]\n",
      "episode 1518, reward 762.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 106\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 1519, reward 1020.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 121\n",
      "Initial State is  [4, 15, 4]\n",
      "episode 1520, reward 931.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 108\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 1521, reward 1104.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 114\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 1522, reward 634.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 106\n",
      "Initial State is  [4, 14, 3]\n",
      "episode 1523, reward 1118.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 119\n",
      "Initial State is  [1, 12, 4]\n",
      "episode 1524, reward 1210.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 104\n",
      "Initial State is  [0, 16, 1]\n",
      "episode 1525, reward 1161.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 112\n",
      "Initial State is  [3, 3, 5]\n",
      "episode 1526, reward 1027.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 120\n",
      "Initial State is  [4, 1, 6]\n",
      "episode 1527, reward 1180.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 114\n",
      "Initial State is  [4, 15, 3]\n",
      "episode 1528, reward 984.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 120\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 1529, reward 1241.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 110\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 1530, reward 918.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 108\n",
      "Initial State is  [4, 21, 1]\n",
      "episode 1531, reward 1447.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 123\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 1532, reward 1103.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 112\n",
      "Initial State is  [4, 14, 2]\n",
      "episode 1533, reward 1452.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 124\n",
      "Initial State is  [0, 16, 1]\n",
      "episode 1534, reward 1207.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 112\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 1535, reward 1180.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 127\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 1536, reward 1075.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 119\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 1537, reward 1373.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 123\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 1538, reward 1247.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 117\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 1539, reward 1314.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 125\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 1540, reward 904.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 111\n",
      "Initial State is  [3, 20, 6]\n",
      "episode 1541, reward 1365.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 120\n",
      "Initial State is  [0, 18, 5]\n",
      "episode 1542, reward 1180.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 123\n",
      "Initial State is  [2, 20, 5]\n",
      "episode 1543, reward 1077.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 104\n",
      "Initial State is  [0, 13, 1]\n",
      "episode 1544, reward 1301.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 111\n",
      "Initial State is  [0, 1, 2]\n",
      "episode 1545, reward 1068.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 127\n",
      "Initial State is  [3, 1, 6]\n",
      "episode 1546, reward 1159.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 111\n",
      "Initial State is  [1, 17, 5]\n",
      "episode 1547, reward 1153.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 117\n",
      "Initial State is  [1, 15, 1]\n",
      "episode 1548, reward 1099.0, memory_length 2000, epsilon 0.0009954703940636294, time 745.0, rides 131\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 1549, reward 987.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 121\n",
      "Initial State is  [0, 0, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1550, reward 1264.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 120\n",
      "Initial State is  [1, 2, 3]\n",
      "episode 1551, reward 1253.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 126\n",
      "Initial State is  [2, 19, 0]\n",
      "episode 1552, reward 1023.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 118\n",
      "Initial State is  [4, 14, 6]\n",
      "episode 1553, reward 774.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 111\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 1554, reward 1317.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 121\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 1555, reward 1082.0, memory_length 2000, epsilon 0.0009954703940636294, time 739.0, rides 116\n",
      "Initial State is  [3, 17, 5]\n",
      "episode 1556, reward 1001.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 105\n",
      "Initial State is  [1, 3, 2]\n",
      "episode 1557, reward 1181.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 115\n",
      "Initial State is  [2, 23, 2]\n",
      "episode 1558, reward 770.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 119\n",
      "Initial State is  [1, 7, 3]\n",
      "episode 1559, reward 1095.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 119\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 1560, reward 999.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 123\n",
      "Initial State is  [3, 16, 1]\n",
      "episode 1561, reward 1094.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 128\n",
      "Initial State is  [0, 1, 1]\n",
      "episode 1562, reward 1283.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 109\n",
      "Initial State is  [2, 1, 1]\n",
      "episode 1563, reward 1112.0, memory_length 2000, epsilon 0.0009954703940636294, time 741.0, rides 123\n",
      "Initial State is  [0, 8, 2]\n",
      "episode 1564, reward 968.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 122\n",
      "Initial State is  [2, 22, 5]\n",
      "episode 1565, reward 947.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 120\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 1566, reward 1215.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 116\n",
      "Initial State is  [3, 12, 6]\n",
      "episode 1567, reward 992.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 113\n",
      "Initial State is  [0, 12, 2]\n",
      "episode 1568, reward 1122.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 120\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 1569, reward 1023.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 112\n",
      "Initial State is  [2, 13, 3]\n",
      "episode 1570, reward 1043.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 116\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 1571, reward 1353.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 121\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 1572, reward 1235.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 116\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 1573, reward 1189.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 123\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 1574, reward 1241.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 123\n",
      "Initial State is  [1, 21, 0]\n",
      "episode 1575, reward 961.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 118\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 1576, reward 1434.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 123\n",
      "Initial State is  [1, 1, 1]\n",
      "episode 1577, reward 1211.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 117\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 1578, reward 1373.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 129\n",
      "Initial State is  [0, 18, 0]\n",
      "episode 1579, reward 1205.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 118\n",
      "Initial State is  [3, 12, 2]\n",
      "episode 1580, reward 1071.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 122\n",
      "Initial State is  [3, 14, 2]\n",
      "episode 1581, reward 1066.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 109\n",
      "Initial State is  [1, 20, 6]\n",
      "episode 1582, reward 1188.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 127\n",
      "Initial State is  [3, 13, 0]\n",
      "episode 1583, reward 971.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 127\n",
      "Initial State is  [2, 22, 5]\n",
      "episode 1584, reward 1425.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 124\n",
      "Initial State is  [1, 1, 0]\n",
      "episode 1585, reward 963.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 114\n",
      "Initial State is  [0, 12, 1]\n",
      "episode 1586, reward 1282.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 110\n",
      "Initial State is  [1, 5, 3]\n",
      "episode 1587, reward 1125.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 107\n",
      "Initial State is  [2, 5, 6]\n",
      "episode 1588, reward 1151.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 115\n",
      "Initial State is  [2, 22, 4]\n",
      "episode 1589, reward 985.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 106\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 1590, reward 953.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 120\n",
      "Initial State is  [1, 22, 4]\n",
      "episode 1591, reward 1111.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 111\n",
      "Initial State is  [4, 1, 6]\n",
      "episode 1592, reward 1239.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 132\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 1593, reward 1228.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 115\n",
      "Initial State is  [3, 14, 1]\n",
      "episode 1594, reward 1041.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 119\n",
      "Initial State is  [2, 8, 0]\n",
      "episode 1595, reward 1174.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 120\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 1596, reward 1317.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 117\n",
      "Initial State is  [3, 2, 3]\n",
      "episode 1597, reward 1418.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 115\n",
      "Initial State is  [1, 21, 0]\n",
      "episode 1598, reward 1134.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 121\n",
      "Initial State is  [4, 11, 0]\n",
      "episode 1599, reward 923.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 123\n",
      "Initial State is  [3, 17, 4]\n",
      "episode 1600, reward 1255.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 124\n",
      "Initial State is  [0, 13, 4]\n",
      "episode 1601, reward 1019.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 109\n",
      "Initial State is  [0, 5, 0]\n",
      "episode 1602, reward 1144.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 116\n",
      "Initial State is  [4, 8, 6]\n",
      "episode 1603, reward 1030.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 122\n",
      "Initial State is  [2, 20, 0]\n",
      "episode 1604, reward 851.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 119\n",
      "Initial State is  [0, 3, 1]\n",
      "episode 1605, reward 1013.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 116\n",
      "Initial State is  [3, 9, 6]\n",
      "episode 1606, reward 955.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 120\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 1607, reward 773.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 117\n",
      "Initial State is  [4, 15, 6]\n",
      "episode 1608, reward 1078.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 120\n",
      "Initial State is  [3, 9, 3]\n",
      "episode 1609, reward 1401.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 124\n",
      "Initial State is  [0, 2, 0]\n",
      "episode 1610, reward 885.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 108\n",
      "Initial State is  [1, 13, 4]\n",
      "episode 1611, reward 993.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 108\n",
      "Initial State is  [0, 23, 5]\n",
      "episode 1612, reward 1045.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 107\n",
      "Initial State is  [3, 22, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1613, reward 1320.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 124\n",
      "Initial State is  [1, 19, 0]\n",
      "episode 1614, reward 983.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 117\n",
      "Initial State is  [2, 18, 1]\n",
      "episode 1615, reward 988.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 119\n",
      "Initial State is  [4, 14, 3]\n",
      "episode 1616, reward 1096.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 124\n",
      "Initial State is  [2, 11, 6]\n",
      "episode 1617, reward 979.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 111\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 1618, reward 961.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 113\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 1619, reward 980.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 114\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 1620, reward 1264.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 124\n",
      "Initial State is  [0, 4, 1]\n",
      "episode 1621, reward 841.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 123\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 1622, reward 840.0, memory_length 2000, epsilon 0.0009954703940636294, time 720.0, rides 114\n",
      "Initial State is  [3, 16, 2]\n",
      "episode 1623, reward 1211.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 118\n",
      "Initial State is  [0, 21, 2]\n",
      "episode 1624, reward 1148.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 116\n",
      "Initial State is  [4, 23, 0]\n",
      "episode 1625, reward 1441.0, memory_length 2000, epsilon 0.0009954703940636294, time 742.0, rides 116\n",
      "Initial State is  [3, 20, 6]\n",
      "episode 1626, reward 988.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 120\n",
      "Initial State is  [0, 2, 1]\n",
      "episode 1627, reward 1307.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 117\n",
      "Initial State is  [3, 23, 1]\n",
      "episode 1628, reward 1112.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 118\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 1629, reward 1094.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 109\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 1630, reward 1506.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 125\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 1631, reward 922.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 117\n",
      "Initial State is  [4, 14, 2]\n",
      "episode 1632, reward 815.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 122\n",
      "Initial State is  [4, 13, 3]\n",
      "episode 1633, reward 888.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 103\n",
      "Initial State is  [2, 5, 6]\n",
      "episode 1634, reward 1405.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 131\n",
      "Initial State is  [2, 14, 0]\n",
      "episode 1635, reward 1282.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 123\n",
      "Initial State is  [3, 12, 0]\n",
      "episode 1636, reward 1394.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 129\n",
      "Initial State is  [4, 22, 1]\n",
      "episode 1637, reward 1384.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 113\n",
      "Initial State is  [0, 0, 1]\n",
      "episode 1638, reward 1165.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 113\n",
      "Initial State is  [0, 13, 3]\n",
      "episode 1639, reward 1476.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 123\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 1640, reward 1292.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 120\n",
      "Initial State is  [0, 6, 3]\n",
      "episode 1641, reward 1196.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 118\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 1642, reward 1034.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 106\n",
      "Initial State is  [1, 13, 3]\n",
      "episode 1643, reward 1211.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 128\n",
      "Initial State is  [0, 1, 0]\n",
      "episode 1644, reward 1064.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 128\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 1645, reward 947.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 127\n",
      "Initial State is  [1, 1, 1]\n",
      "episode 1646, reward 1054.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 125\n",
      "Initial State is  [2, 11, 2]\n",
      "episode 1647, reward 1129.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 113\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 1648, reward 1145.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 126\n",
      "Initial State is  [1, 17, 2]\n",
      "episode 1649, reward 1046.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 118\n",
      "Initial State is  [1, 0, 1]\n",
      "episode 1650, reward 1046.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 126\n",
      "Initial State is  [4, 3, 3]\n",
      "episode 1651, reward 1244.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 128\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 1652, reward 768.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 119\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 1653, reward 1168.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 121\n",
      "Initial State is  [3, 10, 2]\n",
      "episode 1654, reward 894.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 112\n",
      "Initial State is  [1, 21, 5]\n",
      "episode 1655, reward 1367.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 114\n",
      "Initial State is  [0, 1, 5]\n",
      "episode 1656, reward 760.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 110\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 1657, reward 976.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 124\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 1658, reward 1148.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 131\n",
      "Initial State is  [4, 18, 4]\n",
      "episode 1659, reward 1212.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 115\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 1660, reward 1330.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 114\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 1661, reward 900.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 117\n",
      "Initial State is  [3, 8, 2]\n",
      "episode 1662, reward 1110.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 117\n",
      "Initial State is  [2, 14, 0]\n",
      "episode 1663, reward 1175.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 135\n",
      "Initial State is  [0, 20, 1]\n",
      "episode 1664, reward 938.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 114\n",
      "Initial State is  [4, 7, 6]\n",
      "episode 1665, reward 970.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 112\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 1666, reward 1050.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 119\n",
      "Initial State is  [0, 15, 0]\n",
      "episode 1667, reward 998.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 120\n",
      "Initial State is  [2, 2, 4]\n",
      "episode 1668, reward 1026.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 125\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 1669, reward 917.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 121\n",
      "Initial State is  [0, 4, 4]\n",
      "episode 1670, reward 1256.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 129\n",
      "Initial State is  [3, 20, 0]\n",
      "episode 1671, reward 1198.0, memory_length 2000, epsilon 0.0009954703940636294, time 741.0, rides 119\n",
      "Initial State is  [0, 8, 4]\n",
      "episode 1672, reward 1196.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 111\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 1673, reward 827.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 123\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 1674, reward 960.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 111\n",
      "Initial State is  [0, 18, 5]\n",
      "episode 1675, reward 935.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 124\n",
      "Initial State is  [2, 17, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1676, reward 1227.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 128\n",
      "Initial State is  [1, 6, 3]\n",
      "episode 1677, reward 892.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 109\n",
      "Initial State is  [2, 20, 5]\n",
      "episode 1678, reward 928.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 118\n",
      "Initial State is  [1, 11, 4]\n",
      "episode 1679, reward 856.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 103\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 1680, reward 1084.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 113\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 1681, reward 978.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 112\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 1682, reward 929.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 123\n",
      "Initial State is  [4, 4, 1]\n",
      "episode 1683, reward 1231.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 126\n",
      "Initial State is  [2, 15, 1]\n",
      "episode 1684, reward 938.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 110\n",
      "Initial State is  [4, 11, 2]\n",
      "episode 1685, reward 1068.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 113\n",
      "Initial State is  [4, 10, 1]\n",
      "episode 1686, reward 1522.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 120\n",
      "Initial State is  [2, 12, 6]\n",
      "episode 1687, reward 1453.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 119\n",
      "Initial State is  [1, 5, 6]\n",
      "episode 1688, reward 966.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 118\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 1689, reward 1124.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 118\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 1690, reward 1240.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 117\n",
      "Initial State is  [0, 12, 1]\n",
      "episode 1691, reward 1358.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 124\n",
      "Initial State is  [2, 14, 6]\n",
      "episode 1692, reward 831.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 123\n",
      "Initial State is  [4, 18, 2]\n",
      "episode 1693, reward 1322.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 115\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 1694, reward 1281.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 126\n",
      "Initial State is  [4, 0, 0]\n",
      "episode 1695, reward 992.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 118\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 1696, reward 1062.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 123\n",
      "Initial State is  [0, 7, 2]\n",
      "episode 1697, reward 1305.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 110\n",
      "Initial State is  [2, 5, 3]\n",
      "episode 1698, reward 1257.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 125\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 1699, reward 764.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 115\n",
      "Initial State is  [3, 8, 5]\n",
      "episode 1700, reward 1237.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 126\n",
      "Initial State is  [3, 6, 4]\n",
      "episode 1701, reward 853.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 113\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 1702, reward 1226.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 126\n",
      "Initial State is  [3, 8, 1]\n",
      "episode 1703, reward 1391.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 120\n",
      "Initial State is  [3, 5, 5]\n",
      "episode 1704, reward 1001.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 109\n",
      "Initial State is  [1, 22, 4]\n",
      "episode 1705, reward 1118.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 125\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 1706, reward 964.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 114\n",
      "Initial State is  [4, 23, 2]\n",
      "episode 1707, reward 1075.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 107\n",
      "Initial State is  [1, 5, 2]\n",
      "episode 1708, reward 1110.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 112\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 1709, reward 1272.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 126\n",
      "Initial State is  [4, 11, 0]\n",
      "episode 1710, reward 1087.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 115\n",
      "Initial State is  [0, 2, 5]\n",
      "episode 1711, reward 1133.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 112\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 1712, reward 923.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 113\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 1713, reward 1320.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 120\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 1714, reward 856.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 114\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 1715, reward 1258.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 116\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 1716, reward 1017.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 118\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 1717, reward 1151.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 120\n",
      "Initial State is  [3, 10, 1]\n",
      "episode 1718, reward 1269.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 120\n",
      "Initial State is  [4, 10, 6]\n",
      "episode 1719, reward 1254.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 114\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 1720, reward 832.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 105\n",
      "Initial State is  [3, 23, 4]\n",
      "episode 1721, reward 1150.0, memory_length 2000, epsilon 0.0009954703940636294, time 740.0, rides 125\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 1722, reward 1332.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 107\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 1723, reward 1133.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 122\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 1724, reward 1336.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 124\n",
      "Initial State is  [4, 1, 6]\n",
      "episode 1725, reward 1211.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 121\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 1726, reward 973.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 111\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 1727, reward 1220.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 113\n",
      "Initial State is  [3, 19, 4]\n",
      "episode 1728, reward 1204.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 113\n",
      "Initial State is  [0, 21, 4]\n",
      "episode 1729, reward 1435.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 113\n",
      "Initial State is  [4, 5, 6]\n",
      "episode 1730, reward 1137.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 114\n",
      "Initial State is  [2, 1, 3]\n",
      "episode 1731, reward 932.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 113\n",
      "Initial State is  [3, 10, 6]\n",
      "episode 1732, reward 1219.0, memory_length 2000, epsilon 0.0009954703940636294, time 739.0, rides 118\n",
      "Initial State is  [2, 11, 5]\n",
      "episode 1733, reward 1275.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 114\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 1734, reward 1202.0, memory_length 2000, epsilon 0.0009954703940636294, time 720.0, rides 127\n",
      "Initial State is  [3, 23, 6]\n",
      "episode 1735, reward 1013.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 115\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 1736, reward 876.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 124\n",
      "Initial State is  [1, 5, 4]\n",
      "episode 1737, reward 685.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 104\n",
      "Initial State is  [3, 22, 0]\n",
      "episode 1738, reward 1089.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 115\n",
      "Initial State is  [3, 23, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1739, reward 1199.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 126\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 1740, reward 1306.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 127\n",
      "Initial State is  [4, 5, 1]\n",
      "episode 1741, reward 1242.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 117\n",
      "Initial State is  [3, 4, 4]\n",
      "episode 1742, reward 1300.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 124\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 1743, reward 1063.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 114\n",
      "Initial State is  [1, 6, 0]\n",
      "episode 1744, reward 1151.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 117\n",
      "Initial State is  [2, 0, 2]\n",
      "episode 1745, reward 1024.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 117\n",
      "Initial State is  [2, 7, 6]\n",
      "episode 1746, reward 978.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 110\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 1747, reward 1157.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 118\n",
      "Initial State is  [2, 11, 1]\n",
      "episode 1748, reward 1075.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 115\n",
      "Initial State is  [2, 20, 3]\n",
      "episode 1749, reward 1193.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 126\n",
      "Initial State is  [1, 4, 6]\n",
      "episode 1750, reward 1318.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 122\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 1751, reward 1284.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 122\n",
      "Initial State is  [0, 18, 3]\n",
      "episode 1752, reward 1398.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 117\n",
      "Initial State is  [2, 10, 3]\n",
      "episode 1753, reward 933.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 118\n",
      "Initial State is  [2, 19, 0]\n",
      "episode 1754, reward 1065.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 128\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 1755, reward 1161.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 117\n",
      "Initial State is  [3, 15, 0]\n",
      "episode 1756, reward 1210.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 129\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 1757, reward 1359.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 104\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 1758, reward 1059.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 110\n",
      "Initial State is  [1, 3, 1]\n",
      "episode 1759, reward 1146.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 127\n",
      "Initial State is  [2, 8, 2]\n",
      "episode 1760, reward 1321.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 120\n",
      "Initial State is  [1, 4, 5]\n",
      "episode 1761, reward 1137.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 126\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 1762, reward 1197.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 117\n",
      "Initial State is  [2, 11, 5]\n",
      "episode 1763, reward 934.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 123\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 1764, reward 981.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 113\n",
      "Initial State is  [2, 19, 4]\n",
      "episode 1765, reward 963.0, memory_length 2000, epsilon 0.0009954703940636294, time 739.0, rides 120\n",
      "Initial State is  [4, 2, 6]\n",
      "episode 1766, reward 995.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 114\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 1767, reward 951.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 123\n",
      "Initial State is  [4, 9, 0]\n",
      "episode 1768, reward 1158.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 124\n",
      "Initial State is  [3, 0, 3]\n",
      "episode 1769, reward 1159.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 110\n",
      "Initial State is  [4, 7, 1]\n",
      "episode 1770, reward 1527.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 116\n",
      "Initial State is  [4, 14, 2]\n",
      "episode 1771, reward 1143.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 115\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 1772, reward 1052.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 126\n",
      "Initial State is  [2, 1, 1]\n",
      "episode 1773, reward 802.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 112\n",
      "Initial State is  [1, 2, 6]\n",
      "episode 1774, reward 1257.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 118\n",
      "Initial State is  [0, 12, 5]\n",
      "episode 1775, reward 1220.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 122\n",
      "Initial State is  [3, 15, 2]\n",
      "episode 1776, reward 1223.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 121\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 1777, reward 1044.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 115\n",
      "Initial State is  [4, 4, 3]\n",
      "episode 1778, reward 842.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 118\n",
      "Initial State is  [4, 20, 5]\n",
      "episode 1779, reward 1224.0, memory_length 2000, epsilon 0.0009954703940636294, time 740.0, rides 123\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 1780, reward 1094.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 122\n",
      "Initial State is  [2, 23, 5]\n",
      "episode 1781, reward 1255.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 127\n",
      "Initial State is  [4, 12, 2]\n",
      "episode 1782, reward 846.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 120\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 1783, reward 1017.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 112\n",
      "Initial State is  [0, 12, 3]\n",
      "episode 1784, reward 1061.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 130\n",
      "Initial State is  [1, 18, 1]\n",
      "episode 1785, reward 1052.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 121\n",
      "Initial State is  [2, 8, 1]\n",
      "episode 1786, reward 1376.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 133\n",
      "Initial State is  [4, 3, 3]\n",
      "episode 1787, reward 1012.0, memory_length 2000, epsilon 0.0009954703940636294, time 740.0, rides 116\n",
      "Initial State is  [4, 4, 3]\n",
      "episode 1788, reward 906.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 118\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 1789, reward 1564.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 122\n",
      "Initial State is  [0, 4, 1]\n",
      "episode 1790, reward 1345.0, memory_length 2000, epsilon 0.0009954703940636294, time 739.0, rides 132\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 1791, reward 1333.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 120\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 1792, reward 1241.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 127\n",
      "Initial State is  [3, 20, 0]\n",
      "episode 1793, reward 1061.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 123\n",
      "Initial State is  [1, 6, 1]\n",
      "episode 1794, reward 1050.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 116\n",
      "Initial State is  [0, 5, 2]\n",
      "episode 1795, reward 1368.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 126\n",
      "Initial State is  [2, 9, 3]\n",
      "episode 1796, reward 1323.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 122\n",
      "Initial State is  [0, 23, 2]\n",
      "episode 1797, reward 1227.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 129\n",
      "Initial State is  [2, 3, 2]\n",
      "episode 1798, reward 1051.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 110\n",
      "Initial State is  [0, 21, 6]\n",
      "episode 1799, reward 1177.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 109\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 1800, reward 876.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 106\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 1801, reward 997.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 115\n",
      "Initial State is  [1, 6, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1802, reward 923.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 123\n",
      "Initial State is  [1, 5, 1]\n",
      "episode 1803, reward 1323.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 120\n",
      "Initial State is  [4, 0, 4]\n",
      "episode 1804, reward 1105.0, memory_length 2000, epsilon 0.0009954703940636294, time 740.0, rides 121\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 1805, reward 1213.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 123\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 1806, reward 1154.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 114\n",
      "Initial State is  [2, 17, 4]\n",
      "episode 1807, reward 1037.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 119\n",
      "Initial State is  [1, 12, 3]\n",
      "episode 1808, reward 1152.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 106\n",
      "Initial State is  [2, 3, 1]\n",
      "episode 1809, reward 1061.0, memory_length 2000, epsilon 0.0009954703940636294, time 739.0, rides 115\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 1810, reward 984.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 110\n",
      "Initial State is  [0, 12, 5]\n",
      "episode 1811, reward 1240.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 115\n",
      "Initial State is  [4, 19, 0]\n",
      "episode 1812, reward 1221.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 121\n",
      "Initial State is  [4, 10, 3]\n",
      "episode 1813, reward 1232.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 118\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 1814, reward 1127.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 118\n",
      "Initial State is  [3, 1, 0]\n",
      "episode 1815, reward 967.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 115\n",
      "Initial State is  [4, 12, 2]\n",
      "episode 1816, reward 1162.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 117\n",
      "Initial State is  [1, 17, 1]\n",
      "episode 1817, reward 1076.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 111\n",
      "Initial State is  [0, 16, 5]\n",
      "episode 1818, reward 923.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 113\n",
      "Initial State is  [2, 7, 2]\n",
      "episode 1819, reward 1305.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 119\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 1820, reward 1125.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 119\n",
      "Initial State is  [4, 16, 0]\n",
      "episode 1821, reward 988.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 111\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 1822, reward 938.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 120\n",
      "Initial State is  [4, 20, 3]\n",
      "episode 1823, reward 1232.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 119\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 1824, reward 1344.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 132\n",
      "Initial State is  [0, 16, 5]\n",
      "episode 1825, reward 986.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 111\n",
      "Initial State is  [4, 22, 3]\n",
      "episode 1826, reward 1504.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 122\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 1827, reward 1051.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 112\n",
      "Initial State is  [2, 14, 6]\n",
      "episode 1828, reward 1215.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 117\n",
      "Initial State is  [2, 13, 2]\n",
      "episode 1829, reward 1197.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 117\n",
      "Initial State is  [0, 18, 5]\n",
      "episode 1830, reward 1093.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 120\n",
      "Initial State is  [4, 4, 5]\n",
      "episode 1831, reward 753.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 117\n",
      "Initial State is  [2, 14, 5]\n",
      "episode 1832, reward 1244.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 123\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 1833, reward 1082.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 115\n",
      "Initial State is  [0, 0, 5]\n",
      "episode 1834, reward 1157.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 106\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 1835, reward 1436.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 118\n",
      "Initial State is  [1, 3, 2]\n",
      "episode 1836, reward 1284.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 121\n",
      "Initial State is  [4, 18, 4]\n",
      "episode 1837, reward 1032.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 123\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 1838, reward 989.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 112\n",
      "Initial State is  [2, 4, 3]\n",
      "episode 1839, reward 1214.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 117\n",
      "Initial State is  [4, 17, 5]\n",
      "episode 1840, reward 833.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 116\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 1841, reward 1139.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 123\n",
      "Initial State is  [4, 23, 1]\n",
      "episode 1842, reward 1261.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 117\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 1843, reward 1161.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 112\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 1844, reward 1017.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 125\n",
      "Initial State is  [0, 17, 1]\n",
      "episode 1845, reward 1202.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 116\n",
      "Initial State is  [3, 7, 3]\n",
      "episode 1846, reward 1124.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 116\n",
      "Initial State is  [0, 9, 4]\n",
      "episode 1847, reward 1125.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 117\n",
      "Initial State is  [1, 16, 5]\n",
      "episode 1848, reward 1314.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 122\n",
      "Initial State is  [2, 19, 1]\n",
      "episode 1849, reward 980.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 128\n",
      "Initial State is  [3, 22, 1]\n",
      "episode 1850, reward 1008.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 110\n",
      "Initial State is  [1, 5, 6]\n",
      "episode 1851, reward 951.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 119\n",
      "Initial State is  [0, 15, 0]\n",
      "episode 1852, reward 1006.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 121\n",
      "Initial State is  [4, 7, 1]\n",
      "episode 1853, reward 1246.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 114\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 1854, reward 1141.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 119\n",
      "Initial State is  [3, 8, 5]\n",
      "episode 1855, reward 1526.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 118\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 1856, reward 1195.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 124\n",
      "Initial State is  [4, 0, 6]\n",
      "episode 1857, reward 1340.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 114\n",
      "Initial State is  [1, 21, 2]\n",
      "episode 1858, reward 1069.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 127\n",
      "Initial State is  [4, 18, 6]\n",
      "episode 1859, reward 1119.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 115\n",
      "Initial State is  [4, 21, 5]\n",
      "episode 1860, reward 1260.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 121\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 1861, reward 934.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 118\n",
      "Initial State is  [3, 10, 6]\n",
      "episode 1862, reward 1100.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 115\n",
      "Initial State is  [1, 4, 6]\n",
      "episode 1863, reward 1031.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 119\n",
      "Initial State is  [0, 9, 2]\n",
      "episode 1864, reward 883.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 122\n",
      "Initial State is  [0, 19, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1865, reward 929.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 110\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 1866, reward 1252.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 116\n",
      "Initial State is  [3, 19, 4]\n",
      "episode 1867, reward 1006.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 115\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 1868, reward 1037.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 121\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 1869, reward 1242.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 113\n",
      "Initial State is  [4, 18, 5]\n",
      "episode 1870, reward 891.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 129\n",
      "Initial State is  [2, 11, 6]\n",
      "episode 1871, reward 784.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 109\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 1872, reward 1249.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 120\n",
      "Initial State is  [0, 21, 6]\n",
      "episode 1873, reward 1145.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 116\n",
      "Initial State is  [2, 0, 0]\n",
      "episode 1874, reward 1038.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 115\n",
      "Initial State is  [1, 1, 5]\n",
      "episode 1875, reward 1149.0, memory_length 2000, epsilon 0.0009954703940636294, time 739.0, rides 118\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 1876, reward 1101.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 126\n",
      "Initial State is  [4, 14, 4]\n",
      "episode 1877, reward 850.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 111\n",
      "Initial State is  [0, 16, 3]\n",
      "episode 1878, reward 1513.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 117\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 1879, reward 1279.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 120\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 1880, reward 1322.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 126\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 1881, reward 1054.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 122\n",
      "Initial State is  [0, 0, 3]\n",
      "episode 1882, reward 1294.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 122\n",
      "Initial State is  [4, 10, 1]\n",
      "episode 1883, reward 835.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 116\n",
      "Initial State is  [0, 19, 6]\n",
      "episode 1884, reward 931.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 132\n",
      "Initial State is  [2, 0, 3]\n",
      "episode 1885, reward 1116.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 125\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 1886, reward 1295.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 119\n",
      "Initial State is  [3, 5, 0]\n",
      "episode 1887, reward 1036.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 112\n",
      "Initial State is  [1, 11, 4]\n",
      "episode 1888, reward 1278.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 118\n",
      "Initial State is  [1, 21, 6]\n",
      "episode 1889, reward 1305.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 126\n",
      "Initial State is  [1, 16, 5]\n",
      "episode 1890, reward 1260.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 119\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 1891, reward 1027.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 115\n",
      "Initial State is  [2, 4, 2]\n",
      "episode 1892, reward 1331.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 122\n",
      "Initial State is  [2, 20, 4]\n",
      "episode 1893, reward 1049.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 118\n",
      "Initial State is  [2, 11, 6]\n",
      "episode 1894, reward 894.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 124\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 1895, reward 1447.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 129\n",
      "Initial State is  [4, 0, 6]\n",
      "episode 1896, reward 1223.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 116\n",
      "Initial State is  [0, 3, 1]\n",
      "episode 1897, reward 953.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 121\n",
      "Initial State is  [4, 16, 0]\n",
      "episode 1898, reward 962.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 112\n",
      "Initial State is  [2, 4, 2]\n",
      "episode 1899, reward 1087.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 115\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 1900, reward 1027.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 118\n",
      "Initial State is  [0, 16, 5]\n",
      "episode 1901, reward 813.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 119\n",
      "Initial State is  [3, 18, 4]\n",
      "episode 1902, reward 915.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 109\n",
      "Initial State is  [2, 4, 4]\n",
      "episode 1903, reward 1402.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 122\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 1904, reward 1377.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 118\n",
      "Initial State is  [3, 14, 1]\n",
      "episode 1905, reward 1519.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 126\n",
      "Initial State is  [3, 16, 3]\n",
      "episode 1906, reward 687.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 117\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 1907, reward 1238.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 120\n",
      "Initial State is  [4, 18, 6]\n",
      "episode 1908, reward 1074.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 121\n",
      "Initial State is  [1, 4, 5]\n",
      "episode 1909, reward 1132.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 114\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 1910, reward 1152.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 108\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 1911, reward 1414.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 136\n",
      "Initial State is  [0, 3, 1]\n",
      "episode 1912, reward 1473.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 117\n",
      "Initial State is  [3, 12, 6]\n",
      "episode 1913, reward 1210.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 123\n",
      "Initial State is  [1, 11, 4]\n",
      "episode 1914, reward 797.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 118\n",
      "Initial State is  [4, 9, 0]\n",
      "episode 1915, reward 1384.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 119\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 1916, reward 1064.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 123\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 1917, reward 1249.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 112\n",
      "Initial State is  [0, 19, 0]\n",
      "episode 1918, reward 1210.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 118\n",
      "Initial State is  [2, 9, 3]\n",
      "episode 1919, reward 1219.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 119\n",
      "Initial State is  [2, 8, 6]\n",
      "episode 1920, reward 1273.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 122\n",
      "Initial State is  [4, 8, 6]\n",
      "episode 1921, reward 1178.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 118\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 1922, reward 1220.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 117\n",
      "Initial State is  [0, 0, 2]\n",
      "episode 1923, reward 1189.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 125\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 1924, reward 954.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 122\n",
      "Initial State is  [2, 19, 6]\n",
      "episode 1925, reward 1041.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 121\n",
      "Initial State is  [1, 9, 4]\n",
      "episode 1926, reward 1068.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 116\n",
      "Initial State is  [4, 4, 1]\n",
      "episode 1927, reward 1100.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 134\n",
      "Initial State is  [4, 14, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1928, reward 962.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 117\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 1929, reward 1188.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 118\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 1930, reward 1187.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 116\n",
      "Initial State is  [2, 8, 6]\n",
      "episode 1931, reward 1102.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 123\n",
      "Initial State is  [0, 4, 0]\n",
      "episode 1932, reward 1127.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 117\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 1933, reward 1111.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 122\n",
      "Initial State is  [4, 5, 5]\n",
      "episode 1934, reward 1023.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 121\n",
      "Initial State is  [1, 1, 3]\n",
      "episode 1935, reward 1405.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 115\n",
      "Initial State is  [0, 3, 2]\n",
      "episode 1936, reward 1176.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 114\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 1937, reward 1275.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 127\n",
      "Initial State is  [4, 19, 3]\n",
      "episode 1938, reward 1331.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 136\n",
      "Initial State is  [2, 22, 4]\n",
      "episode 1939, reward 945.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 122\n",
      "Initial State is  [4, 23, 1]\n",
      "episode 1940, reward 1415.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 118\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 1941, reward 1104.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 122\n",
      "Initial State is  [2, 0, 3]\n",
      "episode 1942, reward 1067.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 116\n",
      "Initial State is  [4, 0, 0]\n",
      "episode 1943, reward 959.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 124\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 1944, reward 1058.0, memory_length 2000, epsilon 0.0009954703940636294, time 740.0, rides 121\n",
      "Initial State is  [1, 8, 1]\n",
      "episode 1945, reward 1107.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 113\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 1946, reward 1164.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 120\n",
      "Initial State is  [4, 15, 2]\n",
      "episode 1947, reward 898.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 104\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 1948, reward 966.0, memory_length 2000, epsilon 0.0009954703940636294, time 747.0, rides 107\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 1949, reward 1031.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 113\n",
      "Initial State is  [0, 17, 3]\n",
      "episode 1950, reward 1179.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 124\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 1951, reward 1309.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 137\n",
      "Initial State is  [0, 3, 6]\n",
      "episode 1952, reward 1355.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 121\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 1953, reward 1092.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 113\n",
      "Initial State is  [4, 3, 3]\n",
      "episode 1954, reward 1207.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 116\n",
      "Initial State is  [4, 17, 6]\n",
      "episode 1955, reward 928.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 113\n",
      "Initial State is  [2, 18, 1]\n",
      "episode 1956, reward 866.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 117\n",
      "Initial State is  [2, 0, 2]\n",
      "episode 1957, reward 1006.0, memory_length 2000, epsilon 0.0009954703940636294, time 743.0, rides 112\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 1958, reward 804.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 112\n",
      "Initial State is  [0, 5, 3]\n",
      "episode 1959, reward 812.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 118\n",
      "Initial State is  [4, 4, 4]\n",
      "episode 1960, reward 1055.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 124\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 1961, reward 1558.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 120\n",
      "Initial State is  [1, 0, 0]\n",
      "episode 1962, reward 1082.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 120\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 1963, reward 1199.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 113\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 1964, reward 1498.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 125\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 1965, reward 947.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 113\n",
      "Initial State is  [0, 17, 5]\n",
      "episode 1966, reward 934.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 118\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 1967, reward 1174.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 108\n",
      "Initial State is  [1, 5, 4]\n",
      "episode 1968, reward 996.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 118\n",
      "Initial State is  [0, 10, 2]\n",
      "episode 1969, reward 937.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 111\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 1970, reward 681.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 113\n",
      "Initial State is  [0, 6, 3]\n",
      "episode 1971, reward 930.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 112\n",
      "Initial State is  [2, 11, 4]\n",
      "episode 1972, reward 1013.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 110\n",
      "Initial State is  [4, 12, 5]\n",
      "episode 1973, reward 1265.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 116\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 1974, reward 1083.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 119\n",
      "Initial State is  [4, 15, 3]\n",
      "episode 1975, reward 1200.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 114\n",
      "Initial State is  [2, 13, 3]\n",
      "episode 1976, reward 932.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 102\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 1977, reward 1224.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 110\n",
      "Initial State is  [0, 4, 3]\n",
      "episode 1978, reward 916.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 127\n",
      "Initial State is  [2, 1, 0]\n",
      "episode 1979, reward 1068.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 123\n",
      "Initial State is  [0, 10, 5]\n",
      "episode 1980, reward 1091.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 118\n",
      "Initial State is  [3, 0, 3]\n",
      "episode 1981, reward 937.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 118\n",
      "Initial State is  [1, 1, 2]\n",
      "episode 1982, reward 1219.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 117\n",
      "Initial State is  [2, 23, 5]\n",
      "episode 1983, reward 1088.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 119\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 1984, reward 944.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 112\n",
      "Initial State is  [2, 16, 1]\n",
      "episode 1985, reward 916.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 116\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 1986, reward 918.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 102\n",
      "Initial State is  [4, 21, 2]\n",
      "episode 1987, reward 880.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 121\n",
      "Initial State is  [0, 19, 4]\n",
      "episode 1988, reward 1336.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 116\n",
      "Initial State is  [3, 22, 0]\n",
      "episode 1989, reward 1058.0, memory_length 2000, epsilon 0.0009954703940636294, time 740.0, rides 131\n",
      "Initial State is  [4, 18, 4]\n",
      "episode 1990, reward 1169.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 131\n",
      "Initial State is  [4, 21, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1991, reward 1233.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 120\n",
      "Initial State is  [4, 10, 4]\n",
      "episode 1992, reward 1364.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 115\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 1993, reward 1117.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 114\n",
      "Initial State is  [3, 17, 4]\n",
      "episode 1994, reward 942.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 117\n",
      "Initial State is  [1, 3, 3]\n",
      "episode 1995, reward 1446.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 124\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 1996, reward 1047.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 117\n",
      "Initial State is  [1, 10, 3]\n",
      "episode 1997, reward 1235.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 116\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 1998, reward 1231.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 112\n",
      "Initial State is  [1, 10, 3]\n",
      "episode 1999, reward 1156.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 119\n",
      "Initial State is  [2, 6, 4]\n",
      "episode 2000, reward 1031.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 107\n",
      "Initial State is  [3, 20, 1]\n",
      "episode 2001, reward 1421.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 116\n",
      "Initial State is  [0, 3, 2]\n",
      "episode 2002, reward 1133.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 122\n",
      "Initial State is  [2, 2, 2]\n",
      "episode 2003, reward 1156.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 106\n",
      "Initial State is  [3, 1, 2]\n",
      "episode 2004, reward 1084.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 123\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 2005, reward 863.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 130\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 2006, reward 1003.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 115\n",
      "Initial State is  [1, 22, 2]\n",
      "episode 2007, reward 1103.0, memory_length 2000, epsilon 0.0009954703940636294, time 743.0, rides 117\n",
      "Initial State is  [2, 14, 6]\n",
      "episode 2008, reward 1264.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 115\n",
      "Initial State is  [4, 3, 5]\n",
      "episode 2009, reward 1100.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 119\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 2010, reward 1252.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 122\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 2011, reward 1022.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 122\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 2012, reward 1207.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 115\n",
      "Initial State is  [1, 12, 2]\n",
      "episode 2013, reward 1163.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 111\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 2014, reward 1219.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 124\n",
      "Initial State is  [4, 10, 5]\n",
      "episode 2015, reward 1098.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 120\n",
      "Initial State is  [3, 16, 1]\n",
      "episode 2016, reward 1066.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 116\n",
      "Initial State is  [2, 8, 6]\n",
      "episode 2017, reward 785.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 117\n",
      "Initial State is  [3, 10, 3]\n",
      "episode 2018, reward 1306.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 118\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 2019, reward 1129.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 110\n",
      "Initial State is  [4, 5, 6]\n",
      "episode 2020, reward 1068.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 111\n",
      "Initial State is  [0, 12, 2]\n",
      "episode 2021, reward 1008.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 111\n",
      "Initial State is  [3, 10, 6]\n",
      "episode 2022, reward 1504.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 119\n",
      "Initial State is  [3, 23, 6]\n",
      "episode 2023, reward 1275.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 117\n",
      "Initial State is  [2, 21, 0]\n",
      "episode 2024, reward 1346.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 118\n",
      "Initial State is  [0, 7, 4]\n",
      "episode 2025, reward 1114.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 122\n",
      "Initial State is  [3, 22, 4]\n",
      "episode 2026, reward 1126.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 110\n",
      "Initial State is  [2, 21, 4]\n",
      "episode 2027, reward 995.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 113\n",
      "Initial State is  [4, 1, 2]\n",
      "episode 2028, reward 1340.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 120\n",
      "Initial State is  [2, 6, 6]\n",
      "episode 2029, reward 988.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 117\n",
      "Initial State is  [2, 16, 2]\n",
      "episode 2030, reward 1108.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 130\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 2031, reward 923.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 111\n",
      "Initial State is  [3, 13, 4]\n",
      "episode 2032, reward 1100.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 111\n",
      "Initial State is  [2, 3, 5]\n",
      "episode 2033, reward 1099.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 124\n",
      "Initial State is  [1, 4, 6]\n",
      "episode 2034, reward 1159.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 131\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 2035, reward 1292.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 122\n",
      "Initial State is  [1, 11, 5]\n",
      "episode 2036, reward 1217.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 121\n",
      "Initial State is  [0, 12, 5]\n",
      "episode 2037, reward 1301.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 123\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 2038, reward 1241.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 125\n",
      "Initial State is  [2, 11, 0]\n",
      "episode 2039, reward 1195.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 112\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 2040, reward 803.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 111\n",
      "Initial State is  [3, 19, 4]\n",
      "episode 2041, reward 1027.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 111\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 2042, reward 1265.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 115\n",
      "Initial State is  [1, 11, 1]\n",
      "episode 2043, reward 1322.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 110\n",
      "Initial State is  [2, 10, 4]\n",
      "episode 2044, reward 751.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 107\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 2045, reward 989.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 107\n",
      "Initial State is  [2, 19, 0]\n",
      "episode 2046, reward 1127.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 118\n",
      "Initial State is  [1, 3, 5]\n",
      "episode 2047, reward 1341.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 115\n",
      "Initial State is  [3, 10, 1]\n",
      "episode 2048, reward 1363.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 134\n",
      "Initial State is  [3, 7, 2]\n",
      "episode 2049, reward 994.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 119\n",
      "Initial State is  [0, 3, 3]\n",
      "episode 2050, reward 1428.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 113\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 2051, reward 1081.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 122\n",
      "Initial State is  [1, 21, 2]\n",
      "episode 2052, reward 1239.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 129\n",
      "Initial State is  [4, 21, 3]\n",
      "episode 2053, reward 1142.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 128\n",
      "Initial State is  [2, 9, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2054, reward 995.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 125\n",
      "Initial State is  [4, 20, 6]\n",
      "episode 2055, reward 1326.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 112\n",
      "Initial State is  [0, 0, 6]\n",
      "episode 2056, reward 1336.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 116\n",
      "Initial State is  [0, 17, 2]\n",
      "episode 2057, reward 1282.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 120\n",
      "Initial State is  [0, 1, 1]\n",
      "episode 2058, reward 1065.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 112\n",
      "Initial State is  [3, 19, 0]\n",
      "episode 2059, reward 1144.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 121\n",
      "Initial State is  [0, 14, 5]\n",
      "episode 2060, reward 1113.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 117\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 2061, reward 856.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 120\n",
      "Initial State is  [0, 3, 6]\n",
      "episode 2062, reward 1051.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 120\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 2063, reward 934.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 127\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 2064, reward 1112.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 126\n",
      "Initial State is  [4, 20, 6]\n",
      "episode 2065, reward 1587.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 119\n",
      "Initial State is  [0, 2, 4]\n",
      "episode 2066, reward 1220.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 117\n",
      "Initial State is  [2, 6, 1]\n",
      "episode 2067, reward 1208.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 117\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 2068, reward 1061.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 122\n",
      "Initial State is  [1, 0, 0]\n",
      "episode 2069, reward 1248.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 123\n",
      "Initial State is  [4, 1, 6]\n",
      "episode 2070, reward 934.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 121\n",
      "Initial State is  [1, 0, 4]\n",
      "episode 2071, reward 1085.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 105\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 2072, reward 975.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 112\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 2073, reward 1180.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 120\n",
      "Initial State is  [1, 4, 4]\n",
      "episode 2074, reward 1097.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 119\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 2075, reward 1491.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 110\n",
      "Initial State is  [3, 4, 4]\n",
      "episode 2076, reward 1335.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 117\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 2077, reward 1469.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 113\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 2078, reward 1091.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 114\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 2079, reward 1204.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 118\n",
      "Initial State is  [3, 10, 2]\n",
      "episode 2080, reward 1178.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 117\n",
      "Initial State is  [2, 8, 0]\n",
      "episode 2081, reward 1227.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 118\n",
      "Initial State is  [4, 17, 2]\n",
      "episode 2082, reward 1323.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 120\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 2083, reward 1226.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 120\n",
      "Initial State is  [3, 1, 0]\n",
      "episode 2084, reward 1039.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 110\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 2085, reward 1316.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 130\n",
      "Initial State is  [2, 14, 6]\n",
      "episode 2086, reward 913.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 114\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 2087, reward 1137.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 124\n",
      "Initial State is  [3, 22, 0]\n",
      "episode 2088, reward 1192.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 120\n",
      "Initial State is  [2, 18, 4]\n",
      "episode 2089, reward 1132.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 121\n",
      "Initial State is  [0, 1, 2]\n",
      "episode 2090, reward 1098.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 111\n",
      "Initial State is  [3, 10, 2]\n",
      "episode 2091, reward 1423.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 108\n",
      "Initial State is  [1, 5, 4]\n",
      "episode 2092, reward 1306.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 121\n",
      "Initial State is  [3, 4, 4]\n",
      "episode 2093, reward 894.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 121\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 2094, reward 1092.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 116\n",
      "Initial State is  [4, 7, 3]\n",
      "episode 2095, reward 1103.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 123\n",
      "Initial State is  [0, 22, 6]\n",
      "episode 2096, reward 943.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 116\n",
      "Initial State is  [0, 6, 2]\n",
      "episode 2097, reward 968.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 126\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 2098, reward 1195.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 111\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 2099, reward 1298.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 127\n",
      "Initial State is  [2, 12, 2]\n",
      "episode 2100, reward 1305.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 117\n",
      "Initial State is  [1, 12, 6]\n",
      "episode 2101, reward 1052.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 119\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 2102, reward 1202.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 128\n",
      "Initial State is  [2, 23, 4]\n",
      "episode 2103, reward 1074.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 112\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 2104, reward 1030.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 124\n",
      "Initial State is  [4, 7, 4]\n",
      "episode 2105, reward 1196.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 114\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 2106, reward 1419.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 119\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 2107, reward 1175.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 124\n",
      "Initial State is  [2, 8, 2]\n",
      "episode 2108, reward 969.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 118\n",
      "Initial State is  [1, 5, 2]\n",
      "episode 2109, reward 1159.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 122\n",
      "Initial State is  [2, 1, 1]\n",
      "episode 2110, reward 1040.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 114\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 2111, reward 1251.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 115\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 2112, reward 1044.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 121\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 2113, reward 1147.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 116\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 2114, reward 1306.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 123\n",
      "Initial State is  [1, 17, 4]\n",
      "episode 2115, reward 1003.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 117\n",
      "Initial State is  [3, 11, 0]\n",
      "episode 2116, reward 1071.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 121\n",
      "Initial State is  [3, 15, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2117, reward 992.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 106\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 2118, reward 1536.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 126\n",
      "Initial State is  [1, 19, 2]\n",
      "episode 2119, reward 1157.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 115\n",
      "Initial State is  [3, 10, 4]\n",
      "episode 2120, reward 1092.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 127\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 2121, reward 1018.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 129\n",
      "Initial State is  [0, 12, 2]\n",
      "episode 2122, reward 1136.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 105\n",
      "Initial State is  [3, 13, 3]\n",
      "episode 2123, reward 1025.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 117\n",
      "Initial State is  [4, 14, 1]\n",
      "episode 2124, reward 1124.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 121\n",
      "Initial State is  [1, 7, 6]\n",
      "episode 2125, reward 1226.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 115\n",
      "Initial State is  [2, 5, 6]\n",
      "episode 2126, reward 846.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 115\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 2127, reward 1136.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 123\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 2128, reward 1142.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 110\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 2129, reward 818.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 116\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 2130, reward 1396.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 117\n",
      "Initial State is  [0, 19, 3]\n",
      "episode 2131, reward 1631.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 129\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 2132, reward 1299.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 122\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 2133, reward 1047.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 115\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 2134, reward 1182.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 116\n",
      "Initial State is  [4, 16, 6]\n",
      "episode 2135, reward 1124.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 105\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 2136, reward 785.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 116\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 2137, reward 1356.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 129\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 2138, reward 1128.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 121\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 2139, reward 1059.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 122\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 2140, reward 1327.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 120\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 2141, reward 1106.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 122\n",
      "Initial State is  [0, 8, 4]\n",
      "episode 2142, reward 846.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 122\n",
      "Initial State is  [1, 0, 1]\n",
      "episode 2143, reward 1063.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 115\n",
      "Initial State is  [1, 13, 4]\n",
      "episode 2144, reward 1220.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 122\n",
      "Initial State is  [0, 18, 0]\n",
      "episode 2145, reward 1255.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 119\n",
      "Initial State is  [2, 2, 5]\n",
      "episode 2146, reward 940.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 119\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 2147, reward 871.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 116\n",
      "Initial State is  [3, 9, 2]\n",
      "episode 2148, reward 1380.0, memory_length 2000, epsilon 0.0009954703940636294, time 741.0, rides 115\n",
      "Initial State is  [3, 3, 5]\n",
      "episode 2149, reward 957.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 111\n",
      "Initial State is  [1, 4, 4]\n",
      "episode 2150, reward 1113.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 119\n",
      "Initial State is  [3, 5, 1]\n",
      "episode 2151, reward 1069.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 125\n",
      "Initial State is  [2, 6, 0]\n",
      "episode 2152, reward 1091.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 117\n",
      "Initial State is  [2, 5, 6]\n",
      "episode 2153, reward 902.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 115\n",
      "Initial State is  [0, 16, 1]\n",
      "episode 2154, reward 1368.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 115\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 2155, reward 1108.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 117\n",
      "Initial State is  [3, 9, 0]\n",
      "episode 2156, reward 1199.0, memory_length 2000, epsilon 0.0009954703940636294, time 739.0, rides 121\n",
      "Initial State is  [4, 8, 5]\n",
      "episode 2157, reward 1250.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 124\n",
      "Initial State is  [3, 11, 4]\n",
      "episode 2158, reward 1061.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 112\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 2159, reward 1545.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 118\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 2160, reward 1074.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 108\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 2161, reward 1041.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 126\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 2162, reward 1213.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 115\n",
      "Initial State is  [1, 9, 4]\n",
      "episode 2163, reward 1177.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 121\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 2164, reward 850.0, memory_length 2000, epsilon 0.0009954703940636294, time 720.0, rides 121\n",
      "Initial State is  [1, 2, 0]\n",
      "episode 2165, reward 1157.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 124\n",
      "Initial State is  [0, 6, 2]\n",
      "episode 2166, reward 1278.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 116\n",
      "Initial State is  [1, 12, 2]\n",
      "episode 2167, reward 1313.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 122\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 2168, reward 1227.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 144\n",
      "Initial State is  [4, 13, 6]\n",
      "episode 2169, reward 634.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 117\n",
      "Initial State is  [2, 22, 6]\n",
      "episode 2170, reward 801.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 112\n",
      "Initial State is  [3, 22, 1]\n",
      "episode 2171, reward 1270.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 114\n",
      "Initial State is  [0, 3, 2]\n",
      "episode 2172, reward 1420.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 124\n",
      "Initial State is  [1, 21, 6]\n",
      "episode 2173, reward 1452.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 127\n",
      "Initial State is  [4, 17, 2]\n",
      "episode 2174, reward 944.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 112\n",
      "Initial State is  [4, 4, 0]\n",
      "episode 2175, reward 810.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 108\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 2176, reward 1183.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 123\n",
      "Initial State is  [3, 7, 0]\n",
      "episode 2177, reward 1006.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 126\n",
      "Initial State is  [4, 4, 4]\n",
      "episode 2178, reward 1263.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 128\n",
      "Initial State is  [3, 23, 6]\n",
      "episode 2179, reward 1191.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 121\n",
      "Initial State is  [0, 21, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2180, reward 1075.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 122\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 2181, reward 1158.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 112\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 2182, reward 1152.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 136\n",
      "Initial State is  [4, 21, 5]\n",
      "episode 2183, reward 1204.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 113\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 2184, reward 991.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 121\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 2185, reward 906.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 109\n",
      "Initial State is  [1, 20, 4]\n",
      "episode 2186, reward 1230.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 119\n",
      "Initial State is  [3, 7, 4]\n",
      "episode 2187, reward 1002.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 114\n",
      "Initial State is  [2, 4, 1]\n",
      "episode 2188, reward 1311.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 116\n",
      "Initial State is  [0, 0, 5]\n",
      "episode 2189, reward 989.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 127\n",
      "Initial State is  [2, 2, 5]\n",
      "episode 2190, reward 984.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 115\n",
      "Initial State is  [0, 23, 2]\n",
      "episode 2191, reward 1285.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 127\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 2192, reward 953.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 119\n",
      "Initial State is  [3, 21, 0]\n",
      "episode 2193, reward 750.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 116\n",
      "Initial State is  [1, 15, 3]\n",
      "episode 2194, reward 1434.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 117\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 2195, reward 1131.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 128\n",
      "Initial State is  [2, 7, 5]\n",
      "episode 2196, reward 1238.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 113\n",
      "Initial State is  [1, 4, 6]\n",
      "episode 2197, reward 1132.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 113\n",
      "Initial State is  [2, 6, 4]\n",
      "episode 2198, reward 1100.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 126\n",
      "Initial State is  [0, 4, 4]\n",
      "episode 2199, reward 1001.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 115\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 2200, reward 903.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 119\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 2201, reward 1214.0, memory_length 2000, epsilon 0.0009954703940636294, time 740.0, rides 109\n",
      "Initial State is  [0, 1, 4]\n",
      "episode 2202, reward 1057.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 118\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 2203, reward 997.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 112\n",
      "Initial State is  [1, 0, 6]\n",
      "episode 2204, reward 936.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 114\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 2205, reward 1121.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 119\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 2206, reward 771.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 115\n",
      "Initial State is  [0, 21, 1]\n",
      "episode 2207, reward 1042.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 122\n",
      "Initial State is  [2, 21, 1]\n",
      "episode 2208, reward 1125.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 114\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 2209, reward 1264.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 115\n",
      "Initial State is  [4, 1, 6]\n",
      "episode 2210, reward 1262.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 118\n",
      "Initial State is  [2, 8, 4]\n",
      "episode 2211, reward 1122.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 123\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 2212, reward 1220.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 128\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 2213, reward 1521.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 113\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 2214, reward 1126.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 117\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 2215, reward 1417.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 126\n",
      "Initial State is  [0, 14, 5]\n",
      "episode 2216, reward 1017.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 109\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 2217, reward 1533.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 131\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 2218, reward 1081.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 123\n",
      "Initial State is  [3, 13, 0]\n",
      "episode 2219, reward 916.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 116\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 2220, reward 1194.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 128\n",
      "Initial State is  [2, 0, 3]\n",
      "episode 2221, reward 1024.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 123\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 2222, reward 1212.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 118\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 2223, reward 1597.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 121\n",
      "Initial State is  [2, 0, 2]\n",
      "episode 2224, reward 1268.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 130\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 2225, reward 1374.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 121\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 2226, reward 1091.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 108\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 2227, reward 1068.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 124\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 2228, reward 1224.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 114\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 2229, reward 1385.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 124\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 2230, reward 1200.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 131\n",
      "Initial State is  [4, 12, 6]\n",
      "episode 2231, reward 1103.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 112\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 2232, reward 922.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 115\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 2233, reward 1088.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 116\n",
      "Initial State is  [2, 3, 1]\n",
      "episode 2234, reward 1309.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 123\n",
      "Initial State is  [0, 20, 1]\n",
      "episode 2235, reward 1600.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 130\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 2236, reward 1206.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 124\n",
      "Initial State is  [0, 19, 4]\n",
      "episode 2237, reward 1110.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 124\n",
      "Initial State is  [3, 12, 1]\n",
      "episode 2238, reward 1310.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 126\n",
      "Initial State is  [2, 6, 1]\n",
      "episode 2239, reward 984.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 121\n",
      "Initial State is  [1, 6, 4]\n",
      "episode 2240, reward 1308.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 119\n",
      "Initial State is  [2, 16, 2]\n",
      "episode 2241, reward 1149.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 115\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 2242, reward 1029.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 123\n",
      "Initial State is  [4, 0, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2243, reward 1421.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 111\n",
      "Initial State is  [3, 18, 4]\n",
      "episode 2244, reward 1349.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 121\n",
      "Initial State is  [4, 19, 0]\n",
      "episode 2245, reward 861.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 117\n",
      "Initial State is  [1, 21, 5]\n",
      "episode 2246, reward 1059.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 121\n",
      "Initial State is  [1, 0, 4]\n",
      "episode 2247, reward 1056.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 120\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 2248, reward 1102.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 108\n",
      "Initial State is  [2, 5, 5]\n",
      "episode 2249, reward 1178.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 131\n",
      "Initial State is  [4, 5, 3]\n",
      "episode 2250, reward 1144.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 123\n",
      "Initial State is  [2, 2, 5]\n",
      "episode 2251, reward 991.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 114\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 2252, reward 783.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 113\n",
      "Initial State is  [1, 19, 5]\n",
      "episode 2253, reward 1004.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 110\n",
      "Initial State is  [2, 0, 5]\n",
      "episode 2254, reward 835.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 121\n",
      "Initial State is  [0, 19, 5]\n",
      "episode 2255, reward 1157.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 118\n",
      "Initial State is  [1, 4, 4]\n",
      "episode 2256, reward 1357.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 122\n",
      "Initial State is  [1, 15, 0]\n",
      "episode 2257, reward 903.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 114\n",
      "Initial State is  [0, 22, 2]\n",
      "episode 2258, reward 1056.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 115\n",
      "Initial State is  [2, 17, 3]\n",
      "episode 2259, reward 1193.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 120\n",
      "Initial State is  [2, 13, 3]\n",
      "episode 2260, reward 1177.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 125\n",
      "Initial State is  [4, 20, 4]\n",
      "episode 2261, reward 1257.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 117\n",
      "Initial State is  [1, 4, 6]\n",
      "episode 2262, reward 905.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 103\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 2263, reward 1260.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 134\n",
      "Initial State is  [4, 23, 2]\n",
      "episode 2264, reward 991.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 118\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 2265, reward 1160.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 117\n",
      "Initial State is  [2, 14, 0]\n",
      "episode 2266, reward 587.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 105\n",
      "Initial State is  [0, 4, 0]\n",
      "episode 2267, reward 1323.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 120\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 2268, reward 1089.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 126\n",
      "Initial State is  [3, 17, 5]\n",
      "episode 2269, reward 817.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 123\n",
      "Initial State is  [2, 16, 6]\n",
      "episode 2270, reward 1029.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 111\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 2271, reward 1072.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 116\n",
      "Initial State is  [2, 21, 0]\n",
      "episode 2272, reward 1160.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 125\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 2273, reward 946.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 107\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 2274, reward 1089.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 116\n",
      "Initial State is  [1, 2, 3]\n",
      "episode 2275, reward 732.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 124\n",
      "Initial State is  [0, 11, 2]\n",
      "episode 2276, reward 1600.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 120\n",
      "Initial State is  [2, 4, 0]\n",
      "episode 2277, reward 1062.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 120\n",
      "Initial State is  [1, 3, 0]\n",
      "episode 2278, reward 973.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 127\n",
      "Initial State is  [3, 23, 6]\n",
      "episode 2279, reward 976.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 110\n",
      "Initial State is  [2, 21, 1]\n",
      "episode 2280, reward 1280.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 132\n",
      "Initial State is  [2, 6, 6]\n",
      "episode 2281, reward 1257.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 116\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 2282, reward 1158.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 116\n",
      "Initial State is  [2, 15, 2]\n",
      "episode 2283, reward 1345.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 115\n",
      "Initial State is  [0, 14, 1]\n",
      "episode 2284, reward 1104.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 116\n",
      "Initial State is  [3, 23, 5]\n",
      "episode 2285, reward 1021.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 116\n",
      "Initial State is  [3, 21, 2]\n",
      "episode 2286, reward 1135.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 121\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 2287, reward 1184.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 116\n",
      "Initial State is  [4, 20, 1]\n",
      "episode 2288, reward 1229.0, memory_length 2000, epsilon 0.0009954703940636294, time 740.0, rides 115\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 2289, reward 1089.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 116\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 2290, reward 1229.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 123\n",
      "Initial State is  [1, 8, 2]\n",
      "episode 2291, reward 790.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 113\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 2292, reward 1176.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 110\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 2293, reward 1126.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 125\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 2294, reward 1339.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 116\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 2295, reward 968.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 123\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 2296, reward 942.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 111\n",
      "Initial State is  [3, 12, 0]\n",
      "episode 2297, reward 1309.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 115\n",
      "Initial State is  [3, 7, 3]\n",
      "episode 2298, reward 1147.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 112\n",
      "Initial State is  [0, 0, 3]\n",
      "episode 2299, reward 934.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 107\n",
      "Initial State is  [0, 15, 0]\n",
      "episode 2300, reward 935.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 113\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 2301, reward 1217.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 112\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 2302, reward 1253.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 116\n",
      "Initial State is  [2, 2, 2]\n",
      "episode 2303, reward 1177.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 109\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 2304, reward 1208.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 119\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 2305, reward 1019.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 126\n",
      "Initial State is  [0, 8, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2306, reward 1015.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 123\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 2307, reward 1333.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 126\n",
      "Initial State is  [2, 15, 1]\n",
      "episode 2308, reward 1126.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 111\n",
      "Initial State is  [0, 0, 5]\n",
      "episode 2309, reward 1066.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 112\n",
      "Initial State is  [0, 3, 5]\n",
      "episode 2310, reward 1241.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 128\n",
      "Initial State is  [0, 10, 6]\n",
      "episode 2311, reward 1236.0, memory_length 2000, epsilon 0.0009954703940636294, time 740.0, rides 122\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 2312, reward 1245.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 123\n",
      "Initial State is  [2, 12, 6]\n",
      "episode 2313, reward 1243.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 121\n",
      "Initial State is  [3, 12, 3]\n",
      "episode 2314, reward 1224.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 125\n",
      "Initial State is  [4, 9, 5]\n",
      "episode 2315, reward 810.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 115\n",
      "Initial State is  [0, 7, 2]\n",
      "episode 2316, reward 1254.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 129\n",
      "Initial State is  [1, 12, 0]\n",
      "episode 2317, reward 1262.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 115\n",
      "Initial State is  [3, 17, 2]\n",
      "episode 2318, reward 839.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 115\n",
      "Initial State is  [3, 7, 3]\n",
      "episode 2319, reward 1515.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 123\n",
      "Initial State is  [3, 19, 6]\n",
      "episode 2320, reward 859.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 115\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 2321, reward 1263.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 122\n",
      "Initial State is  [3, 11, 3]\n",
      "episode 2322, reward 1075.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 114\n",
      "Initial State is  [3, 4, 4]\n",
      "episode 2323, reward 1095.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 122\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 2324, reward 1036.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 115\n",
      "Initial State is  [3, 1, 0]\n",
      "episode 2325, reward 967.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 126\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 2326, reward 1044.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 122\n",
      "Initial State is  [1, 17, 0]\n",
      "episode 2327, reward 1436.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 127\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 2328, reward 1323.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 127\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 2329, reward 1338.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 117\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 2330, reward 1202.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 124\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 2331, reward 1198.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 123\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 2332, reward 968.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 119\n",
      "Initial State is  [3, 12, 3]\n",
      "episode 2333, reward 1019.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 119\n",
      "Initial State is  [1, 3, 1]\n",
      "episode 2334, reward 1238.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 120\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 2335, reward 1221.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 126\n",
      "Initial State is  [0, 3, 6]\n",
      "episode 2336, reward 1339.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 118\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 2337, reward 1072.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 119\n",
      "Initial State is  [1, 16, 3]\n",
      "episode 2338, reward 1291.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 124\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 2339, reward 1116.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 114\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 2340, reward 1103.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 120\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 2341, reward 1411.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 133\n",
      "Initial State is  [2, 18, 6]\n",
      "episode 2342, reward 1296.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 119\n",
      "Initial State is  [2, 8, 0]\n",
      "episode 2343, reward 1000.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 120\n",
      "Initial State is  [0, 17, 1]\n",
      "episode 2344, reward 1263.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 118\n",
      "Initial State is  [1, 14, 5]\n",
      "episode 2345, reward 1052.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 115\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 2346, reward 854.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 117\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 2347, reward 1017.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 133\n",
      "Initial State is  [3, 9, 2]\n",
      "episode 2348, reward 784.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 109\n",
      "Initial State is  [1, 19, 3]\n",
      "episode 2349, reward 1227.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 121\n",
      "Initial State is  [1, 4, 6]\n",
      "episode 2350, reward 962.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 123\n",
      "Initial State is  [2, 13, 1]\n",
      "episode 2351, reward 1189.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 111\n",
      "Initial State is  [3, 9, 0]\n",
      "episode 2352, reward 1083.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 131\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 2353, reward 834.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 112\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 2354, reward 1188.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 119\n",
      "Initial State is  [1, 21, 3]\n",
      "episode 2355, reward 1270.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 128\n",
      "Initial State is  [3, 4, 2]\n",
      "episode 2356, reward 1402.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 126\n",
      "Initial State is  [3, 2, 2]\n",
      "episode 2357, reward 908.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 118\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 2358, reward 1406.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 121\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 2359, reward 1194.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 117\n",
      "Initial State is  [2, 11, 6]\n",
      "episode 2360, reward 849.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 116\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 2361, reward 732.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 124\n",
      "Initial State is  [2, 8, 0]\n",
      "episode 2362, reward 921.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 116\n",
      "Initial State is  [1, 21, 3]\n",
      "episode 2363, reward 1056.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 123\n",
      "Initial State is  [3, 6, 1]\n",
      "episode 2364, reward 963.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 117\n",
      "Initial State is  [4, 23, 4]\n",
      "episode 2365, reward 1305.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 134\n",
      "Initial State is  [3, 17, 2]\n",
      "episode 2366, reward 1289.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 117\n",
      "Initial State is  [3, 22, 6]\n",
      "episode 2367, reward 844.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 115\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 2368, reward 1038.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 116\n",
      "Initial State is  [4, 11, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2369, reward 1260.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 123\n",
      "Initial State is  [1, 12, 2]\n",
      "episode 2370, reward 1103.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 121\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 2371, reward 1043.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 109\n",
      "Initial State is  [0, 8, 4]\n",
      "episode 2372, reward 1418.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 128\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 2373, reward 1256.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 118\n",
      "Initial State is  [2, 2, 5]\n",
      "episode 2374, reward 984.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 108\n",
      "Initial State is  [1, 17, 0]\n",
      "episode 2375, reward 1349.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 125\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 2376, reward 1091.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 115\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 2377, reward 1203.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 125\n",
      "Initial State is  [3, 9, 1]\n",
      "episode 2378, reward 795.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 111\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 2379, reward 1180.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 126\n",
      "Initial State is  [4, 2, 6]\n",
      "episode 2380, reward 900.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 118\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 2381, reward 1054.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 118\n",
      "Initial State is  [2, 1, 3]\n",
      "episode 2382, reward 938.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 121\n",
      "Initial State is  [2, 16, 4]\n",
      "episode 2383, reward 1056.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 123\n",
      "Initial State is  [1, 15, 1]\n",
      "episode 2384, reward 1088.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 114\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 2385, reward 915.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 116\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 2386, reward 915.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 103\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 2387, reward 799.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 114\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 2388, reward 1403.0, memory_length 2000, epsilon 0.0009954703940636294, time 739.0, rides 123\n",
      "Initial State is  [0, 8, 1]\n",
      "episode 2389, reward 1075.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 111\n",
      "Initial State is  [4, 23, 1]\n",
      "episode 2390, reward 1231.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 119\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 2391, reward 1378.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 119\n",
      "Initial State is  [1, 12, 4]\n",
      "episode 2392, reward 988.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 111\n",
      "Initial State is  [0, 11, 2]\n",
      "episode 2393, reward 1189.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 114\n",
      "Initial State is  [1, 17, 0]\n",
      "episode 2394, reward 1255.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 125\n",
      "Initial State is  [3, 5, 5]\n",
      "episode 2395, reward 1391.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 128\n",
      "Initial State is  [4, 9, 3]\n",
      "episode 2396, reward 1235.0, memory_length 2000, epsilon 0.0009954703940636294, time 720.0, rides 123\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 2397, reward 1019.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 105\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 2398, reward 976.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 114\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 2399, reward 877.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 116\n",
      "Initial State is  [4, 8, 4]\n",
      "episode 2400, reward 954.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 122\n",
      "Initial State is  [3, 15, 2]\n",
      "episode 2401, reward 1161.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 123\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 2402, reward 1169.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 122\n",
      "Initial State is  [0, 2, 5]\n",
      "episode 2403, reward 1137.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 117\n",
      "Initial State is  [3, 9, 1]\n",
      "episode 2404, reward 1135.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 118\n",
      "Initial State is  [1, 10, 1]\n",
      "episode 2405, reward 1309.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 126\n",
      "Initial State is  [4, 16, 2]\n",
      "episode 2406, reward 1427.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 122\n",
      "Initial State is  [4, 15, 0]\n",
      "episode 2407, reward 1124.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 116\n",
      "Initial State is  [4, 6, 2]\n",
      "episode 2408, reward 1202.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 113\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 2409, reward 1028.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 121\n",
      "Initial State is  [1, 0, 6]\n",
      "episode 2410, reward 1028.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 116\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 2411, reward 1203.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 121\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 2412, reward 837.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 119\n",
      "Initial State is  [1, 20, 5]\n",
      "episode 2413, reward 1367.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 117\n",
      "Initial State is  [1, 3, 1]\n",
      "episode 2414, reward 1134.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 117\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 2415, reward 1393.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 115\n",
      "Initial State is  [3, 8, 1]\n",
      "episode 2416, reward 1561.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 120\n",
      "Initial State is  [1, 10, 0]\n",
      "episode 2417, reward 1303.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 134\n",
      "Initial State is  [4, 20, 2]\n",
      "episode 2418, reward 1065.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 115\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 2419, reward 967.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 115\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 2420, reward 645.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 126\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 2421, reward 1230.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 119\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 2422, reward 937.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 116\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 2423, reward 1181.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 116\n",
      "Initial State is  [3, 7, 0]\n",
      "episode 2424, reward 1277.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 120\n",
      "Initial State is  [4, 23, 4]\n",
      "episode 2425, reward 1325.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 118\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 2426, reward 1155.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 118\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 2427, reward 1029.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 118\n",
      "Initial State is  [1, 11, 6]\n",
      "episode 2428, reward 1231.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 119\n",
      "Initial State is  [1, 14, 6]\n",
      "episode 2429, reward 856.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 117\n",
      "Initial State is  [1, 13, 2]\n",
      "episode 2430, reward 995.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 118\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 2431, reward 1120.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 118\n",
      "Initial State is  [1, 8, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2432, reward 1111.0, memory_length 2000, epsilon 0.0009954703940636294, time 739.0, rides 112\n",
      "Initial State is  [3, 22, 4]\n",
      "episode 2433, reward 1161.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 116\n",
      "Initial State is  [4, 4, 0]\n",
      "episode 2434, reward 1236.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 110\n",
      "Initial State is  [4, 20, 4]\n",
      "episode 2435, reward 1035.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 113\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 2436, reward 789.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 110\n",
      "Initial State is  [3, 16, 5]\n",
      "episode 2437, reward 1236.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 124\n",
      "Initial State is  [3, 20, 6]\n",
      "episode 2438, reward 1125.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 111\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 2439, reward 1072.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 120\n",
      "Initial State is  [4, 22, 3]\n",
      "episode 2440, reward 1254.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 114\n",
      "Initial State is  [4, 8, 3]\n",
      "episode 2441, reward 921.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 121\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 2442, reward 1207.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 130\n",
      "Initial State is  [1, 15, 0]\n",
      "episode 2443, reward 964.0, memory_length 2000, epsilon 0.0009954703940636294, time 744.0, rides 116\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 2444, reward 1139.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 110\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 2445, reward 1025.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 119\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 2446, reward 1324.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 123\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 2447, reward 899.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 111\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 2448, reward 1181.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 123\n",
      "Initial State is  [1, 2, 3]\n",
      "episode 2449, reward 1238.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 120\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 2450, reward 1156.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 135\n",
      "Initial State is  [0, 8, 6]\n",
      "episode 2451, reward 1189.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 114\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 2452, reward 979.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 116\n",
      "Initial State is  [3, 23, 3]\n",
      "episode 2453, reward 1344.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 113\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 2454, reward 1192.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 126\n",
      "Initial State is  [3, 11, 3]\n",
      "episode 2455, reward 1168.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 117\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 2456, reward 854.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 115\n",
      "Initial State is  [4, 16, 2]\n",
      "episode 2457, reward 1198.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 119\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 2458, reward 1170.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 118\n",
      "Initial State is  [4, 23, 2]\n",
      "episode 2459, reward 1158.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 119\n",
      "Initial State is  [0, 3, 5]\n",
      "episode 2460, reward 1129.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 119\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 2461, reward 1287.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 121\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 2462, reward 1220.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 124\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 2463, reward 1252.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 127\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 2464, reward 1071.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 111\n",
      "Initial State is  [4, 2, 2]\n",
      "episode 2465, reward 1360.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 110\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 2466, reward 1288.0, memory_length 2000, epsilon 0.0009954703940636294, time 720.0, rides 122\n",
      "Initial State is  [0, 23, 0]\n",
      "episode 2467, reward 996.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 128\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 2468, reward 1388.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 121\n",
      "Initial State is  [3, 12, 6]\n",
      "episode 2469, reward 827.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 122\n",
      "Initial State is  [2, 6, 5]\n",
      "episode 2470, reward 1169.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 114\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 2471, reward 958.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 110\n",
      "Initial State is  [1, 22, 1]\n",
      "episode 2472, reward 926.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 125\n",
      "Initial State is  [4, 22, 1]\n",
      "episode 2473, reward 1087.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 113\n",
      "Initial State is  [1, 13, 1]\n",
      "episode 2474, reward 910.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 119\n",
      "Initial State is  [3, 9, 6]\n",
      "episode 2475, reward 1265.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 116\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 2476, reward 1168.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 117\n",
      "Initial State is  [3, 2, 4]\n",
      "episode 2477, reward 937.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 105\n",
      "Initial State is  [2, 1, 4]\n",
      "episode 2478, reward 1414.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 121\n",
      "Initial State is  [1, 12, 0]\n",
      "episode 2479, reward 1245.0, memory_length 2000, epsilon 0.0009954703940636294, time 739.0, rides 111\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 2480, reward 1312.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 119\n",
      "Initial State is  [2, 8, 0]\n",
      "episode 2481, reward 1206.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 115\n",
      "Initial State is  [3, 19, 2]\n",
      "episode 2482, reward 936.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 118\n",
      "Initial State is  [4, 19, 6]\n",
      "episode 2483, reward 1316.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 131\n",
      "Initial State is  [4, 10, 3]\n",
      "episode 2484, reward 908.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 109\n",
      "Initial State is  [3, 2, 1]\n",
      "episode 2485, reward 1242.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 124\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 2486, reward 1065.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 123\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 2487, reward 1406.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 121\n",
      "Initial State is  [3, 4, 4]\n",
      "episode 2488, reward 1152.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 114\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 2489, reward 1164.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 125\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 2490, reward 1198.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 123\n",
      "Initial State is  [4, 7, 6]\n",
      "episode 2491, reward 1083.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 112\n",
      "Initial State is  [4, 8, 2]\n",
      "episode 2492, reward 989.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 116\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 2493, reward 1110.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 122\n",
      "Initial State is  [4, 9, 4]\n",
      "episode 2494, reward 1153.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 135\n",
      "Initial State is  [1, 2, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2495, reward 1183.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 124\n",
      "Initial State is  [4, 2, 2]\n",
      "episode 2496, reward 1093.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 119\n",
      "Initial State is  [2, 8, 4]\n",
      "episode 2497, reward 1324.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 124\n",
      "Initial State is  [0, 0, 5]\n",
      "episode 2498, reward 1464.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 119\n",
      "Initial State is  [4, 3, 6]\n",
      "episode 2499, reward 1231.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 115\n",
      "Initial State is  [2, 11, 5]\n",
      "episode 2500, reward 957.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 112\n",
      "Initial State is  [4, 8, 1]\n",
      "episode 2501, reward 1037.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 114\n",
      "Initial State is  [4, 0, 0]\n",
      "episode 2502, reward 1002.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 117\n",
      "Initial State is  [0, 6, 1]\n",
      "episode 2503, reward 730.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 113\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 2504, reward 1393.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 122\n",
      "Initial State is  [1, 8, 6]\n",
      "episode 2505, reward 1118.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 119\n",
      "Initial State is  [3, 20, 5]\n",
      "episode 2506, reward 1217.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 125\n",
      "Initial State is  [0, 17, 3]\n",
      "episode 2507, reward 1118.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 113\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 2508, reward 1148.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 115\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 2509, reward 1113.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 120\n",
      "Initial State is  [1, 6, 0]\n",
      "episode 2510, reward 1157.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 114\n",
      "Initial State is  [0, 23, 0]\n",
      "episode 2511, reward 1232.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 119\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 2512, reward 1187.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 111\n",
      "Initial State is  [2, 20, 4]\n",
      "episode 2513, reward 1255.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 121\n",
      "Initial State is  [1, 13, 2]\n",
      "episode 2514, reward 1172.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 120\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 2515, reward 922.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 115\n",
      "Initial State is  [1, 14, 4]\n",
      "episode 2516, reward 1292.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 132\n",
      "Initial State is  [1, 18, 0]\n",
      "episode 2517, reward 1108.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 120\n",
      "Initial State is  [4, 5, 3]\n",
      "episode 2518, reward 871.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 110\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 2519, reward 1200.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 124\n",
      "Initial State is  [1, 7, 3]\n",
      "episode 2520, reward 1376.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 111\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 2521, reward 1418.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 124\n",
      "Initial State is  [3, 7, 6]\n",
      "episode 2522, reward 1050.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 124\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 2523, reward 941.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 114\n",
      "Initial State is  [2, 13, 4]\n",
      "episode 2524, reward 1132.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 130\n",
      "Initial State is  [3, 15, 4]\n",
      "episode 2525, reward 1198.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 119\n",
      "Initial State is  [0, 12, 2]\n",
      "episode 2526, reward 1110.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 118\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 2527, reward 1138.0, memory_length 2000, epsilon 0.0009954703940636294, time 740.0, rides 117\n",
      "Initial State is  [3, 4, 0]\n",
      "episode 2528, reward 937.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 117\n",
      "Initial State is  [4, 20, 2]\n",
      "episode 2529, reward 1026.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 122\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 2530, reward 1229.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 125\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 2531, reward 988.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 117\n",
      "Initial State is  [0, 21, 6]\n",
      "episode 2532, reward 892.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 115\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 2533, reward 1131.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 118\n",
      "Initial State is  [1, 14, 6]\n",
      "episode 2534, reward 1197.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 134\n",
      "Initial State is  [1, 18, 1]\n",
      "episode 2535, reward 1247.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 118\n",
      "Initial State is  [3, 1, 1]\n",
      "episode 2536, reward 1193.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 119\n",
      "Initial State is  [0, 5, 6]\n",
      "episode 2537, reward 1087.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 120\n",
      "Initial State is  [3, 17, 1]\n",
      "episode 2538, reward 1077.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 115\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 2539, reward 1057.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 112\n",
      "Initial State is  [4, 5, 3]\n",
      "episode 2540, reward 956.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 119\n",
      "Initial State is  [1, 3, 6]\n",
      "episode 2541, reward 886.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 120\n",
      "Initial State is  [1, 17, 2]\n",
      "episode 2542, reward 1003.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 120\n",
      "Initial State is  [0, 8, 4]\n",
      "episode 2543, reward 1176.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 130\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 2544, reward 1213.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 120\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 2545, reward 1087.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 111\n",
      "Initial State is  [2, 0, 1]\n",
      "episode 2546, reward 1166.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 116\n",
      "Initial State is  [4, 12, 0]\n",
      "episode 2547, reward 1234.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 117\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 2548, reward 1189.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 122\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 2549, reward 1100.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 117\n",
      "Initial State is  [0, 16, 2]\n",
      "episode 2550, reward 1147.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 119\n",
      "Initial State is  [1, 3, 6]\n",
      "episode 2551, reward 1195.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 120\n",
      "Initial State is  [4, 23, 3]\n",
      "episode 2552, reward 721.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 109\n",
      "Initial State is  [4, 16, 6]\n",
      "episode 2553, reward 1230.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 132\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 2554, reward 1157.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 120\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 2555, reward 943.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 121\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 2556, reward 1029.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 113\n",
      "Initial State is  [2, 20, 4]\n",
      "episode 2557, reward 1203.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 114\n",
      "Initial State is  [3, 14, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2558, reward 1032.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 118\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 2559, reward 1232.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 115\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 2560, reward 822.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 114\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 2561, reward 1302.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 123\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 2562, reward 932.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 113\n",
      "Initial State is  [4, 7, 4]\n",
      "episode 2563, reward 836.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 119\n",
      "Initial State is  [3, 3, 3]\n",
      "episode 2564, reward 1022.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 114\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 2565, reward 1018.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 116\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 2566, reward 897.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 118\n",
      "Initial State is  [1, 17, 5]\n",
      "episode 2567, reward 1513.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 125\n",
      "Initial State is  [0, 0, 1]\n",
      "episode 2568, reward 1087.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 120\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 2569, reward 960.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 107\n",
      "Initial State is  [0, 19, 4]\n",
      "episode 2570, reward 1098.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 118\n",
      "Initial State is  [0, 2, 6]\n",
      "episode 2571, reward 762.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 117\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 2572, reward 1211.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 117\n",
      "Initial State is  [4, 22, 2]\n",
      "episode 2573, reward 1430.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 129\n",
      "Initial State is  [1, 3, 0]\n",
      "episode 2574, reward 1162.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 115\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 2575, reward 1332.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 121\n",
      "Initial State is  [4, 18, 2]\n",
      "episode 2576, reward 1063.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 131\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 2577, reward 1173.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 120\n",
      "Initial State is  [4, 12, 5]\n",
      "episode 2578, reward 1054.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 118\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 2579, reward 1027.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 123\n",
      "Initial State is  [0, 4, 4]\n",
      "episode 2580, reward 1050.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 114\n",
      "Initial State is  [4, 18, 6]\n",
      "episode 2581, reward 1073.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 116\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 2582, reward 1172.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 121\n",
      "Initial State is  [2, 2, 0]\n",
      "episode 2583, reward 1203.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 115\n",
      "Initial State is  [0, 17, 2]\n",
      "episode 2584, reward 904.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 123\n",
      "Initial State is  [2, 5, 3]\n",
      "episode 2585, reward 795.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 117\n",
      "Initial State is  [0, 21, 5]\n",
      "episode 2586, reward 1245.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 129\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 2587, reward 876.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 115\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 2588, reward 834.0, memory_length 2000, epsilon 0.0009954703940636294, time 720.0, rides 123\n",
      "Initial State is  [2, 23, 5]\n",
      "episode 2589, reward 1068.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 111\n",
      "Initial State is  [4, 2, 2]\n",
      "episode 2590, reward 1342.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 126\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 2591, reward 1026.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 111\n",
      "Initial State is  [0, 20, 3]\n",
      "episode 2592, reward 839.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 128\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 2593, reward 1041.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 111\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 2594, reward 1032.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 117\n",
      "Initial State is  [3, 0, 0]\n",
      "episode 2595, reward 1028.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 117\n",
      "Initial State is  [2, 1, 1]\n",
      "episode 2596, reward 1146.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 114\n",
      "Initial State is  [0, 12, 1]\n",
      "episode 2597, reward 1202.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 111\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 2598, reward 1142.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 124\n",
      "Initial State is  [3, 22, 4]\n",
      "episode 2599, reward 952.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 122\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 2600, reward 1344.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 123\n",
      "Initial State is  [0, 8, 2]\n",
      "episode 2601, reward 1057.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 124\n",
      "Initial State is  [4, 9, 4]\n",
      "episode 2602, reward 1303.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 115\n",
      "Initial State is  [4, 14, 1]\n",
      "episode 2603, reward 1000.0, memory_length 2000, epsilon 0.0009954703940636294, time 741.0, rides 109\n",
      "Initial State is  [2, 5, 6]\n",
      "episode 2604, reward 1043.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 116\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 2605, reward 1194.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 130\n",
      "Initial State is  [1, 21, 5]\n",
      "episode 2606, reward 1341.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 117\n",
      "Initial State is  [0, 20, 4]\n",
      "episode 2607, reward 970.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 114\n",
      "Initial State is  [2, 3, 2]\n",
      "episode 2608, reward 998.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 114\n",
      "Initial State is  [4, 0, 4]\n",
      "episode 2609, reward 1020.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 121\n",
      "Initial State is  [1, 2, 3]\n",
      "episode 2610, reward 1182.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 120\n",
      "Initial State is  [3, 8, 0]\n",
      "episode 2611, reward 1247.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 121\n",
      "Initial State is  [3, 23, 0]\n",
      "episode 2612, reward 1062.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 118\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 2613, reward 1118.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 115\n",
      "Initial State is  [1, 21, 5]\n",
      "episode 2614, reward 783.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 108\n",
      "Initial State is  [2, 5, 6]\n",
      "episode 2615, reward 1158.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 118\n",
      "Initial State is  [2, 9, 3]\n",
      "episode 2616, reward 1005.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 106\n",
      "Initial State is  [2, 1, 4]\n",
      "episode 2617, reward 1031.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 128\n",
      "Initial State is  [4, 18, 6]\n",
      "episode 2618, reward 1307.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 124\n",
      "Initial State is  [3, 2, 6]\n",
      "episode 2619, reward 1126.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 126\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 2620, reward 1092.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 126\n",
      "Initial State is  [2, 18, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2621, reward 1181.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 131\n",
      "Initial State is  [2, 12, 5]\n",
      "episode 2622, reward 1092.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 130\n",
      "Initial State is  [0, 19, 3]\n",
      "episode 2623, reward 1084.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 120\n",
      "Initial State is  [3, 11, 0]\n",
      "episode 2624, reward 1095.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 119\n",
      "Initial State is  [3, 8, 3]\n",
      "episode 2625, reward 1058.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 126\n",
      "Initial State is  [0, 10, 5]\n",
      "episode 2626, reward 1078.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 117\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 2627, reward 793.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 106\n",
      "Initial State is  [1, 21, 0]\n",
      "episode 2628, reward 1487.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 116\n",
      "Initial State is  [3, 5, 3]\n",
      "episode 2629, reward 1123.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 120\n",
      "Initial State is  [2, 6, 4]\n",
      "episode 2630, reward 1196.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 110\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 2631, reward 1140.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 126\n",
      "Initial State is  [1, 5, 1]\n",
      "episode 2632, reward 833.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 120\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 2633, reward 1152.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 113\n",
      "Initial State is  [3, 19, 2]\n",
      "episode 2634, reward 905.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 113\n",
      "Initial State is  [0, 5, 2]\n",
      "episode 2635, reward 1104.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 134\n",
      "Initial State is  [4, 6, 2]\n",
      "episode 2636, reward 1236.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 114\n",
      "Initial State is  [1, 5, 1]\n",
      "episode 2637, reward 955.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 105\n",
      "Initial State is  [4, 18, 6]\n",
      "episode 2638, reward 1147.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 126\n",
      "Initial State is  [2, 20, 2]\n",
      "episode 2639, reward 1313.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 116\n",
      "Initial State is  [2, 3, 1]\n",
      "episode 2640, reward 1265.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 121\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 2641, reward 1082.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 119\n",
      "Initial State is  [0, 20, 0]\n",
      "episode 2642, reward 1123.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 115\n",
      "Initial State is  [3, 19, 4]\n",
      "episode 2643, reward 1254.0, memory_length 2000, epsilon 0.0009954703940636294, time 720.0, rides 119\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 2644, reward 864.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 112\n",
      "Initial State is  [3, 10, 1]\n",
      "episode 2645, reward 1266.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 127\n",
      "Initial State is  [4, 3, 3]\n",
      "episode 2646, reward 1073.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 122\n",
      "Initial State is  [3, 9, 5]\n",
      "episode 2647, reward 1070.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 108\n",
      "Initial State is  [0, 21, 6]\n",
      "episode 2648, reward 1037.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 120\n",
      "Initial State is  [3, 17, 5]\n",
      "episode 2649, reward 1344.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 131\n",
      "Initial State is  [4, 20, 2]\n",
      "episode 2650, reward 972.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 119\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 2651, reward 1103.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 117\n",
      "Initial State is  [2, 12, 6]\n",
      "episode 2652, reward 989.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 119\n",
      "Initial State is  [4, 1, 2]\n",
      "episode 2653, reward 1066.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 108\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 2654, reward 1362.0, memory_length 2000, epsilon 0.0009954703940636294, time 739.0, rides 118\n",
      "Initial State is  [4, 14, 4]\n",
      "episode 2655, reward 1339.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 123\n",
      "Initial State is  [1, 13, 0]\n",
      "episode 2656, reward 1108.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 115\n",
      "Initial State is  [2, 6, 1]\n",
      "episode 2657, reward 919.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 114\n",
      "Initial State is  [0, 2, 6]\n",
      "episode 2658, reward 1094.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 134\n",
      "Initial State is  [1, 11, 0]\n",
      "episode 2659, reward 1276.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 125\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 2660, reward 1255.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 123\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 2661, reward 1187.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 123\n",
      "Initial State is  [3, 8, 1]\n",
      "episode 2662, reward 885.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 111\n",
      "Initial State is  [2, 20, 1]\n",
      "episode 2663, reward 839.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 120\n",
      "Initial State is  [3, 8, 1]\n",
      "episode 2664, reward 1215.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 127\n",
      "Initial State is  [0, 18, 4]\n",
      "episode 2665, reward 676.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 125\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 2666, reward 1078.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 129\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 2667, reward 1155.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 116\n",
      "Initial State is  [2, 2, 4]\n",
      "episode 2668, reward 1256.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 128\n",
      "Initial State is  [3, 19, 0]\n",
      "episode 2669, reward 1283.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 122\n",
      "Initial State is  [3, 19, 0]\n",
      "episode 2670, reward 1322.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 117\n",
      "Initial State is  [2, 2, 5]\n",
      "episode 2671, reward 1206.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 113\n",
      "Initial State is  [1, 22, 2]\n",
      "episode 2672, reward 1288.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 109\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 2673, reward 928.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 116\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 2674, reward 1461.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 128\n",
      "Initial State is  [2, 3, 0]\n",
      "episode 2675, reward 1123.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 115\n",
      "Initial State is  [1, 18, 3]\n",
      "episode 2676, reward 1164.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 118\n",
      "Initial State is  [1, 1, 5]\n",
      "episode 2677, reward 770.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 111\n",
      "Initial State is  [4, 11, 0]\n",
      "episode 2678, reward 1270.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 110\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 2679, reward 1301.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 119\n",
      "Initial State is  [2, 4, 1]\n",
      "episode 2680, reward 1125.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 112\n",
      "Initial State is  [4, 21, 3]\n",
      "episode 2681, reward 1003.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 123\n",
      "Initial State is  [3, 2, 3]\n",
      "episode 2682, reward 1022.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 123\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 2683, reward 1138.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 114\n",
      "Initial State is  [1, 1, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2684, reward 1151.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 119\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 2685, reward 1200.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 117\n",
      "Initial State is  [1, 18, 3]\n",
      "episode 2686, reward 1258.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 122\n",
      "Initial State is  [4, 9, 3]\n",
      "episode 2687, reward 1119.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 117\n",
      "Initial State is  [0, 5, 5]\n",
      "episode 2688, reward 979.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 117\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 2689, reward 777.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 110\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 2690, reward 898.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 126\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 2691, reward 918.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 121\n",
      "Initial State is  [2, 4, 2]\n",
      "episode 2692, reward 1071.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 112\n",
      "Initial State is  [3, 11, 0]\n",
      "episode 2693, reward 1087.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 115\n",
      "Initial State is  [0, 0, 2]\n",
      "episode 2694, reward 1033.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 123\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 2695, reward 1007.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 118\n",
      "Initial State is  [0, 16, 2]\n",
      "episode 2696, reward 1332.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 125\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 2697, reward 1479.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 122\n",
      "Initial State is  [4, 17, 3]\n",
      "episode 2698, reward 1340.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 118\n",
      "Initial State is  [0, 23, 4]\n",
      "episode 2699, reward 1111.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 108\n",
      "Initial State is  [0, 6, 3]\n",
      "episode 2700, reward 1266.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 108\n",
      "Initial State is  [4, 2, 2]\n",
      "episode 2701, reward 1026.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 120\n",
      "Initial State is  [3, 2, 1]\n",
      "episode 2702, reward 1127.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 113\n",
      "Initial State is  [2, 15, 6]\n",
      "episode 2703, reward 1325.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 126\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 2704, reward 1197.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 115\n",
      "Initial State is  [3, 23, 6]\n",
      "episode 2705, reward 821.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 122\n",
      "Initial State is  [1, 4, 1]\n",
      "episode 2706, reward 784.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 106\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 2707, reward 1175.0, memory_length 2000, epsilon 0.0009954703940636294, time 740.0, rides 119\n",
      "Initial State is  [3, 6, 4]\n",
      "episode 2708, reward 1214.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 106\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 2709, reward 1222.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 124\n",
      "Initial State is  [4, 23, 4]\n",
      "episode 2710, reward 1116.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 115\n",
      "Initial State is  [4, 15, 0]\n",
      "episode 2711, reward 1126.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 114\n",
      "Initial State is  [1, 18, 4]\n",
      "episode 2712, reward 1087.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 114\n",
      "Initial State is  [3, 21, 3]\n",
      "episode 2713, reward 946.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 117\n",
      "Initial State is  [1, 4, 3]\n",
      "episode 2714, reward 1075.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 130\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 2715, reward 1240.0, memory_length 2000, epsilon 0.0009954703940636294, time 740.0, rides 117\n",
      "Initial State is  [2, 17, 4]\n",
      "episode 2716, reward 1098.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 126\n",
      "Initial State is  [4, 14, 6]\n",
      "episode 2717, reward 1219.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 126\n",
      "Initial State is  [3, 19, 0]\n",
      "episode 2718, reward 1017.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 119\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 2719, reward 792.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 116\n",
      "Initial State is  [0, 12, 1]\n",
      "episode 2720, reward 1360.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 122\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 2721, reward 1058.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 114\n",
      "Initial State is  [0, 5, 3]\n",
      "episode 2722, reward 1227.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 118\n",
      "Initial State is  [4, 10, 6]\n",
      "episode 2723, reward 1161.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 129\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 2724, reward 1247.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 126\n",
      "Initial State is  [1, 4, 6]\n",
      "episode 2725, reward 929.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 113\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 2726, reward 724.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 114\n",
      "Initial State is  [0, 18, 1]\n",
      "episode 2727, reward 1188.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 121\n",
      "Initial State is  [4, 6, 5]\n",
      "episode 2728, reward 1386.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 126\n",
      "Initial State is  [4, 13, 5]\n",
      "episode 2729, reward 1127.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 112\n",
      "Initial State is  [2, 9, 0]\n",
      "episode 2730, reward 1216.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 122\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 2731, reward 993.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 114\n",
      "Initial State is  [0, 17, 0]\n",
      "episode 2732, reward 917.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 128\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 2733, reward 1158.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 123\n",
      "Initial State is  [4, 13, 3]\n",
      "episode 2734, reward 988.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 120\n",
      "Initial State is  [0, 21, 4]\n",
      "episode 2735, reward 1243.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 120\n",
      "Initial State is  [4, 14, 3]\n",
      "episode 2736, reward 1371.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 123\n",
      "Initial State is  [1, 18, 3]\n",
      "episode 2737, reward 983.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 121\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 2738, reward 1281.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 130\n",
      "Initial State is  [1, 4, 1]\n",
      "episode 2739, reward 1231.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 109\n",
      "Initial State is  [0, 5, 2]\n",
      "episode 2740, reward 1281.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 122\n",
      "Initial State is  [0, 11, 2]\n",
      "episode 2741, reward 1306.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 125\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 2742, reward 1236.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 108\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 2743, reward 1237.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 120\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 2744, reward 1114.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 123\n",
      "Initial State is  [4, 1, 2]\n",
      "episode 2745, reward 994.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 116\n",
      "Initial State is  [0, 4, 1]\n",
      "episode 2746, reward 1190.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 117\n",
      "Initial State is  [3, 14, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2747, reward 1339.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 118\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 2748, reward 1046.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 111\n",
      "Initial State is  [4, 21, 5]\n",
      "episode 2749, reward 1137.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 122\n",
      "Initial State is  [2, 8, 6]\n",
      "episode 2750, reward 1172.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 118\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 2751, reward 1010.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 120\n",
      "Initial State is  [3, 6, 2]\n",
      "episode 2752, reward 1051.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 117\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 2753, reward 1166.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 114\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 2754, reward 1170.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 116\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 2755, reward 1053.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 108\n",
      "Initial State is  [2, 16, 0]\n",
      "episode 2756, reward 1088.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 119\n",
      "Initial State is  [4, 1, 0]\n",
      "episode 2757, reward 1322.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 116\n",
      "Initial State is  [0, 23, 2]\n",
      "episode 2758, reward 1148.0, memory_length 2000, epsilon 0.0009954703940636294, time 739.0, rides 116\n",
      "Initial State is  [1, 1, 0]\n",
      "episode 2759, reward 1153.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 130\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 2760, reward 1212.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 119\n",
      "Initial State is  [1, 12, 2]\n",
      "episode 2761, reward 1140.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 115\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 2762, reward 1216.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 113\n",
      "Initial State is  [0, 1, 4]\n",
      "episode 2763, reward 1247.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 118\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 2764, reward 835.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 112\n",
      "Initial State is  [2, 6, 0]\n",
      "episode 2765, reward 729.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 108\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 2766, reward 1094.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 128\n",
      "Initial State is  [3, 13, 6]\n",
      "episode 2767, reward 944.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 105\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 2768, reward 978.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 112\n",
      "Initial State is  [1, 21, 2]\n",
      "episode 2769, reward 1253.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 111\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 2770, reward 1348.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 122\n",
      "Initial State is  [0, 4, 4]\n",
      "episode 2771, reward 1090.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 121\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 2772, reward 1211.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 111\n",
      "Initial State is  [4, 0, 4]\n",
      "episode 2773, reward 877.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 114\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 2774, reward 1370.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 127\n",
      "Initial State is  [3, 13, 3]\n",
      "episode 2775, reward 1122.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 116\n",
      "Initial State is  [3, 20, 5]\n",
      "episode 2776, reward 1132.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 115\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 2777, reward 1434.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 126\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 2778, reward 1112.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 111\n",
      "Initial State is  [0, 20, 2]\n",
      "episode 2779, reward 1087.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 108\n",
      "Initial State is  [0, 4, 0]\n",
      "episode 2780, reward 1328.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 118\n",
      "Initial State is  [0, 6, 5]\n",
      "episode 2781, reward 1304.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 117\n",
      "Initial State is  [2, 16, 0]\n",
      "episode 2782, reward 1156.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 117\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 2783, reward 1275.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 136\n",
      "Initial State is  [2, 21, 4]\n",
      "episode 2784, reward 1244.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 120\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 2785, reward 1242.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 119\n",
      "Initial State is  [0, 15, 0]\n",
      "episode 2786, reward 1180.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 127\n",
      "Initial State is  [2, 22, 2]\n",
      "episode 2787, reward 1125.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 114\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 2788, reward 1299.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 131\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 2789, reward 1075.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 111\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 2790, reward 1116.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 120\n",
      "Initial State is  [4, 21, 6]\n",
      "episode 2791, reward 1148.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 100\n",
      "Initial State is  [2, 8, 0]\n",
      "episode 2792, reward 1113.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 113\n",
      "Initial State is  [4, 0, 6]\n",
      "episode 2793, reward 1154.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 121\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 2794, reward 1016.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 126\n",
      "Initial State is  [3, 20, 1]\n",
      "episode 2795, reward 1152.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 114\n",
      "Initial State is  [0, 2, 4]\n",
      "episode 2796, reward 1104.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 119\n",
      "Initial State is  [1, 2, 2]\n",
      "episode 2797, reward 1037.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 115\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 2798, reward 1133.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 111\n",
      "Initial State is  [3, 23, 1]\n",
      "episode 2799, reward 1333.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 126\n",
      "Initial State is  [1, 3, 1]\n",
      "episode 2800, reward 1199.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 117\n",
      "Initial State is  [4, 6, 2]\n",
      "episode 2801, reward 818.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 124\n",
      "Initial State is  [1, 15, 3]\n",
      "episode 2802, reward 1343.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 116\n",
      "Initial State is  [4, 2, 2]\n",
      "episode 2803, reward 1239.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 113\n",
      "Initial State is  [2, 15, 6]\n",
      "episode 2804, reward 1008.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 128\n",
      "Initial State is  [4, 12, 6]\n",
      "episode 2805, reward 1213.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 128\n",
      "Initial State is  [1, 20, 5]\n",
      "episode 2806, reward 1032.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 116\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 2807, reward 1138.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 125\n",
      "Initial State is  [4, 19, 3]\n",
      "episode 2808, reward 1452.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 130\n",
      "Initial State is  [3, 23, 4]\n",
      "episode 2809, reward 1163.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 124\n",
      "Initial State is  [3, 4, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2810, reward 1004.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 109\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 2811, reward 1392.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 128\n",
      "Initial State is  [3, 19, 4]\n",
      "episode 2812, reward 849.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 115\n",
      "Initial State is  [0, 23, 2]\n",
      "episode 2813, reward 1244.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 123\n",
      "Initial State is  [4, 14, 2]\n",
      "episode 2814, reward 970.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 110\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 2815, reward 998.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 118\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 2816, reward 1218.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 131\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 2817, reward 925.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 115\n",
      "Initial State is  [0, 5, 5]\n",
      "episode 2818, reward 1080.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 113\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 2819, reward 1244.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 112\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 2820, reward 1534.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 115\n",
      "Initial State is  [4, 8, 0]\n",
      "episode 2821, reward 1328.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 126\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 2822, reward 1354.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 123\n",
      "Initial State is  [0, 15, 3]\n",
      "episode 2823, reward 1097.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 116\n",
      "Initial State is  [4, 7, 3]\n",
      "episode 2824, reward 958.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 121\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 2825, reward 1220.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 117\n",
      "Initial State is  [2, 15, 6]\n",
      "episode 2826, reward 1186.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 122\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 2827, reward 1280.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 115\n",
      "Initial State is  [4, 21, 0]\n",
      "episode 2828, reward 1142.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 125\n",
      "Initial State is  [3, 5, 3]\n",
      "episode 2829, reward 1310.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 122\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 2830, reward 1049.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 122\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 2831, reward 1194.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 119\n",
      "Initial State is  [1, 1, 2]\n",
      "episode 2832, reward 1268.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 128\n",
      "Initial State is  [0, 10, 2]\n",
      "episode 2833, reward 1393.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 130\n",
      "Initial State is  [2, 23, 3]\n",
      "episode 2834, reward 1098.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 118\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 2835, reward 1330.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 125\n",
      "Initial State is  [2, 0, 1]\n",
      "episode 2836, reward 1175.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 120\n",
      "Initial State is  [4, 2, 4]\n",
      "episode 2837, reward 1107.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 111\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 2838, reward 1243.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 133\n",
      "Initial State is  [1, 11, 5]\n",
      "episode 2839, reward 1108.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 117\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 2840, reward 1115.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 121\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 2841, reward 873.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 113\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 2842, reward 1077.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 123\n",
      "Initial State is  [4, 8, 3]\n",
      "episode 2843, reward 1132.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 111\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 2844, reward 1001.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 121\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 2845, reward 1125.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 114\n",
      "Initial State is  [2, 22, 4]\n",
      "episode 2846, reward 1274.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 118\n",
      "Initial State is  [3, 7, 6]\n",
      "episode 2847, reward 1023.0, memory_length 2000, epsilon 0.0009954703940636294, time 746.0, rides 118\n",
      "Initial State is  [3, 10, 6]\n",
      "episode 2848, reward 1246.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 111\n",
      "Initial State is  [4, 9, 4]\n",
      "episode 2849, reward 1149.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 116\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 2850, reward 878.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 118\n",
      "Initial State is  [3, 5, 5]\n",
      "episode 2851, reward 1106.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 116\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 2852, reward 1482.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 130\n",
      "Initial State is  [0, 20, 0]\n",
      "episode 2853, reward 1296.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 124\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 2854, reward 1001.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 125\n",
      "Initial State is  [2, 11, 4]\n",
      "episode 2855, reward 1339.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 127\n",
      "Initial State is  [2, 23, 4]\n",
      "episode 2856, reward 845.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 112\n",
      "Initial State is  [1, 11, 0]\n",
      "episode 2857, reward 1160.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 111\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 2858, reward 1084.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 122\n",
      "Initial State is  [2, 17, 4]\n",
      "episode 2859, reward 922.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 116\n",
      "Initial State is  [0, 9, 4]\n",
      "episode 2860, reward 1100.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 129\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 2861, reward 969.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 118\n",
      "Initial State is  [3, 14, 0]\n",
      "episode 2862, reward 1241.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 121\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 2863, reward 1169.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 121\n",
      "Initial State is  [2, 16, 0]\n",
      "episode 2864, reward 1072.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 122\n",
      "Initial State is  [3, 10, 5]\n",
      "episode 2865, reward 1097.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 106\n",
      "Initial State is  [0, 14, 1]\n",
      "episode 2866, reward 1340.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 126\n",
      "Initial State is  [1, 11, 0]\n",
      "episode 2867, reward 1125.0, memory_length 2000, epsilon 0.0009954703940636294, time 740.0, rides 131\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 2868, reward 1001.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 115\n",
      "Initial State is  [1, 6, 4]\n",
      "episode 2869, reward 1161.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 114\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 2870, reward 1223.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 112\n",
      "Initial State is  [1, 22, 2]\n",
      "episode 2871, reward 1134.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 119\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 2872, reward 1167.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 120\n",
      "Initial State is  [0, 14, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2873, reward 1117.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 116\n",
      "Initial State is  [2, 4, 0]\n",
      "episode 2874, reward 879.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 115\n",
      "Initial State is  [4, 17, 3]\n",
      "episode 2875, reward 1009.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 117\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 2876, reward 1046.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 120\n",
      "Initial State is  [1, 7, 6]\n",
      "episode 2877, reward 1049.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 109\n",
      "Initial State is  [0, 19, 6]\n",
      "episode 2878, reward 1260.0, memory_length 2000, epsilon 0.0009954703940636294, time 739.0, rides 113\n",
      "Initial State is  [2, 16, 4]\n",
      "episode 2879, reward 1154.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 107\n",
      "Initial State is  [0, 23, 2]\n",
      "episode 2880, reward 1057.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 111\n",
      "Initial State is  [3, 21, 5]\n",
      "episode 2881, reward 1266.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 128\n",
      "Initial State is  [1, 12, 0]\n",
      "episode 2882, reward 1014.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 127\n",
      "Initial State is  [4, 7, 1]\n",
      "episode 2883, reward 1254.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 120\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 2884, reward 1086.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 106\n",
      "Initial State is  [3, 8, 6]\n",
      "episode 2885, reward 978.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 113\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 2886, reward 1229.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 122\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 2887, reward 1380.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 127\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 2888, reward 893.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 111\n",
      "Initial State is  [0, 5, 5]\n",
      "episode 2889, reward 1187.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 124\n",
      "Initial State is  [3, 23, 3]\n",
      "episode 2890, reward 943.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 115\n",
      "Initial State is  [1, 6, 0]\n",
      "episode 2891, reward 1280.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 122\n",
      "Initial State is  [0, 8, 4]\n",
      "episode 2892, reward 1123.0, memory_length 2000, epsilon 0.0009954703940636294, time 742.0, rides 128\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 2893, reward 1077.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 110\n",
      "Initial State is  [0, 21, 6]\n",
      "episode 2894, reward 1187.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 116\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 2895, reward 1290.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 112\n",
      "Initial State is  [4, 9, 5]\n",
      "episode 2896, reward 958.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 114\n",
      "Initial State is  [2, 4, 0]\n",
      "episode 2897, reward 1243.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 130\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 2898, reward 1142.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 116\n",
      "Initial State is  [2, 11, 2]\n",
      "episode 2899, reward 764.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 117\n",
      "Initial State is  [1, 18, 1]\n",
      "episode 2900, reward 1193.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 113\n",
      "Initial State is  [0, 4, 3]\n",
      "episode 2901, reward 1312.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 117\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 2902, reward 974.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 113\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 2903, reward 900.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 117\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 2904, reward 895.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 108\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 2905, reward 973.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 112\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 2906, reward 1084.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 116\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 2907, reward 1084.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 119\n",
      "Initial State is  [1, 17, 4]\n",
      "episode 2908, reward 1304.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 117\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 2909, reward 1351.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 122\n",
      "Initial State is  [3, 0, 1]\n",
      "episode 2910, reward 872.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 110\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 2911, reward 1277.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 131\n",
      "Initial State is  [3, 17, 4]\n",
      "episode 2912, reward 1125.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 122\n",
      "Initial State is  [4, 5, 6]\n",
      "episode 2913, reward 1273.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 125\n",
      "Initial State is  [2, 3, 5]\n",
      "episode 2914, reward 1186.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 109\n",
      "Initial State is  [1, 2, 0]\n",
      "episode 2915, reward 1004.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 109\n",
      "Initial State is  [0, 2, 6]\n",
      "episode 2916, reward 1198.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 122\n",
      "Initial State is  [3, 8, 0]\n",
      "episode 2917, reward 1057.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 120\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 2918, reward 965.0, memory_length 2000, epsilon 0.0009954703940636294, time 740.0, rides 126\n",
      "Initial State is  [2, 8, 1]\n",
      "episode 2919, reward 1186.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 116\n",
      "Initial State is  [4, 9, 3]\n",
      "episode 2920, reward 1242.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 125\n",
      "Initial State is  [0, 12, 2]\n",
      "episode 2921, reward 1287.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 114\n",
      "Initial State is  [2, 22, 3]\n",
      "episode 2922, reward 1233.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 113\n",
      "Initial State is  [4, 14, 1]\n",
      "episode 2923, reward 1496.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 119\n",
      "Initial State is  [0, 3, 3]\n",
      "episode 2924, reward 1213.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 119\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 2925, reward 1343.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 119\n",
      "Initial State is  [2, 16, 5]\n",
      "episode 2926, reward 1023.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 120\n",
      "Initial State is  [0, 8, 2]\n",
      "episode 2927, reward 1486.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 120\n",
      "Initial State is  [0, 12, 3]\n",
      "episode 2928, reward 1349.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 124\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 2929, reward 1091.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 108\n",
      "Initial State is  [3, 10, 3]\n",
      "episode 2930, reward 1065.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 113\n",
      "Initial State is  [1, 21, 5]\n",
      "episode 2931, reward 1218.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 122\n",
      "Initial State is  [3, 15, 2]\n",
      "episode 2932, reward 1145.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 116\n",
      "Initial State is  [4, 19, 3]\n",
      "episode 2933, reward 928.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 124\n",
      "Initial State is  [0, 4, 5]\n",
      "episode 2934, reward 1181.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 114\n",
      "Initial State is  [3, 7, 0]\n",
      "episode 2935, reward 1035.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 118\n",
      "Initial State is  [0, 1, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2936, reward 917.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 110\n",
      "Initial State is  [1, 0, 5]\n",
      "episode 2937, reward 1072.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 121\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 2938, reward 1089.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 115\n",
      "Initial State is  [1, 22, 4]\n",
      "episode 2939, reward 1254.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 121\n",
      "Initial State is  [4, 5, 5]\n",
      "episode 2940, reward 1215.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 120\n",
      "Initial State is  [1, 23, 2]\n",
      "episode 2941, reward 1114.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 112\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 2942, reward 832.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 113\n",
      "Initial State is  [0, 8, 1]\n",
      "episode 2943, reward 1252.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 133\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 2944, reward 1140.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 125\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 2945, reward 1094.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 118\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 2946, reward 1322.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 119\n",
      "Initial State is  [1, 23, 6]\n",
      "episode 2947, reward 940.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 126\n",
      "Initial State is  [1, 0, 4]\n",
      "episode 2948, reward 1412.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 123\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 2949, reward 963.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 121\n",
      "Initial State is  [1, 13, 2]\n",
      "episode 2950, reward 670.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 110\n",
      "Initial State is  [3, 0, 5]\n",
      "episode 2951, reward 1147.0, memory_length 2000, epsilon 0.0009954703940636294, time 743.0, rides 105\n",
      "Initial State is  [4, 11, 4]\n",
      "episode 2952, reward 853.0, memory_length 2000, epsilon 0.0009954703940636294, time 748.0, rides 116\n",
      "Initial State is  [4, 18, 6]\n",
      "episode 2953, reward 1258.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 118\n",
      "Initial State is  [2, 3, 1]\n",
      "episode 2954, reward 1124.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 109\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 2955, reward 1485.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 131\n",
      "Initial State is  [3, 18, 6]\n",
      "episode 2956, reward 1174.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 122\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 2957, reward 1225.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 130\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 2958, reward 1337.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 117\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 2959, reward 983.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 107\n",
      "Initial State is  [4, 5, 5]\n",
      "episode 2960, reward 1401.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 122\n",
      "Initial State is  [1, 7, 6]\n",
      "episode 2961, reward 1466.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 135\n",
      "Initial State is  [0, 8, 4]\n",
      "episode 2962, reward 983.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 116\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 2963, reward 1047.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 125\n",
      "Initial State is  [1, 20, 6]\n",
      "episode 2964, reward 1103.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 119\n",
      "Initial State is  [0, 2, 4]\n",
      "episode 2965, reward 1274.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 123\n",
      "Initial State is  [1, 15, 0]\n",
      "episode 2966, reward 1061.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 117\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 2967, reward 1073.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 123\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 2968, reward 1063.0, memory_length 2000, epsilon 0.0009954703940636294, time 741.0, rides 117\n",
      "Initial State is  [3, 23, 1]\n",
      "episode 2969, reward 1373.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 117\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 2970, reward 1160.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 114\n",
      "Initial State is  [4, 4, 3]\n",
      "episode 2971, reward 1022.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 123\n",
      "Initial State is  [3, 17, 5]\n",
      "episode 2972, reward 969.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 117\n",
      "Initial State is  [3, 3, 3]\n",
      "episode 2973, reward 1259.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 131\n",
      "Initial State is  [3, 8, 2]\n",
      "episode 2974, reward 1324.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 125\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 2975, reward 1142.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 125\n",
      "Initial State is  [1, 19, 5]\n",
      "episode 2976, reward 1102.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 120\n",
      "Initial State is  [4, 8, 2]\n",
      "episode 2977, reward 1167.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 115\n",
      "Initial State is  [3, 12, 1]\n",
      "episode 2978, reward 970.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 113\n",
      "Initial State is  [0, 2, 5]\n",
      "episode 2979, reward 1358.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 116\n",
      "Initial State is  [3, 16, 0]\n",
      "episode 2980, reward 888.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 117\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 2981, reward 1183.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 117\n",
      "Initial State is  [1, 21, 1]\n",
      "episode 2982, reward 1137.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 112\n",
      "Initial State is  [2, 20, 6]\n",
      "episode 2983, reward 1389.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 118\n",
      "Initial State is  [0, 12, 1]\n",
      "episode 2984, reward 900.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 112\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 2985, reward 876.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 113\n",
      "Initial State is  [1, 10, 5]\n",
      "episode 2986, reward 1128.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 122\n",
      "Initial State is  [2, 22, 5]\n",
      "episode 2987, reward 972.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 111\n",
      "Initial State is  [3, 8, 5]\n",
      "episode 2988, reward 1364.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 114\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 2989, reward 948.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 116\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 2990, reward 1217.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 115\n",
      "Initial State is  [0, 20, 3]\n",
      "episode 2991, reward 1197.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 127\n",
      "Initial State is  [3, 20, 2]\n",
      "episode 2992, reward 946.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 123\n",
      "Initial State is  [0, 14, 0]\n",
      "episode 2993, reward 1037.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 119\n",
      "Initial State is  [0, 9, 3]\n",
      "episode 2994, reward 1048.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 128\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 2995, reward 1016.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 114\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 2996, reward 1171.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 125\n",
      "Initial State is  [4, 15, 4]\n",
      "episode 2997, reward 1166.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 120\n",
      "Initial State is  [2, 6, 0]\n",
      "episode 2998, reward 818.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 122\n",
      "Initial State is  [0, 21, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2999, reward 1143.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 124\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 3000, reward 1249.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 118\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 3001, reward 1240.0, memory_length 2000, epsilon 0.0009954703940636294, time 741.0, rides 114\n",
      "Initial State is  [2, 16, 5]\n",
      "episode 3002, reward 1043.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 121\n",
      "Initial State is  [3, 16, 1]\n",
      "episode 3003, reward 980.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 121\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 3004, reward 821.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 121\n",
      "Initial State is  [3, 10, 2]\n",
      "episode 3005, reward 1259.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 119\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 3006, reward 1180.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 116\n",
      "Initial State is  [1, 8, 1]\n",
      "episode 3007, reward 1329.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 126\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 3008, reward 819.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 114\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 3009, reward 1347.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 130\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 3010, reward 945.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 120\n",
      "Initial State is  [3, 3, 3]\n",
      "episode 3011, reward 998.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 115\n",
      "Initial State is  [4, 15, 3]\n",
      "episode 3012, reward 1217.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 109\n",
      "Initial State is  [2, 20, 6]\n",
      "episode 3013, reward 1245.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 120\n",
      "Initial State is  [4, 7, 0]\n",
      "episode 3014, reward 1178.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 122\n",
      "Initial State is  [0, 5, 0]\n",
      "episode 3015, reward 1031.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 105\n",
      "Initial State is  [4, 7, 1]\n",
      "episode 3016, reward 1251.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 118\n",
      "Initial State is  [1, 5, 6]\n",
      "episode 3017, reward 1127.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 116\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 3018, reward 703.0, memory_length 2000, epsilon 0.0009954703940636294, time 739.0, rides 112\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 3019, reward 1079.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 111\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 3020, reward 1063.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 121\n",
      "Initial State is  [0, 22, 1]\n",
      "episode 3021, reward 946.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 116\n",
      "Initial State is  [1, 21, 3]\n",
      "episode 3022, reward 1107.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 113\n",
      "Initial State is  [1, 0, 0]\n",
      "episode 3023, reward 1473.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 114\n",
      "Initial State is  [3, 13, 4]\n",
      "episode 3024, reward 976.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 118\n",
      "Initial State is  [4, 2, 6]\n",
      "episode 3025, reward 636.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 114\n",
      "Initial State is  [1, 12, 0]\n",
      "episode 3026, reward 1084.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 121\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 3027, reward 1179.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 122\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 3028, reward 1066.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 123\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 3029, reward 1249.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 117\n",
      "Initial State is  [4, 7, 4]\n",
      "episode 3030, reward 1043.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 123\n",
      "Initial State is  [2, 10, 3]\n",
      "episode 3031, reward 819.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 108\n",
      "Initial State is  [0, 17, 3]\n",
      "episode 3032, reward 1233.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 113\n",
      "Initial State is  [1, 7, 3]\n",
      "episode 3033, reward 1124.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 119\n",
      "Initial State is  [2, 16, 5]\n",
      "episode 3034, reward 1269.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 121\n",
      "Initial State is  [2, 15, 1]\n",
      "episode 3035, reward 1212.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 123\n",
      "Initial State is  [4, 15, 6]\n",
      "episode 3036, reward 1103.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 126\n",
      "Initial State is  [1, 13, 6]\n",
      "episode 3037, reward 1139.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 119\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 3038, reward 1030.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 120\n",
      "Initial State is  [1, 21, 6]\n",
      "episode 3039, reward 1125.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 126\n",
      "Initial State is  [2, 0, 1]\n",
      "episode 3040, reward 1141.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 125\n",
      "Initial State is  [3, 8, 1]\n",
      "episode 3041, reward 1044.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 130\n",
      "Initial State is  [4, 21, 4]\n",
      "episode 3042, reward 1261.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 131\n",
      "Initial State is  [3, 21, 0]\n",
      "episode 3043, reward 1206.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 126\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 3044, reward 1135.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 131\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 3045, reward 1277.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 115\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 3046, reward 1113.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 105\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 3047, reward 1057.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 114\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 3048, reward 1378.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 116\n",
      "Initial State is  [2, 16, 0]\n",
      "episode 3049, reward 1244.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 133\n",
      "Initial State is  [2, 23, 3]\n",
      "episode 3050, reward 1390.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 116\n",
      "Initial State is  [4, 10, 4]\n",
      "episode 3051, reward 1299.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 127\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 3052, reward 1090.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 115\n",
      "Initial State is  [2, 17, 3]\n",
      "episode 3053, reward 1049.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 125\n",
      "Initial State is  [3, 17, 2]\n",
      "episode 3054, reward 1106.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 126\n",
      "Initial State is  [2, 13, 5]\n",
      "episode 3055, reward 869.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 131\n",
      "Initial State is  [4, 16, 6]\n",
      "episode 3056, reward 1165.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 120\n",
      "Initial State is  [2, 15, 5]\n",
      "episode 3057, reward 1470.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 126\n",
      "Initial State is  [4, 3, 3]\n",
      "episode 3058, reward 1023.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 114\n",
      "Initial State is  [4, 4, 6]\n",
      "episode 3059, reward 1278.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 122\n",
      "Initial State is  [4, 15, 6]\n",
      "episode 3060, reward 783.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 121\n",
      "Initial State is  [0, 15, 1]\n",
      "episode 3061, reward 1155.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 111\n",
      "Initial State is  [4, 2, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3062, reward 1183.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 117\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 3063, reward 1331.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 128\n",
      "Initial State is  [1, 6, 3]\n",
      "episode 3064, reward 1270.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 120\n",
      "Initial State is  [2, 10, 4]\n",
      "episode 3065, reward 1027.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 126\n",
      "Initial State is  [2, 10, 4]\n",
      "episode 3066, reward 1207.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 120\n",
      "Initial State is  [4, 10, 3]\n",
      "episode 3067, reward 1408.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 112\n",
      "Initial State is  [3, 12, 0]\n",
      "episode 3068, reward 1242.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 127\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 3069, reward 1389.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 116\n",
      "Initial State is  [1, 3, 1]\n",
      "episode 3070, reward 1325.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 124\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 3071, reward 1135.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 126\n",
      "Initial State is  [0, 20, 2]\n",
      "episode 3072, reward 1315.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 113\n",
      "Initial State is  [4, 13, 3]\n",
      "episode 3073, reward 1116.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 121\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 3074, reward 1015.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 109\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 3075, reward 1001.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 123\n",
      "Initial State is  [1, 18, 4]\n",
      "episode 3076, reward 1146.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 122\n",
      "Initial State is  [3, 10, 5]\n",
      "episode 3077, reward 1286.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 119\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 3078, reward 930.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 115\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 3079, reward 1398.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 126\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 3080, reward 1117.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 114\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 3081, reward 1208.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 129\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 3082, reward 1020.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 118\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 3083, reward 1330.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 115\n",
      "Initial State is  [3, 22, 0]\n",
      "episode 3084, reward 1464.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 121\n",
      "Initial State is  [0, 13, 3]\n",
      "episode 3085, reward 1151.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 124\n",
      "Initial State is  [4, 2, 4]\n",
      "episode 3086, reward 1199.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 135\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 3087, reward 1079.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 116\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 3088, reward 966.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 115\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 3089, reward 1244.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 130\n",
      "Initial State is  [0, 21, 1]\n",
      "episode 3090, reward 1252.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 124\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 3091, reward 1101.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 125\n",
      "Initial State is  [3, 1, 0]\n",
      "episode 3092, reward 976.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 125\n",
      "Initial State is  [3, 16, 2]\n",
      "episode 3093, reward 1144.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 124\n",
      "Initial State is  [4, 15, 2]\n",
      "episode 3094, reward 1147.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 113\n",
      "Initial State is  [0, 21, 6]\n",
      "episode 3095, reward 1137.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 118\n",
      "Initial State is  [0, 13, 3]\n",
      "episode 3096, reward 1310.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 122\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 3097, reward 1033.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 122\n",
      "Initial State is  [4, 17, 4]\n",
      "episode 3098, reward 1116.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 121\n",
      "Initial State is  [1, 19, 3]\n",
      "episode 3099, reward 1104.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 130\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 3100, reward 1295.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 123\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 3101, reward 1197.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 118\n",
      "Initial State is  [4, 19, 1]\n",
      "episode 3102, reward 1255.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 117\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 3103, reward 757.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 120\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 3104, reward 1006.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 120\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 3105, reward 1444.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 123\n",
      "Initial State is  [3, 13, 3]\n",
      "episode 3106, reward 1335.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 124\n",
      "Initial State is  [4, 11, 2]\n",
      "episode 3107, reward 1002.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 121\n",
      "Initial State is  [3, 22, 1]\n",
      "episode 3108, reward 1224.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 116\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 3109, reward 1049.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 115\n",
      "Initial State is  [3, 8, 2]\n",
      "episode 3110, reward 1188.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 126\n",
      "Initial State is  [0, 14, 0]\n",
      "episode 3111, reward 953.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 109\n",
      "Initial State is  [3, 5, 0]\n",
      "episode 3112, reward 1172.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 119\n",
      "Initial State is  [4, 4, 6]\n",
      "episode 3113, reward 848.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 118\n",
      "Initial State is  [3, 11, 1]\n",
      "episode 3114, reward 1061.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 121\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 3115, reward 951.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 127\n",
      "Initial State is  [3, 22, 0]\n",
      "episode 3116, reward 1126.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 120\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 3117, reward 1048.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 112\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 3118, reward 963.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 122\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 3119, reward 1089.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 132\n",
      "Initial State is  [1, 11, 4]\n",
      "episode 3120, reward 1010.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 124\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 3121, reward 1147.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 119\n",
      "Initial State is  [1, 22, 2]\n",
      "episode 3122, reward 1432.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 127\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 3123, reward 1315.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 125\n",
      "Initial State is  [4, 4, 5]\n",
      "episode 3124, reward 674.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 112\n",
      "Initial State is  [3, 18, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3125, reward 1007.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 113\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 3126, reward 1198.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 117\n",
      "Initial State is  [0, 0, 6]\n",
      "episode 3127, reward 1268.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 124\n",
      "Initial State is  [1, 8, 1]\n",
      "episode 3128, reward 1277.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 113\n",
      "Initial State is  [1, 20, 4]\n",
      "episode 3129, reward 961.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 106\n",
      "Initial State is  [3, 22, 0]\n",
      "episode 3130, reward 1030.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 116\n",
      "Initial State is  [0, 13, 4]\n",
      "episode 3131, reward 1083.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 121\n",
      "Initial State is  [3, 2, 2]\n",
      "episode 3132, reward 1271.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 123\n",
      "Initial State is  [0, 11, 6]\n",
      "episode 3133, reward 1237.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 123\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 3134, reward 1035.0, memory_length 2000, epsilon 0.0009954703940636294, time 747.0, rides 119\n",
      "Initial State is  [2, 17, 4]\n",
      "episode 3135, reward 996.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 112\n",
      "Initial State is  [2, 9, 0]\n",
      "episode 3136, reward 1306.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 124\n",
      "Initial State is  [4, 15, 3]\n",
      "episode 3137, reward 1338.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 116\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 3138, reward 941.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 113\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 3139, reward 1390.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 123\n",
      "Initial State is  [3, 3, 6]\n",
      "episode 3140, reward 1153.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 118\n",
      "Initial State is  [0, 9, 4]\n",
      "episode 3141, reward 1346.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 128\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 3142, reward 858.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 122\n",
      "Initial State is  [3, 17, 5]\n",
      "episode 3143, reward 1269.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 130\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 3144, reward 920.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 113\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 3145, reward 1062.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 124\n",
      "Initial State is  [3, 14, 2]\n",
      "episode 3146, reward 1216.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 117\n",
      "Initial State is  [3, 18, 6]\n",
      "episode 3147, reward 1078.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 111\n",
      "Initial State is  [2, 11, 4]\n",
      "episode 3148, reward 1321.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 126\n",
      "Initial State is  [4, 16, 4]\n",
      "episode 3149, reward 1128.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 119\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 3150, reward 860.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 121\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 3151, reward 1084.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 114\n",
      "Initial State is  [3, 23, 3]\n",
      "episode 3152, reward 1174.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 122\n",
      "Initial State is  [2, 6, 0]\n",
      "episode 3153, reward 1389.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 119\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 3154, reward 1258.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 118\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 3155, reward 1279.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 116\n",
      "Initial State is  [0, 6, 5]\n",
      "episode 3156, reward 1257.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 135\n",
      "Initial State is  [2, 16, 6]\n",
      "episode 3157, reward 1157.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 125\n",
      "Initial State is  [3, 17, 2]\n",
      "episode 3158, reward 1129.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 124\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 3159, reward 988.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 114\n",
      "Initial State is  [4, 3, 5]\n",
      "episode 3160, reward 1176.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 132\n",
      "Initial State is  [4, 19, 3]\n",
      "episode 3161, reward 1282.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 128\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 3162, reward 1314.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 114\n",
      "Initial State is  [2, 20, 4]\n",
      "episode 3163, reward 1410.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 133\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 3164, reward 1343.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 120\n",
      "Initial State is  [0, 7, 2]\n",
      "episode 3165, reward 1510.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 123\n",
      "Initial State is  [1, 20, 4]\n",
      "episode 3166, reward 1628.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 126\n",
      "Initial State is  [0, 5, 3]\n",
      "episode 3167, reward 918.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 113\n",
      "Initial State is  [1, 13, 3]\n",
      "episode 3168, reward 1428.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 126\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 3169, reward 1021.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 119\n",
      "Initial State is  [2, 0, 3]\n",
      "episode 3170, reward 974.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 117\n",
      "Initial State is  [1, 20, 4]\n",
      "episode 3171, reward 1142.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 111\n",
      "Initial State is  [4, 19, 3]\n",
      "episode 3172, reward 987.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 119\n",
      "Initial State is  [1, 0, 0]\n",
      "episode 3173, reward 1318.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 130\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 3174, reward 823.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 118\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 3175, reward 1404.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 123\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 3176, reward 1220.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 119\n",
      "Initial State is  [4, 19, 0]\n",
      "episode 3177, reward 1254.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 118\n",
      "Initial State is  [1, 4, 1]\n",
      "episode 3178, reward 1074.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 128\n",
      "Initial State is  [3, 20, 0]\n",
      "episode 3179, reward 1178.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 117\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 3180, reward 1162.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 125\n",
      "Initial State is  [3, 11, 2]\n",
      "episode 3181, reward 1410.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 128\n",
      "Initial State is  [3, 18, 6]\n",
      "episode 3182, reward 1414.0, memory_length 2000, epsilon 0.0009954703940636294, time 740.0, rides 125\n",
      "Initial State is  [0, 14, 1]\n",
      "episode 3183, reward 1181.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 119\n",
      "Initial State is  [2, 20, 1]\n",
      "episode 3184, reward 1089.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 108\n",
      "Initial State is  [2, 14, 2]\n",
      "episode 3185, reward 799.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 128\n",
      "Initial State is  [4, 3, 0]\n",
      "episode 3186, reward 1109.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 120\n",
      "Initial State is  [3, 9, 0]\n",
      "episode 3187, reward 1093.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 114\n",
      "Initial State is  [1, 11, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3188, reward 787.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 113\n",
      "Initial State is  [3, 4, 4]\n",
      "episode 3189, reward 849.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 108\n",
      "Initial State is  [4, 16, 0]\n",
      "episode 3190, reward 1373.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 135\n",
      "Initial State is  [0, 21, 1]\n",
      "episode 3191, reward 1127.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 117\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 3192, reward 863.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 122\n",
      "Initial State is  [1, 19, 3]\n",
      "episode 3193, reward 1084.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 118\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 3194, reward 1015.0, memory_length 2000, epsilon 0.0009954703940636294, time 741.0, rides 120\n",
      "Initial State is  [3, 5, 6]\n",
      "episode 3195, reward 1037.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 116\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 3196, reward 1339.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 136\n",
      "Initial State is  [4, 3, 5]\n",
      "episode 3197, reward 1197.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 124\n",
      "Initial State is  [0, 13, 1]\n",
      "episode 3198, reward 952.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 128\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 3199, reward 994.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 115\n",
      "Initial State is  [4, 20, 5]\n",
      "episode 3200, reward 1273.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 119\n",
      "Initial State is  [1, 16, 5]\n",
      "episode 3201, reward 1298.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 118\n",
      "Initial State is  [4, 5, 6]\n",
      "episode 3202, reward 1200.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 124\n",
      "Initial State is  [1, 13, 3]\n",
      "episode 3203, reward 1144.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 114\n",
      "Initial State is  [3, 9, 1]\n",
      "episode 3204, reward 1270.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 122\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 3205, reward 1258.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 138\n",
      "Initial State is  [0, 0, 1]\n",
      "episode 3206, reward 1148.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 124\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 3207, reward 1280.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 120\n",
      "Initial State is  [1, 0, 6]\n",
      "episode 3208, reward 1173.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 122\n",
      "Initial State is  [4, 17, 6]\n",
      "episode 3209, reward 1245.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 122\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 3210, reward 1379.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 122\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 3211, reward 1310.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 119\n",
      "Initial State is  [2, 14, 1]\n",
      "episode 3212, reward 1324.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 115\n",
      "Initial State is  [2, 1, 1]\n",
      "episode 3213, reward 1061.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 122\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 3214, reward 1065.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 122\n",
      "Initial State is  [2, 8, 6]\n",
      "episode 3215, reward 1020.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 122\n",
      "Initial State is  [0, 17, 1]\n",
      "episode 3216, reward 1100.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 121\n",
      "Initial State is  [1, 8, 0]\n",
      "episode 3217, reward 1311.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 109\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 3218, reward 1310.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 124\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 3219, reward 952.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 108\n",
      "Initial State is  [0, 23, 0]\n",
      "episode 3220, reward 1339.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 122\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 3221, reward 1144.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 120\n",
      "Initial State is  [2, 11, 1]\n",
      "episode 3222, reward 1081.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 116\n",
      "Initial State is  [4, 5, 6]\n",
      "episode 3223, reward 834.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 105\n",
      "Initial State is  [4, 14, 6]\n",
      "episode 3224, reward 1266.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 130\n",
      "Initial State is  [4, 5, 1]\n",
      "episode 3225, reward 1216.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 125\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 3226, reward 986.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 113\n",
      "Initial State is  [0, 22, 6]\n",
      "episode 3227, reward 1342.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 115\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 3228, reward 1045.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 123\n",
      "Initial State is  [4, 4, 2]\n",
      "episode 3229, reward 966.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 118\n",
      "Initial State is  [0, 14, 0]\n",
      "episode 3230, reward 1303.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 129\n",
      "Initial State is  [4, 9, 5]\n",
      "episode 3231, reward 1437.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 127\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 3232, reward 1029.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 119\n",
      "Initial State is  [1, 6, 4]\n",
      "episode 3233, reward 1103.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 109\n",
      "Initial State is  [1, 12, 2]\n",
      "episode 3234, reward 1014.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 113\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 3235, reward 1319.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 121\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 3236, reward 1053.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 130\n",
      "Initial State is  [2, 8, 2]\n",
      "episode 3237, reward 1358.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 118\n",
      "Initial State is  [4, 3, 6]\n",
      "episode 3238, reward 1070.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 123\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 3239, reward 1092.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 112\n",
      "Initial State is  [1, 5, 4]\n",
      "episode 3240, reward 1314.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 114\n",
      "Initial State is  [4, 15, 4]\n",
      "episode 3241, reward 1142.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 126\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 3242, reward 1229.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 113\n",
      "Initial State is  [0, 22, 3]\n",
      "episode 3243, reward 931.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 115\n",
      "Initial State is  [2, 11, 5]\n",
      "episode 3244, reward 1041.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 125\n",
      "Initial State is  [2, 18, 3]\n",
      "episode 3245, reward 1392.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 107\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 3246, reward 1174.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 110\n",
      "Initial State is  [2, 22, 6]\n",
      "episode 3247, reward 1102.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 114\n",
      "Initial State is  [2, 17, 3]\n",
      "episode 3248, reward 959.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 119\n",
      "Initial State is  [0, 13, 3]\n",
      "episode 3249, reward 1103.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 124\n",
      "Initial State is  [3, 16, 0]\n",
      "episode 3250, reward 1141.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 128\n",
      "Initial State is  [0, 2, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3251, reward 1031.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 114\n",
      "Initial State is  [4, 6, 1]\n",
      "episode 3252, reward 1422.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 133\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 3253, reward 1426.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 122\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 3254, reward 1254.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 114\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 3255, reward 1122.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 121\n",
      "Initial State is  [0, 8, 6]\n",
      "episode 3256, reward 1432.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 118\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 3257, reward 1215.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 117\n",
      "Initial State is  [0, 20, 0]\n",
      "episode 3258, reward 1286.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 120\n",
      "Initial State is  [1, 10, 3]\n",
      "episode 3259, reward 1380.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 120\n",
      "Initial State is  [3, 11, 4]\n",
      "episode 3260, reward 1233.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 121\n",
      "Initial State is  [2, 8, 2]\n",
      "episode 3261, reward 1176.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 126\n",
      "Initial State is  [3, 15, 0]\n",
      "episode 3262, reward 1272.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 125\n",
      "Initial State is  [2, 16, 4]\n",
      "episode 3263, reward 1023.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 122\n",
      "Initial State is  [3, 2, 3]\n",
      "episode 3264, reward 749.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 112\n",
      "Initial State is  [3, 17, 3]\n",
      "episode 3265, reward 958.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 117\n",
      "Initial State is  [1, 5, 2]\n",
      "episode 3266, reward 909.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 120\n",
      "Initial State is  [1, 10, 0]\n",
      "episode 3267, reward 1153.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 119\n",
      "Initial State is  [4, 16, 1]\n",
      "episode 3268, reward 1489.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 123\n",
      "Initial State is  [4, 12, 1]\n",
      "episode 3269, reward 1274.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 120\n",
      "Initial State is  [1, 7, 6]\n",
      "episode 3270, reward 1139.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 122\n",
      "Initial State is  [3, 10, 6]\n",
      "episode 3271, reward 1242.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 126\n",
      "Initial State is  [4, 4, 6]\n",
      "episode 3272, reward 1119.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 118\n",
      "Initial State is  [3, 15, 5]\n",
      "episode 3273, reward 1105.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 112\n",
      "Initial State is  [0, 0, 1]\n",
      "episode 3274, reward 1205.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 119\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 3275, reward 940.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 115\n",
      "Initial State is  [0, 3, 2]\n",
      "episode 3276, reward 1261.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 121\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 3277, reward 1088.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 107\n",
      "Initial State is  [4, 3, 5]\n",
      "episode 3278, reward 1176.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 123\n",
      "Initial State is  [0, 5, 0]\n",
      "episode 3279, reward 1198.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 124\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 3280, reward 1062.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 135\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 3281, reward 1183.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 115\n",
      "Initial State is  [3, 9, 2]\n",
      "episode 3282, reward 1072.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 126\n",
      "Initial State is  [2, 23, 1]\n",
      "episode 3283, reward 1062.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 115\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 3284, reward 877.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 120\n",
      "Initial State is  [1, 17, 2]\n",
      "episode 3285, reward 1307.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 112\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 3286, reward 1259.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 121\n",
      "Initial State is  [2, 0, 5]\n",
      "episode 3287, reward 998.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 119\n",
      "Initial State is  [1, 5, 2]\n",
      "episode 3288, reward 1344.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 117\n",
      "Initial State is  [4, 3, 3]\n",
      "episode 3289, reward 1372.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 109\n",
      "Initial State is  [3, 6, 4]\n",
      "episode 3290, reward 1125.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 111\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 3291, reward 1238.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 124\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 3292, reward 1179.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 121\n",
      "Initial State is  [1, 1, 3]\n",
      "episode 3293, reward 1106.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 122\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 3294, reward 938.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 110\n",
      "Initial State is  [4, 23, 1]\n",
      "episode 3295, reward 807.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 111\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 3296, reward 1073.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 108\n",
      "Initial State is  [1, 14, 2]\n",
      "episode 3297, reward 1207.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 115\n",
      "Initial State is  [2, 23, 6]\n",
      "episode 3298, reward 1468.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 130\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 3299, reward 1388.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 125\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 3300, reward 975.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 118\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 3301, reward 1467.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 126\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 3302, reward 965.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 128\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 3303, reward 1145.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 112\n",
      "Initial State is  [2, 17, 4]\n",
      "episode 3304, reward 1122.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 117\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 3305, reward 1215.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 123\n",
      "Initial State is  [1, 3, 6]\n",
      "episode 3306, reward 1269.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 116\n",
      "Initial State is  [2, 16, 2]\n",
      "episode 3307, reward 1249.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 120\n",
      "Initial State is  [4, 10, 4]\n",
      "episode 3308, reward 1713.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 122\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 3309, reward 1343.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 118\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 3310, reward 988.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 121\n",
      "Initial State is  [1, 1, 0]\n",
      "episode 3311, reward 936.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 111\n",
      "Initial State is  [4, 22, 4]\n",
      "episode 3312, reward 1192.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 126\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 3313, reward 1055.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 117\n",
      "Initial State is  [3, 1, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3314, reward 835.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 114\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 3315, reward 975.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 109\n",
      "Initial State is  [1, 1, 0]\n",
      "episode 3316, reward 1307.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 117\n",
      "Initial State is  [3, 18, 1]\n",
      "episode 3317, reward 1385.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 115\n",
      "Initial State is  [3, 5, 1]\n",
      "episode 3318, reward 1078.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 114\n",
      "Initial State is  [4, 22, 4]\n",
      "episode 3319, reward 1140.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 114\n",
      "Initial State is  [0, 18, 3]\n",
      "episode 3320, reward 1301.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 115\n",
      "Initial State is  [3, 10, 3]\n",
      "episode 3321, reward 987.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 110\n",
      "Initial State is  [0, 2, 1]\n",
      "episode 3322, reward 1180.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 115\n",
      "Initial State is  [3, 16, 3]\n",
      "episode 3323, reward 1186.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 118\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 3324, reward 910.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 109\n",
      "Initial State is  [1, 0, 3]\n",
      "episode 3325, reward 946.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 108\n",
      "Initial State is  [2, 1, 4]\n",
      "episode 3326, reward 1307.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 125\n",
      "Initial State is  [2, 14, 1]\n",
      "episode 3327, reward 1121.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 110\n",
      "Initial State is  [1, 20, 3]\n",
      "episode 3328, reward 1346.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 121\n",
      "Initial State is  [1, 22, 4]\n",
      "episode 3329, reward 931.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 112\n",
      "Initial State is  [3, 3, 2]\n",
      "episode 3330, reward 1034.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 112\n",
      "Initial State is  [2, 19, 3]\n",
      "episode 3331, reward 935.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 113\n",
      "Initial State is  [0, 0, 0]\n",
      "episode 3332, reward 1388.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 136\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 3333, reward 910.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 108\n",
      "Initial State is  [0, 23, 0]\n",
      "episode 3334, reward 1070.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 118\n",
      "Initial State is  [2, 13, 1]\n",
      "episode 3335, reward 1086.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 115\n",
      "Initial State is  [1, 23, 5]\n",
      "episode 3336, reward 1144.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 124\n",
      "Initial State is  [4, 20, 4]\n",
      "episode 3337, reward 1096.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 114\n",
      "Initial State is  [4, 7, 4]\n",
      "episode 3338, reward 1023.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 118\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 3339, reward 1019.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 108\n",
      "Initial State is  [3, 23, 0]\n",
      "episode 3340, reward 1126.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 127\n",
      "Initial State is  [4, 4, 4]\n",
      "episode 3341, reward 1116.0, memory_length 2000, epsilon 0.0009954703940636294, time 742.0, rides 123\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 3342, reward 1129.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 133\n",
      "Initial State is  [2, 19, 3]\n",
      "episode 3343, reward 1624.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 131\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 3344, reward 1543.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 123\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 3345, reward 1372.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 128\n",
      "Initial State is  [2, 17, 0]\n",
      "episode 3346, reward 1182.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 118\n",
      "Initial State is  [0, 4, 1]\n",
      "episode 3347, reward 1079.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 107\n",
      "Initial State is  [2, 21, 0]\n",
      "episode 3348, reward 1461.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 123\n",
      "Initial State is  [2, 15, 0]\n",
      "episode 3349, reward 1276.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 131\n",
      "Initial State is  [1, 10, 1]\n",
      "episode 3350, reward 1572.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 117\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 3351, reward 1039.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 118\n",
      "Initial State is  [0, 16, 5]\n",
      "episode 3352, reward 1379.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 131\n",
      "Initial State is  [0, 4, 5]\n",
      "episode 3353, reward 1461.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 130\n",
      "Initial State is  [0, 6, 3]\n",
      "episode 3354, reward 1120.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 125\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 3355, reward 1318.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 124\n",
      "Initial State is  [4, 10, 0]\n",
      "episode 3356, reward 1235.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 117\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 3357, reward 1216.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 116\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 3358, reward 1095.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 125\n",
      "Initial State is  [3, 9, 6]\n",
      "episode 3359, reward 1187.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 128\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 3360, reward 1144.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 131\n",
      "Initial State is  [2, 19, 1]\n",
      "episode 3361, reward 1337.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 136\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 3362, reward 1162.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 129\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 3363, reward 1392.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 114\n",
      "Initial State is  [2, 12, 5]\n",
      "episode 3364, reward 1286.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 131\n",
      "Initial State is  [0, 3, 1]\n",
      "episode 3365, reward 1250.0, memory_length 2000, epsilon 0.0009954703940636294, time 743.0, rides 122\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 3366, reward 1403.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 132\n",
      "Initial State is  [3, 16, 2]\n",
      "episode 3367, reward 1390.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 130\n",
      "Initial State is  [1, 10, 2]\n",
      "episode 3368, reward 1010.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 128\n",
      "Initial State is  [3, 2, 3]\n",
      "episode 3369, reward 843.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 123\n",
      "Initial State is  [3, 15, 6]\n",
      "episode 3370, reward 1167.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 118\n",
      "Initial State is  [4, 14, 2]\n",
      "episode 3371, reward 1043.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 116\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 3372, reward 882.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 118\n",
      "Initial State is  [1, 21, 5]\n",
      "episode 3373, reward 1315.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 130\n",
      "Initial State is  [2, 0, 0]\n",
      "episode 3374, reward 1326.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 120\n",
      "Initial State is  [3, 17, 1]\n",
      "episode 3375, reward 1071.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 110\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 3376, reward 1205.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 127\n",
      "Initial State is  [2, 15, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3377, reward 1204.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 123\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 3378, reward 1312.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 127\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 3379, reward 1250.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 141\n",
      "Initial State is  [1, 11, 0]\n",
      "episode 3380, reward 1159.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 126\n",
      "Initial State is  [0, 5, 0]\n",
      "episode 3381, reward 1193.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 134\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 3382, reward 1587.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 133\n",
      "Initial State is  [1, 6, 4]\n",
      "episode 3383, reward 1143.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 132\n",
      "Initial State is  [3, 18, 1]\n",
      "episode 3384, reward 1145.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 112\n",
      "Initial State is  [1, 3, 2]\n",
      "episode 3385, reward 1088.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 134\n",
      "Initial State is  [3, 9, 6]\n",
      "episode 3386, reward 1274.0, memory_length 2000, epsilon 0.0009954703940636294, time 740.0, rides 116\n",
      "Initial State is  [2, 23, 6]\n",
      "episode 3387, reward 1224.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 119\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 3388, reward 1518.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 126\n",
      "Initial State is  [4, 6, 5]\n",
      "episode 3389, reward 1170.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 124\n",
      "Initial State is  [2, 19, 6]\n",
      "episode 3390, reward 1167.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 121\n",
      "Initial State is  [4, 22, 6]\n",
      "episode 3391, reward 1352.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 135\n",
      "Initial State is  [4, 13, 4]\n",
      "episode 3392, reward 1392.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 132\n",
      "Initial State is  [4, 12, 4]\n",
      "episode 3393, reward 919.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 130\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 3394, reward 1251.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 121\n",
      "Initial State is  [2, 21, 0]\n",
      "episode 3395, reward 1343.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 134\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 3396, reward 1264.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 127\n",
      "Initial State is  [2, 13, 4]\n",
      "episode 3397, reward 1401.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 126\n",
      "Initial State is  [1, 8, 6]\n",
      "episode 3398, reward 1218.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 120\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 3399, reward 1120.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 119\n",
      "Initial State is  [2, 9, 1]\n",
      "episode 3400, reward 1388.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 127\n",
      "Initial State is  [1, 10, 1]\n",
      "episode 3401, reward 1286.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 122\n",
      "Initial State is  [4, 20, 2]\n",
      "episode 3402, reward 1478.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 137\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 3403, reward 1338.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 120\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 3404, reward 1296.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 141\n",
      "Initial State is  [1, 5, 2]\n",
      "episode 3405, reward 1238.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 117\n",
      "Initial State is  [0, 19, 4]\n",
      "episode 3406, reward 1241.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 125\n",
      "Initial State is  [4, 21, 5]\n",
      "episode 3407, reward 1287.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 126\n",
      "Initial State is  [0, 12, 6]\n",
      "episode 3408, reward 1337.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 126\n",
      "Initial State is  [1, 19, 0]\n",
      "episode 3409, reward 1521.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 128\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 3410, reward 1621.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 129\n",
      "Initial State is  [4, 3, 3]\n",
      "episode 3411, reward 953.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 113\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 3412, reward 1007.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 123\n",
      "Initial State is  [0, 4, 3]\n",
      "episode 3413, reward 1496.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 132\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 3414, reward 1253.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 125\n",
      "Initial State is  [1, 16, 4]\n",
      "episode 3415, reward 1294.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 118\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 3416, reward 1276.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 120\n",
      "Initial State is  [2, 19, 4]\n",
      "episode 3417, reward 1065.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 125\n",
      "Initial State is  [2, 11, 5]\n",
      "episode 3418, reward 1142.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 123\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 3419, reward 1195.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 122\n",
      "Initial State is  [3, 14, 1]\n",
      "episode 3420, reward 1279.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 124\n",
      "Initial State is  [4, 10, 5]\n",
      "episode 3421, reward 1157.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 136\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 3422, reward 1353.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 122\n",
      "Initial State is  [3, 10, 4]\n",
      "episode 3423, reward 1391.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 123\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 3424, reward 1135.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 126\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 3425, reward 1231.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 121\n",
      "Initial State is  [2, 8, 6]\n",
      "episode 3426, reward 1069.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 121\n",
      "Initial State is  [4, 5, 1]\n",
      "episode 3427, reward 1204.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 121\n",
      "Initial State is  [2, 23, 3]\n",
      "episode 3428, reward 1171.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 114\n",
      "Initial State is  [0, 10, 2]\n",
      "episode 3429, reward 1289.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 124\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 3430, reward 1269.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 132\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 3431, reward 943.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 128\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 3432, reward 1441.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 128\n",
      "Initial State is  [1, 3, 6]\n",
      "episode 3433, reward 1255.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 133\n",
      "Initial State is  [2, 22, 6]\n",
      "episode 3434, reward 1284.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 122\n",
      "Initial State is  [2, 23, 6]\n",
      "episode 3435, reward 1105.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 142\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 3436, reward 813.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 130\n",
      "Initial State is  [0, 4, 3]\n",
      "episode 3437, reward 1151.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 131\n",
      "Initial State is  [0, 4, 1]\n",
      "episode 3438, reward 1115.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 122\n",
      "Initial State is  [3, 10, 4]\n",
      "episode 3439, reward 1223.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 130\n",
      "Initial State is  [2, 17, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3440, reward 1373.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 122\n",
      "Initial State is  [3, 3, 1]\n",
      "episode 3441, reward 731.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 116\n",
      "Initial State is  [1, 21, 0]\n",
      "episode 3442, reward 1334.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 124\n",
      "Initial State is  [0, 0, 0]\n",
      "episode 3443, reward 1473.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 121\n",
      "Initial State is  [0, 2, 4]\n",
      "episode 3444, reward 1204.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 139\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 3445, reward 1248.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 136\n",
      "Initial State is  [0, 23, 0]\n",
      "episode 3446, reward 1325.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 119\n",
      "Initial State is  [0, 4, 5]\n",
      "episode 3447, reward 1287.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 133\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 3448, reward 1367.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 120\n",
      "Initial State is  [1, 10, 1]\n",
      "episode 3449, reward 1355.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 117\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 3450, reward 882.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 117\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 3451, reward 1192.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 120\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 3452, reward 1075.0, memory_length 2000, epsilon 0.0009954703940636294, time 739.0, rides 128\n",
      "Initial State is  [4, 21, 1]\n",
      "episode 3453, reward 1376.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 123\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 3454, reward 1213.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 117\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 3455, reward 1291.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 121\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 3456, reward 1242.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 142\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 3457, reward 1560.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 129\n",
      "Initial State is  [3, 3, 3]\n",
      "episode 3458, reward 1432.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 124\n",
      "Initial State is  [4, 7, 0]\n",
      "episode 3459, reward 1014.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 111\n",
      "Initial State is  [0, 20, 5]\n",
      "episode 3460, reward 1497.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 129\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 3461, reward 1229.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 122\n",
      "Initial State is  [4, 17, 4]\n",
      "episode 3462, reward 1151.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 124\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 3463, reward 1374.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 135\n",
      "Initial State is  [4, 3, 0]\n",
      "episode 3464, reward 1306.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 126\n",
      "Initial State is  [2, 11, 2]\n",
      "episode 3465, reward 1424.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 121\n",
      "Initial State is  [4, 4, 6]\n",
      "episode 3466, reward 1519.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 116\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 3467, reward 1262.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 122\n",
      "Initial State is  [2, 19, 1]\n",
      "episode 3468, reward 1401.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 116\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 3469, reward 1042.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 116\n",
      "Initial State is  [2, 3, 1]\n",
      "episode 3470, reward 1256.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 128\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 3471, reward 1345.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 138\n",
      "Initial State is  [1, 1, 5]\n",
      "episode 3472, reward 1746.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 131\n",
      "Initial State is  [0, 13, 3]\n",
      "episode 3473, reward 1054.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 124\n",
      "Initial State is  [1, 8, 6]\n",
      "episode 3474, reward 1113.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 124\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 3475, reward 1232.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 131\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 3476, reward 996.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 125\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 3477, reward 1204.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 120\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 3478, reward 1393.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 119\n",
      "Initial State is  [2, 11, 4]\n",
      "episode 3479, reward 1163.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 120\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 3480, reward 1130.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 124\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 3481, reward 1069.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 113\n",
      "Initial State is  [2, 0, 5]\n",
      "episode 3482, reward 1067.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 119\n",
      "Initial State is  [0, 12, 0]\n",
      "episode 3483, reward 1047.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 110\n",
      "Initial State is  [4, 17, 5]\n",
      "episode 3484, reward 1109.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 113\n",
      "Initial State is  [4, 21, 3]\n",
      "episode 3485, reward 1154.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 125\n",
      "Initial State is  [1, 1, 5]\n",
      "episode 3486, reward 1154.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 131\n",
      "Initial State is  [4, 21, 1]\n",
      "episode 3487, reward 1268.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 126\n",
      "Initial State is  [1, 15, 0]\n",
      "episode 3488, reward 1166.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 120\n",
      "Initial State is  [1, 11, 4]\n",
      "episode 3489, reward 1185.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 130\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 3490, reward 1386.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 121\n",
      "Initial State is  [4, 14, 3]\n",
      "episode 3491, reward 1505.0, memory_length 2000, epsilon 0.0009954703940636294, time 741.0, rides 137\n",
      "Initial State is  [0, 22, 3]\n",
      "episode 3492, reward 1620.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 120\n",
      "Initial State is  [3, 5, 3]\n",
      "episode 3493, reward 1263.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 117\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 3494, reward 1496.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 125\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 3495, reward 1309.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 135\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 3496, reward 1205.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 127\n",
      "Initial State is  [3, 9, 1]\n",
      "episode 3497, reward 1400.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 140\n",
      "Initial State is  [4, 10, 0]\n",
      "episode 3498, reward 1258.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 122\n",
      "Initial State is  [2, 19, 6]\n",
      "episode 3499, reward 1335.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 131\n",
      "Initial State is  [3, 20, 2]\n",
      "episode 3500, reward 1427.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 127\n",
      "Initial State is  [1, 23, 5]\n",
      "episode 3501, reward 1349.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 121\n",
      "Initial State is  [2, 12, 6]\n",
      "episode 3502, reward 1399.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 125\n",
      "Initial State is  [2, 18, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3503, reward 1267.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 123\n",
      "Initial State is  [2, 13, 5]\n",
      "episode 3504, reward 1155.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 131\n",
      "Initial State is  [3, 10, 6]\n",
      "episode 3505, reward 1138.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 118\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 3506, reward 1079.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 124\n",
      "Initial State is  [0, 1, 5]\n",
      "episode 3507, reward 1533.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 125\n",
      "Initial State is  [4, 4, 3]\n",
      "episode 3508, reward 1193.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 122\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 3509, reward 1016.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 119\n",
      "Initial State is  [2, 5, 6]\n",
      "episode 3510, reward 1096.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 113\n",
      "Initial State is  [2, 14, 2]\n",
      "episode 3511, reward 961.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 120\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 3512, reward 1107.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 122\n",
      "Initial State is  [1, 1, 5]\n",
      "episode 3513, reward 1018.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 123\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 3514, reward 1044.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 135\n",
      "Initial State is  [0, 18, 0]\n",
      "episode 3515, reward 1333.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 122\n",
      "Initial State is  [3, 8, 0]\n",
      "episode 3516, reward 1319.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 129\n",
      "Initial State is  [4, 3, 6]\n",
      "episode 3517, reward 1135.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 123\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 3518, reward 1170.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 115\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 3519, reward 1271.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 133\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 3520, reward 1113.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 129\n",
      "Initial State is  [2, 9, 0]\n",
      "episode 3521, reward 1382.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 128\n",
      "Initial State is  [0, 3, 3]\n",
      "episode 3522, reward 1143.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 120\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 3523, reward 1366.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 121\n",
      "Initial State is  [3, 18, 1]\n",
      "episode 3524, reward 1256.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 120\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 3525, reward 1340.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 125\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 3526, reward 1189.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 113\n",
      "Initial State is  [0, 8, 6]\n",
      "episode 3527, reward 1271.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 130\n",
      "Initial State is  [3, 20, 6]\n",
      "episode 3528, reward 1295.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 122\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 3529, reward 1122.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 130\n",
      "Initial State is  [4, 18, 2]\n",
      "episode 3530, reward 1297.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 126\n",
      "Initial State is  [3, 11, 3]\n",
      "episode 3531, reward 1270.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 113\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 3532, reward 1223.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 131\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 3533, reward 938.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 126\n",
      "Initial State is  [1, 8, 6]\n",
      "episode 3534, reward 1153.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 134\n",
      "Initial State is  [4, 8, 2]\n",
      "episode 3535, reward 1373.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 128\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 3536, reward 1421.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 125\n",
      "Initial State is  [3, 10, 2]\n",
      "episode 3537, reward 1273.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 121\n",
      "Initial State is  [3, 21, 2]\n",
      "episode 3538, reward 1162.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 131\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 3539, reward 1430.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 122\n",
      "Initial State is  [4, 18, 6]\n",
      "episode 3540, reward 1243.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 109\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 3541, reward 1412.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 128\n",
      "Initial State is  [4, 14, 1]\n",
      "episode 3542, reward 1457.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 123\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 3543, reward 1328.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 142\n",
      "Initial State is  [4, 21, 0]\n",
      "episode 3544, reward 1235.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 126\n",
      "Initial State is  [4, 16, 4]\n",
      "episode 3545, reward 889.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 130\n",
      "Initial State is  [3, 7, 2]\n",
      "episode 3546, reward 1179.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 124\n",
      "Initial State is  [0, 22, 6]\n",
      "episode 3547, reward 1548.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 138\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 3548, reward 1123.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 124\n",
      "Initial State is  [2, 22, 5]\n",
      "episode 3549, reward 1113.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 120\n",
      "Initial State is  [3, 4, 2]\n",
      "episode 3550, reward 933.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 124\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 3551, reward 1058.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 121\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 3552, reward 1513.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 129\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 3553, reward 1188.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 117\n",
      "Initial State is  [3, 22, 6]\n",
      "episode 3554, reward 1113.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 117\n",
      "Initial State is  [2, 16, 2]\n",
      "episode 3555, reward 1168.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 122\n",
      "Initial State is  [0, 9, 3]\n",
      "episode 3556, reward 1563.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 135\n",
      "Initial State is  [1, 13, 3]\n",
      "episode 3557, reward 1313.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 133\n",
      "Initial State is  [4, 11, 2]\n",
      "episode 3558, reward 1307.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 127\n",
      "Initial State is  [0, 6, 3]\n",
      "episode 3559, reward 1397.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 126\n",
      "Initial State is  [3, 14, 2]\n",
      "episode 3560, reward 1251.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 119\n",
      "Initial State is  [2, 15, 0]\n",
      "episode 3561, reward 1299.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 132\n",
      "Initial State is  [4, 21, 1]\n",
      "episode 3562, reward 1214.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 126\n",
      "Initial State is  [4, 15, 3]\n",
      "episode 3563, reward 1314.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 129\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 3564, reward 1192.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 132\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 3565, reward 1062.0, memory_length 2000, epsilon 0.0009954703940636294, time 742.0, rides 140\n",
      "Initial State is  [1, 19, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3566, reward 1407.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 127\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 3567, reward 1263.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 118\n",
      "Initial State is  [4, 20, 1]\n",
      "episode 3568, reward 1048.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 122\n",
      "Initial State is  [4, 6, 1]\n",
      "episode 3569, reward 1316.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 123\n",
      "Initial State is  [1, 1, 5]\n",
      "episode 3570, reward 1238.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 132\n",
      "Initial State is  [0, 19, 6]\n",
      "episode 3571, reward 1085.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 131\n",
      "Initial State is  [1, 21, 6]\n",
      "episode 3572, reward 1326.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 123\n",
      "Initial State is  [4, 6, 1]\n",
      "episode 3573, reward 1406.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 122\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 3574, reward 1156.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 123\n",
      "Initial State is  [3, 15, 6]\n",
      "episode 3575, reward 1148.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 133\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 3576, reward 1014.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 123\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 3577, reward 883.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 125\n",
      "Initial State is  [2, 20, 1]\n",
      "episode 3578, reward 1223.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 140\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 3579, reward 1539.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 138\n",
      "Initial State is  [0, 12, 6]\n",
      "episode 3580, reward 1524.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 125\n",
      "Initial State is  [4, 16, 5]\n",
      "episode 3581, reward 1213.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 131\n",
      "Initial State is  [2, 14, 6]\n",
      "episode 3582, reward 1188.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 123\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 3583, reward 1106.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 125\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 3584, reward 1348.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 122\n",
      "Initial State is  [4, 23, 6]\n",
      "episode 3585, reward 1142.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 121\n",
      "Initial State is  [1, 3, 6]\n",
      "episode 3586, reward 1174.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 130\n",
      "Initial State is  [4, 1, 2]\n",
      "episode 3587, reward 1276.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 116\n",
      "Initial State is  [0, 1, 0]\n",
      "episode 3588, reward 1135.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 133\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 3589, reward 1005.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 122\n",
      "Initial State is  [4, 3, 6]\n",
      "episode 3590, reward 1186.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 127\n",
      "Initial State is  [0, 20, 4]\n",
      "episode 3591, reward 1295.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 124\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 3592, reward 1315.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 125\n",
      "Initial State is  [3, 10, 6]\n",
      "episode 3593, reward 1234.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 134\n",
      "Initial State is  [3, 16, 5]\n",
      "episode 3594, reward 1274.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 120\n",
      "Initial State is  [0, 18, 5]\n",
      "episode 3595, reward 1358.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 124\n",
      "Initial State is  [1, 18, 0]\n",
      "episode 3596, reward 1154.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 118\n",
      "Initial State is  [0, 9, 2]\n",
      "episode 3597, reward 1280.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 127\n",
      "Initial State is  [0, 15, 1]\n",
      "episode 3598, reward 1126.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 118\n",
      "Initial State is  [1, 8, 6]\n",
      "episode 3599, reward 1107.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 129\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 3600, reward 1386.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 130\n",
      "Initial State is  [1, 23, 5]\n",
      "episode 3601, reward 1283.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 118\n",
      "Initial State is  [3, 16, 2]\n",
      "episode 3602, reward 1267.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 115\n",
      "Initial State is  [0, 13, 1]\n",
      "episode 3603, reward 1168.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 131\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 3604, reward 1054.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 124\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 3605, reward 1221.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 124\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 3606, reward 1291.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 120\n",
      "Initial State is  [4, 6, 5]\n",
      "episode 3607, reward 1227.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 126\n",
      "Initial State is  [2, 1, 1]\n",
      "episode 3608, reward 982.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 114\n",
      "Initial State is  [2, 3, 2]\n",
      "episode 3609, reward 1225.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 126\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 3610, reward 1356.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 118\n",
      "Initial State is  [3, 23, 5]\n",
      "episode 3611, reward 1059.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 138\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 3612, reward 1161.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 127\n",
      "Initial State is  [3, 4, 5]\n",
      "episode 3613, reward 1202.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 128\n",
      "Initial State is  [4, 9, 3]\n",
      "episode 3614, reward 1070.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 129\n",
      "Initial State is  [2, 0, 3]\n",
      "episode 3615, reward 1376.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 129\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 3616, reward 1005.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 125\n",
      "Initial State is  [1, 10, 3]\n",
      "episode 3617, reward 969.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 131\n",
      "Initial State is  [2, 0, 4]\n",
      "episode 3618, reward 1326.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 116\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 3619, reward 1200.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 126\n",
      "Initial State is  [2, 15, 6]\n",
      "episode 3620, reward 1523.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 130\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 3621, reward 1350.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 128\n",
      "Initial State is  [4, 16, 2]\n",
      "episode 3622, reward 1363.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 112\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 3623, reward 1248.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 122\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 3624, reward 1188.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 129\n",
      "Initial State is  [2, 16, 1]\n",
      "episode 3625, reward 1464.0, memory_length 2000, epsilon 0.0009954703940636294, time 720.0, rides 127\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 3626, reward 1147.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 126\n",
      "Initial State is  [0, 19, 3]\n",
      "episode 3627, reward 898.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 125\n",
      "Initial State is  [0, 21, 5]\n",
      "episode 3628, reward 1283.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 130\n",
      "Initial State is  [2, 14, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3629, reward 1395.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 123\n",
      "Initial State is  [1, 1, 5]\n",
      "episode 3630, reward 1128.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 121\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 3631, reward 1191.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 130\n",
      "Initial State is  [2, 0, 1]\n",
      "episode 3632, reward 1144.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 120\n",
      "Initial State is  [2, 21, 1]\n",
      "episode 3633, reward 1435.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 141\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 3634, reward 1360.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 132\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 3635, reward 1045.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 127\n",
      "Initial State is  [1, 19, 5]\n",
      "episode 3636, reward 1259.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 131\n",
      "Initial State is  [1, 19, 5]\n",
      "episode 3637, reward 1613.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 124\n",
      "Initial State is  [0, 13, 3]\n",
      "episode 3638, reward 1344.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 124\n",
      "Initial State is  [0, 15, 0]\n",
      "episode 3639, reward 1357.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 122\n",
      "Initial State is  [1, 20, 6]\n",
      "episode 3640, reward 1249.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 126\n",
      "Initial State is  [4, 1, 0]\n",
      "episode 3641, reward 1334.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 125\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 3642, reward 1048.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 125\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 3643, reward 1245.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 135\n",
      "Initial State is  [0, 5, 2]\n",
      "episode 3644, reward 1227.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 116\n",
      "Initial State is  [2, 19, 0]\n",
      "episode 3645, reward 1214.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 123\n",
      "Initial State is  [0, 0, 0]\n",
      "episode 3646, reward 1086.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 124\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 3647, reward 1116.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 128\n",
      "Initial State is  [0, 4, 1]\n",
      "episode 3648, reward 1502.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 129\n",
      "Initial State is  [2, 20, 0]\n",
      "episode 3649, reward 1313.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 126\n",
      "Initial State is  [0, 23, 4]\n",
      "episode 3650, reward 1176.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 125\n",
      "Initial State is  [2, 20, 6]\n",
      "episode 3651, reward 1472.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 115\n",
      "Initial State is  [4, 0, 6]\n",
      "episode 3652, reward 1533.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 127\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 3653, reward 1453.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 121\n",
      "Initial State is  [1, 8, 6]\n",
      "episode 3654, reward 1191.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 117\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 3655, reward 1474.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 129\n",
      "Initial State is  [3, 5, 2]\n",
      "episode 3656, reward 1247.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 137\n",
      "Initial State is  [0, 8, 6]\n",
      "episode 3657, reward 1357.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 135\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 3658, reward 1084.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 136\n",
      "Initial State is  [2, 12, 6]\n",
      "episode 3659, reward 1375.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 125\n",
      "Initial State is  [4, 12, 5]\n",
      "episode 3660, reward 1075.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 110\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 3661, reward 1221.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 135\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 3662, reward 1275.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 124\n",
      "Initial State is  [3, 13, 3]\n",
      "episode 3663, reward 1195.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 121\n",
      "Initial State is  [3, 1, 6]\n",
      "episode 3664, reward 1388.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 127\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 3665, reward 1019.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 128\n",
      "Initial State is  [2, 16, 6]\n",
      "episode 3666, reward 1474.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 129\n",
      "Initial State is  [3, 1, 2]\n",
      "episode 3667, reward 1471.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 141\n",
      "Initial State is  [4, 12, 0]\n",
      "episode 3668, reward 1214.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 133\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 3669, reward 1213.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 126\n",
      "Initial State is  [4, 8, 2]\n",
      "episode 3670, reward 1344.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 109\n",
      "Initial State is  [0, 21, 1]\n",
      "episode 3671, reward 1456.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 121\n",
      "Initial State is  [3, 13, 6]\n",
      "episode 3672, reward 1437.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 131\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 3673, reward 1247.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 118\n",
      "Initial State is  [0, 15, 3]\n",
      "episode 3674, reward 1215.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 121\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 3675, reward 1438.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 119\n",
      "Initial State is  [2, 20, 6]\n",
      "episode 3676, reward 1031.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 119\n",
      "Initial State is  [4, 17, 5]\n",
      "episode 3677, reward 1321.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 123\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 3678, reward 1215.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 126\n",
      "Initial State is  [3, 3, 0]\n",
      "episode 3679, reward 1449.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 121\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 3680, reward 1141.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 127\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 3681, reward 1478.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 123\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 3682, reward 1325.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 132\n",
      "Initial State is  [0, 5, 5]\n",
      "episode 3683, reward 1246.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 141\n",
      "Initial State is  [0, 9, 2]\n",
      "episode 3684, reward 1314.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 130\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 3685, reward 1252.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 131\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 3686, reward 1053.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 118\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 3687, reward 1506.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 129\n",
      "Initial State is  [3, 19, 2]\n",
      "episode 3688, reward 1181.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 130\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 3689, reward 1302.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 123\n",
      "Initial State is  [1, 6, 4]\n",
      "episode 3690, reward 1166.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 132\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 3691, reward 970.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 123\n",
      "Initial State is  [0, 1, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3692, reward 1377.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 124\n",
      "Initial State is  [2, 17, 3]\n",
      "episode 3693, reward 1079.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 127\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 3694, reward 1352.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 126\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 3695, reward 847.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 119\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 3696, reward 996.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 120\n",
      "Initial State is  [4, 4, 1]\n",
      "episode 3697, reward 1266.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 123\n",
      "Initial State is  [3, 4, 0]\n",
      "episode 3698, reward 1323.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 127\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 3699, reward 895.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 123\n",
      "Initial State is  [2, 23, 4]\n",
      "episode 3700, reward 1434.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 122\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 3701, reward 923.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 110\n",
      "Initial State is  [2, 6, 6]\n",
      "episode 3702, reward 1259.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 127\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 3703, reward 1378.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 121\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 3704, reward 1395.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 115\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 3705, reward 1201.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 136\n",
      "Initial State is  [2, 13, 5]\n",
      "episode 3706, reward 1393.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 121\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 3707, reward 1163.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 121\n",
      "Initial State is  [2, 18, 5]\n",
      "episode 3708, reward 1399.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 127\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 3709, reward 1389.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 116\n",
      "Initial State is  [4, 16, 0]\n",
      "episode 3710, reward 1310.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 130\n",
      "Initial State is  [1, 8, 2]\n",
      "episode 3711, reward 1211.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 114\n",
      "Initial State is  [3, 22, 1]\n",
      "episode 3712, reward 1050.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 120\n",
      "Initial State is  [3, 16, 0]\n",
      "episode 3713, reward 1315.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 126\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 3714, reward 1429.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 122\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 3715, reward 1394.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 121\n",
      "Initial State is  [0, 9, 0]\n",
      "episode 3716, reward 1221.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 133\n",
      "Initial State is  [0, 4, 3]\n",
      "episode 3717, reward 1020.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 123\n",
      "Initial State is  [1, 12, 3]\n",
      "episode 3718, reward 1232.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 125\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 3719, reward 1203.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 124\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 3720, reward 1293.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 133\n",
      "Initial State is  [4, 9, 4]\n",
      "episode 3721, reward 1232.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 121\n",
      "Initial State is  [0, 4, 4]\n",
      "episode 3722, reward 1098.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 118\n",
      "Initial State is  [3, 14, 5]\n",
      "episode 3723, reward 1356.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 123\n",
      "Initial State is  [1, 13, 6]\n",
      "episode 3724, reward 1143.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 123\n",
      "Initial State is  [2, 8, 4]\n",
      "episode 3725, reward 1276.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 125\n",
      "Initial State is  [3, 2, 1]\n",
      "episode 3726, reward 1342.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 122\n",
      "Initial State is  [1, 21, 0]\n",
      "episode 3727, reward 1176.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 122\n",
      "Initial State is  [2, 15, 1]\n",
      "episode 3728, reward 1309.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 122\n",
      "Initial State is  [2, 19, 6]\n",
      "episode 3729, reward 1248.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 118\n",
      "Initial State is  [4, 13, 1]\n",
      "episode 3730, reward 1244.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 129\n",
      "Initial State is  [2, 13, 5]\n",
      "episode 3731, reward 1063.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 138\n",
      "Initial State is  [2, 6, 4]\n",
      "episode 3732, reward 1208.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 125\n",
      "Initial State is  [2, 16, 0]\n",
      "episode 3733, reward 1415.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 121\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 3734, reward 1282.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 132\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 3735, reward 1395.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 123\n",
      "Initial State is  [4, 10, 1]\n",
      "episode 3736, reward 1110.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 130\n",
      "Initial State is  [0, 4, 4]\n",
      "episode 3737, reward 1240.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 123\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 3738, reward 1293.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 116\n",
      "Initial State is  [1, 1, 4]\n",
      "episode 3739, reward 1146.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 134\n",
      "Initial State is  [1, 16, 4]\n",
      "episode 3740, reward 1199.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 120\n",
      "Initial State is  [4, 10, 5]\n",
      "episode 3741, reward 1414.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 124\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 3742, reward 1354.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 121\n",
      "Initial State is  [1, 12, 0]\n",
      "episode 3743, reward 1453.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 115\n",
      "Initial State is  [4, 3, 0]\n",
      "episode 3744, reward 1564.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 123\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 3745, reward 1080.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 128\n",
      "Initial State is  [1, 13, 3]\n",
      "episode 3746, reward 938.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 113\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 3747, reward 1249.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 124\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 3748, reward 1287.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 116\n",
      "Initial State is  [1, 13, 4]\n",
      "episode 3749, reward 1156.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 118\n",
      "Initial State is  [2, 13, 5]\n",
      "episode 3750, reward 1016.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 122\n",
      "Initial State is  [0, 2, 5]\n",
      "episode 3751, reward 1044.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 125\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 3752, reward 1221.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 125\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 3753, reward 1164.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 130\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 3754, reward 1207.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 124\n",
      "Initial State is  [1, 16, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3755, reward 1379.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 136\n",
      "Initial State is  [1, 3, 6]\n",
      "episode 3756, reward 1284.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 124\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 3757, reward 1420.0, memory_length 2000, epsilon 0.0009954703940636294, time 743.0, rides 131\n",
      "Initial State is  [2, 20, 1]\n",
      "episode 3758, reward 1167.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 115\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 3759, reward 1254.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 127\n",
      "Initial State is  [4, 11, 5]\n",
      "episode 3760, reward 955.0, memory_length 2000, epsilon 0.0009954703940636294, time 739.0, rides 114\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 3761, reward 1446.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 127\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 3762, reward 1401.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 129\n",
      "Initial State is  [4, 15, 1]\n",
      "episode 3763, reward 1262.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 121\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 3764, reward 1301.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 128\n",
      "Initial State is  [2, 13, 2]\n",
      "episode 3765, reward 1043.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 111\n",
      "Initial State is  [2, 21, 0]\n",
      "episode 3766, reward 1348.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 128\n",
      "Initial State is  [2, 4, 3]\n",
      "episode 3767, reward 1113.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 123\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 3768, reward 1147.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 127\n",
      "Initial State is  [0, 21, 2]\n",
      "episode 3769, reward 1305.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 124\n",
      "Initial State is  [3, 10, 1]\n",
      "episode 3770, reward 1158.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 123\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 3771, reward 1352.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 119\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 3772, reward 1528.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 126\n",
      "Initial State is  [3, 20, 0]\n",
      "episode 3773, reward 1424.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 144\n",
      "Initial State is  [4, 23, 6]\n",
      "episode 3774, reward 1118.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 126\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 3775, reward 1242.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 130\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 3776, reward 1229.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 122\n",
      "Initial State is  [2, 14, 1]\n",
      "episode 3777, reward 1287.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 119\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 3778, reward 1171.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 125\n",
      "Initial State is  [1, 2, 0]\n",
      "episode 3779, reward 1431.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 128\n",
      "Initial State is  [4, 22, 2]\n",
      "episode 3780, reward 823.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 121\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 3781, reward 1090.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 125\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 3782, reward 968.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 123\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 3783, reward 1426.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 129\n",
      "Initial State is  [2, 20, 6]\n",
      "episode 3784, reward 1266.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 121\n",
      "Initial State is  [0, 15, 1]\n",
      "episode 3785, reward 994.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 118\n",
      "Initial State is  [4, 22, 4]\n",
      "episode 3786, reward 1423.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 126\n",
      "Initial State is  [3, 9, 6]\n",
      "episode 3787, reward 1492.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 129\n",
      "Initial State is  [2, 23, 3]\n",
      "episode 3788, reward 1216.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 121\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 3789, reward 1249.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 130\n",
      "Initial State is  [3, 9, 0]\n",
      "episode 3790, reward 1148.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 122\n",
      "Initial State is  [2, 16, 0]\n",
      "episode 3791, reward 1202.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 130\n",
      "Initial State is  [3, 19, 3]\n",
      "episode 3792, reward 1409.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 121\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 3793, reward 1289.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 135\n",
      "Initial State is  [4, 17, 4]\n",
      "episode 3794, reward 1209.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 122\n",
      "Initial State is  [0, 13, 1]\n",
      "episode 3795, reward 1258.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 122\n",
      "Initial State is  [0, 17, 2]\n",
      "episode 3796, reward 1287.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 125\n",
      "Initial State is  [1, 12, 6]\n",
      "episode 3797, reward 1288.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 118\n",
      "Initial State is  [4, 21, 1]\n",
      "episode 3798, reward 1124.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 119\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 3799, reward 1072.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 118\n",
      "Initial State is  [3, 5, 6]\n",
      "episode 3800, reward 1388.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 136\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 3801, reward 1037.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 125\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 3802, reward 1518.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 119\n",
      "Initial State is  [4, 12, 4]\n",
      "episode 3803, reward 1392.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 121\n",
      "Initial State is  [3, 6, 3]\n",
      "episode 3804, reward 1374.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 130\n",
      "Initial State is  [4, 22, 3]\n",
      "episode 3805, reward 1347.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 128\n",
      "Initial State is  [0, 20, 0]\n",
      "episode 3806, reward 1275.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 124\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 3807, reward 1262.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 119\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 3808, reward 1481.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 127\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 3809, reward 1313.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 121\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 3810, reward 1372.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 135\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 3811, reward 1301.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 123\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 3812, reward 1382.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 130\n",
      "Initial State is  [1, 17, 5]\n",
      "episode 3813, reward 948.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 128\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 3814, reward 1354.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 129\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 3815, reward 1375.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 124\n",
      "Initial State is  [1, 1, 1]\n",
      "episode 3816, reward 1212.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 113\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 3817, reward 1304.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 134\n",
      "Initial State is  [0, 15, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3818, reward 1233.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 122\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 3819, reward 1238.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 127\n",
      "Initial State is  [1, 21, 3]\n",
      "episode 3820, reward 1465.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 123\n",
      "Initial State is  [1, 20, 5]\n",
      "episode 3821, reward 1266.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 117\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 3822, reward 1259.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 120\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 3823, reward 1286.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 130\n",
      "Initial State is  [1, 10, 0]\n",
      "episode 3824, reward 884.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 131\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 3825, reward 1151.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 126\n",
      "Initial State is  [4, 23, 1]\n",
      "episode 3826, reward 1106.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 132\n",
      "Initial State is  [3, 15, 5]\n",
      "episode 3827, reward 1080.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 122\n",
      "Initial State is  [2, 14, 5]\n",
      "episode 3828, reward 1307.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 121\n",
      "Initial State is  [2, 6, 4]\n",
      "episode 3829, reward 1009.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 129\n",
      "Initial State is  [0, 5, 5]\n",
      "episode 3830, reward 1408.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 130\n",
      "Initial State is  [3, 9, 1]\n",
      "episode 3831, reward 1397.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 126\n",
      "Initial State is  [0, 23, 5]\n",
      "episode 3832, reward 1105.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 134\n",
      "Initial State is  [3, 3, 1]\n",
      "episode 3833, reward 1319.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 124\n",
      "Initial State is  [2, 0, 4]\n",
      "episode 3834, reward 931.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 123\n",
      "Initial State is  [2, 12, 4]\n",
      "episode 3835, reward 1274.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 123\n",
      "Initial State is  [0, 16, 3]\n",
      "episode 3836, reward 1220.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 127\n",
      "Initial State is  [0, 17, 0]\n",
      "episode 3837, reward 1145.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 127\n",
      "Initial State is  [2, 21, 1]\n",
      "episode 3838, reward 1317.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 117\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 3839, reward 1282.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 124\n",
      "Initial State is  [4, 3, 6]\n",
      "episode 3840, reward 1220.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 117\n",
      "Initial State is  [4, 17, 4]\n",
      "episode 3841, reward 1088.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 117\n",
      "Initial State is  [2, 9, 3]\n",
      "episode 3842, reward 1355.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 128\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 3843, reward 1131.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 125\n",
      "Initial State is  [2, 15, 1]\n",
      "episode 3844, reward 1192.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 129\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 3845, reward 1390.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 117\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 3846, reward 1216.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 117\n",
      "Initial State is  [1, 22, 4]\n",
      "episode 3847, reward 1365.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 131\n",
      "Initial State is  [4, 16, 0]\n",
      "episode 3848, reward 1066.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 118\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 3849, reward 1059.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 136\n",
      "Initial State is  [1, 21, 3]\n",
      "episode 3850, reward 1183.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 117\n",
      "Initial State is  [3, 3, 5]\n",
      "episode 3851, reward 1549.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 120\n",
      "Initial State is  [1, 19, 5]\n",
      "episode 3852, reward 1224.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 121\n",
      "Initial State is  [1, 4, 4]\n",
      "episode 3853, reward 1161.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 139\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 3854, reward 915.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 110\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 3855, reward 1378.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 129\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 3856, reward 1459.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 128\n",
      "Initial State is  [1, 1, 4]\n",
      "episode 3857, reward 1326.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 120\n",
      "Initial State is  [3, 19, 3]\n",
      "episode 3858, reward 1291.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 122\n",
      "Initial State is  [3, 3, 1]\n",
      "episode 3859, reward 1185.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 113\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 3860, reward 1336.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 117\n",
      "Initial State is  [1, 1, 5]\n",
      "episode 3861, reward 1260.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 122\n",
      "Initial State is  [0, 17, 0]\n",
      "episode 3862, reward 1436.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 133\n",
      "Initial State is  [1, 15, 4]\n",
      "episode 3863, reward 1339.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 125\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 3864, reward 1349.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 127\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 3865, reward 1028.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 110\n",
      "Initial State is  [4, 18, 2]\n",
      "episode 3866, reward 1165.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 116\n",
      "Initial State is  [0, 2, 5]\n",
      "episode 3867, reward 1151.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 124\n",
      "Initial State is  [2, 16, 4]\n",
      "episode 3868, reward 1261.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 125\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 3869, reward 1255.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 116\n",
      "Initial State is  [2, 6, 5]\n",
      "episode 3870, reward 1519.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 122\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 3871, reward 1145.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 114\n",
      "Initial State is  [2, 23, 3]\n",
      "episode 3872, reward 1175.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 130\n",
      "Initial State is  [4, 15, 2]\n",
      "episode 3873, reward 992.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 114\n",
      "Initial State is  [3, 12, 2]\n",
      "episode 3874, reward 1078.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 117\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 3875, reward 945.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 121\n",
      "Initial State is  [1, 1, 0]\n",
      "episode 3876, reward 889.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 117\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 3877, reward 1338.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 127\n",
      "Initial State is  [4, 16, 5]\n",
      "episode 3878, reward 1046.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 134\n",
      "Initial State is  [1, 13, 2]\n",
      "episode 3879, reward 1280.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 128\n",
      "Initial State is  [1, 1, 0]\n",
      "episode 3880, reward 1215.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 125\n",
      "Initial State is  [3, 8, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3881, reward 1243.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 123\n",
      "Initial State is  [3, 17, 6]\n",
      "episode 3882, reward 1146.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 117\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 3883, reward 1360.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 114\n",
      "Initial State is  [2, 23, 0]\n",
      "episode 3884, reward 1133.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 123\n",
      "Initial State is  [4, 11, 2]\n",
      "episode 3885, reward 1308.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 116\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 3886, reward 1029.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 129\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 3887, reward 1312.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 123\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 3888, reward 1436.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 129\n",
      "Initial State is  [4, 21, 4]\n",
      "episode 3889, reward 1398.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 133\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 3890, reward 1257.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 123\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 3891, reward 1221.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 118\n",
      "Initial State is  [0, 4, 5]\n",
      "episode 3892, reward 945.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 113\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 3893, reward 1030.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 129\n",
      "Initial State is  [3, 1, 0]\n",
      "episode 3894, reward 1320.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 116\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 3895, reward 1203.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 122\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 3896, reward 1304.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 121\n",
      "Initial State is  [1, 13, 1]\n",
      "episode 3897, reward 1208.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 127\n",
      "Initial State is  [2, 19, 0]\n",
      "episode 3898, reward 1174.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 129\n",
      "Initial State is  [3, 2, 3]\n",
      "episode 3899, reward 1205.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 129\n",
      "Initial State is  [4, 22, 5]\n",
      "episode 3900, reward 1234.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 131\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 3901, reward 1242.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 121\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 3902, reward 1240.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 120\n",
      "Initial State is  [4, 12, 6]\n",
      "episode 3903, reward 1435.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 138\n",
      "Initial State is  [0, 21, 4]\n",
      "episode 3904, reward 1237.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 121\n",
      "Initial State is  [3, 3, 6]\n",
      "episode 3905, reward 1250.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 116\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 3906, reward 1287.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 129\n",
      "Initial State is  [2, 0, 1]\n",
      "episode 3907, reward 1459.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 124\n",
      "Initial State is  [2, 1, 1]\n",
      "episode 3908, reward 1046.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 133\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 3909, reward 1054.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 128\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 3910, reward 880.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 120\n",
      "Initial State is  [4, 22, 1]\n",
      "episode 3911, reward 1115.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 113\n",
      "Initial State is  [1, 3, 1]\n",
      "episode 3912, reward 1335.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 125\n",
      "Initial State is  [4, 16, 6]\n",
      "episode 3913, reward 1275.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 133\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 3914, reward 1048.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 131\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 3915, reward 1173.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 126\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 3916, reward 1281.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 124\n",
      "Initial State is  [4, 16, 1]\n",
      "episode 3917, reward 1317.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 136\n",
      "Initial State is  [1, 4, 5]\n",
      "episode 3918, reward 1167.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 130\n",
      "Initial State is  [3, 20, 0]\n",
      "episode 3919, reward 958.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 106\n",
      "Initial State is  [2, 1, 4]\n",
      "episode 3920, reward 1135.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 114\n",
      "Initial State is  [1, 14, 6]\n",
      "episode 3921, reward 1248.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 117\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 3922, reward 1145.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 128\n",
      "Initial State is  [3, 8, 3]\n",
      "episode 3923, reward 1038.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 120\n",
      "Initial State is  [4, 11, 4]\n",
      "episode 3924, reward 1023.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 120\n",
      "Initial State is  [0, 23, 4]\n",
      "episode 3925, reward 1629.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 131\n",
      "Initial State is  [1, 12, 6]\n",
      "episode 3926, reward 1185.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 130\n",
      "Initial State is  [4, 12, 5]\n",
      "episode 3927, reward 1316.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 125\n",
      "Initial State is  [1, 14, 2]\n",
      "episode 3928, reward 1292.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 118\n",
      "Initial State is  [4, 5, 2]\n",
      "episode 3929, reward 1320.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 134\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 3930, reward 1284.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 127\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 3931, reward 1124.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 119\n",
      "Initial State is  [0, 6, 2]\n",
      "episode 3932, reward 962.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 117\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 3933, reward 1354.0, memory_length 2000, epsilon 0.0009954703940636294, time 743.0, rides 129\n",
      "Initial State is  [4, 16, 0]\n",
      "episode 3934, reward 984.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 124\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 3935, reward 1238.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 128\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 3936, reward 1275.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 116\n",
      "Initial State is  [4, 15, 0]\n",
      "episode 3937, reward 1355.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 125\n",
      "Initial State is  [2, 2, 0]\n",
      "episode 3938, reward 1201.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 125\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 3939, reward 1256.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 113\n",
      "Initial State is  [2, 5, 4]\n",
      "episode 3940, reward 1256.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 124\n",
      "Initial State is  [3, 12, 0]\n",
      "episode 3941, reward 1279.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 127\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 3942, reward 1446.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 119\n",
      "Initial State is  [4, 6, 2]\n",
      "episode 3943, reward 1275.0, memory_length 2000, epsilon 0.0009954703940636294, time 720.0, rides 128\n",
      "Initial State is  [1, 20, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3944, reward 1303.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 125\n",
      "Initial State is  [2, 11, 4]\n",
      "episode 3945, reward 1069.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 111\n",
      "Initial State is  [2, 3, 0]\n",
      "episode 3946, reward 1419.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 120\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 3947, reward 1196.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 128\n",
      "Initial State is  [1, 6, 0]\n",
      "episode 3948, reward 1230.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 121\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 3949, reward 1193.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 133\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 3950, reward 1158.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 130\n",
      "Initial State is  [3, 6, 0]\n",
      "episode 3951, reward 1252.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 133\n",
      "Initial State is  [1, 3, 2]\n",
      "episode 3952, reward 1227.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 134\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 3953, reward 1077.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 130\n",
      "Initial State is  [0, 15, 0]\n",
      "episode 3954, reward 1320.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 124\n",
      "Initial State is  [0, 9, 0]\n",
      "episode 3955, reward 1107.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 120\n",
      "Initial State is  [1, 1, 2]\n",
      "episode 3956, reward 1192.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 121\n",
      "Initial State is  [3, 6, 2]\n",
      "episode 3957, reward 1315.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 120\n",
      "Initial State is  [2, 2, 4]\n",
      "episode 3958, reward 1324.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 128\n",
      "Initial State is  [1, 19, 5]\n",
      "episode 3959, reward 1250.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 129\n",
      "Initial State is  [2, 18, 1]\n",
      "episode 3960, reward 1223.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 120\n",
      "Initial State is  [2, 21, 0]\n",
      "episode 3961, reward 1151.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 121\n",
      "Initial State is  [1, 10, 6]\n",
      "episode 3962, reward 1072.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 107\n",
      "Initial State is  [2, 20, 5]\n",
      "episode 3963, reward 1250.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 128\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 3964, reward 1240.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 113\n",
      "Initial State is  [4, 23, 0]\n",
      "episode 3965, reward 1255.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 116\n",
      "Initial State is  [2, 11, 1]\n",
      "episode 3966, reward 1285.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 124\n",
      "Initial State is  [2, 8, 4]\n",
      "episode 3967, reward 1118.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 130\n",
      "Initial State is  [1, 17, 2]\n",
      "episode 3968, reward 1000.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 121\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 3969, reward 1545.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 124\n",
      "Initial State is  [1, 21, 0]\n",
      "episode 3970, reward 1137.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 129\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 3971, reward 1098.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 116\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 3972, reward 1077.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 121\n",
      "Initial State is  [4, 22, 4]\n",
      "episode 3973, reward 1051.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 125\n",
      "Initial State is  [1, 18, 3]\n",
      "episode 3974, reward 1347.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 117\n",
      "Initial State is  [2, 2, 5]\n",
      "episode 3975, reward 1202.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 127\n",
      "Initial State is  [1, 1, 4]\n",
      "episode 3976, reward 1043.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 121\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 3977, reward 1331.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 124\n",
      "Initial State is  [2, 0, 2]\n",
      "episode 3978, reward 706.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 114\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 3979, reward 1380.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 126\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 3980, reward 997.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 130\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 3981, reward 1282.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 123\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 3982, reward 1316.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 123\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 3983, reward 1252.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 124\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 3984, reward 1077.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 128\n",
      "Initial State is  [3, 5, 6]\n",
      "episode 3985, reward 1130.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 137\n",
      "Initial State is  [0, 13, 3]\n",
      "episode 3986, reward 1167.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 125\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 3987, reward 1197.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 117\n",
      "Initial State is  [4, 17, 6]\n",
      "episode 3988, reward 1235.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 119\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 3989, reward 1052.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 128\n",
      "Initial State is  [2, 2, 4]\n",
      "episode 3990, reward 1294.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 120\n",
      "Initial State is  [1, 9, 2]\n",
      "episode 3991, reward 777.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 117\n",
      "Initial State is  [2, 5, 3]\n",
      "episode 3992, reward 1297.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 119\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 3993, reward 1236.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 116\n",
      "Initial State is  [1, 1, 2]\n",
      "episode 3994, reward 940.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 121\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 3995, reward 1220.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 118\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 3996, reward 1156.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 122\n",
      "Initial State is  [2, 13, 5]\n",
      "episode 3997, reward 1294.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 129\n",
      "Initial State is  [3, 22, 0]\n",
      "episode 3998, reward 1191.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 117\n",
      "Initial State is  [4, 14, 3]\n",
      "episode 3999, reward 1394.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 134\n",
      "Initial State is  [3, 20, 1]\n",
      "episode 4000, reward 1155.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 126\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 4001, reward 1249.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 121\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 4002, reward 1158.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 117\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 4003, reward 1134.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 119\n",
      "Initial State is  [4, 4, 1]\n",
      "episode 4004, reward 1087.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 131\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 4005, reward 1198.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 120\n",
      "Initial State is  [3, 5, 6]\n",
      "episode 4006, reward 1465.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 121\n",
      "Initial State is  [0, 20, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4007, reward 1263.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 116\n",
      "Initial State is  [2, 5, 1]\n",
      "episode 4008, reward 911.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 127\n",
      "Initial State is  [1, 19, 5]\n",
      "episode 4009, reward 1175.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 133\n",
      "Initial State is  [3, 4, 5]\n",
      "episode 4010, reward 1324.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 125\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 4011, reward 867.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 122\n",
      "Initial State is  [2, 4, 0]\n",
      "episode 4012, reward 1341.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 122\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 4013, reward 1370.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 127\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 4014, reward 1017.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 128\n",
      "Initial State is  [2, 2, 0]\n",
      "episode 4015, reward 1289.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 138\n",
      "Initial State is  [2, 8, 4]\n",
      "episode 4016, reward 929.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 126\n",
      "Initial State is  [2, 9, 0]\n",
      "episode 4017, reward 1112.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 122\n",
      "Initial State is  [1, 9, 3]\n",
      "episode 4018, reward 965.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 125\n",
      "Initial State is  [4, 12, 6]\n",
      "episode 4019, reward 1283.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 125\n",
      "Initial State is  [3, 11, 3]\n",
      "episode 4020, reward 1358.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 124\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 4021, reward 1418.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 117\n",
      "Initial State is  [4, 8, 4]\n",
      "episode 4022, reward 1164.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 128\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 4023, reward 941.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 123\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 4024, reward 1102.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 131\n",
      "Initial State is  [1, 3, 5]\n",
      "episode 4025, reward 1128.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 114\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 4026, reward 1244.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 126\n",
      "Initial State is  [1, 22, 4]\n",
      "episode 4027, reward 1430.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 129\n",
      "Initial State is  [4, 14, 1]\n",
      "episode 4028, reward 1178.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 122\n",
      "Initial State is  [0, 22, 3]\n",
      "episode 4029, reward 1378.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 130\n",
      "Initial State is  [0, 12, 6]\n",
      "episode 4030, reward 1379.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 123\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 4031, reward 1122.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 125\n",
      "Initial State is  [1, 13, 1]\n",
      "episode 4032, reward 1440.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 126\n",
      "Initial State is  [3, 4, 4]\n",
      "episode 4033, reward 1286.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 127\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 4034, reward 1109.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 125\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 4035, reward 1307.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 142\n",
      "Initial State is  [2, 15, 0]\n",
      "episode 4036, reward 1214.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 129\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 4037, reward 1255.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 118\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 4038, reward 989.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 114\n",
      "Initial State is  [1, 16, 1]\n",
      "episode 4039, reward 1253.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 130\n",
      "Initial State is  [0, 0, 0]\n",
      "episode 4040, reward 1291.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 125\n",
      "Initial State is  [2, 19, 3]\n",
      "episode 4041, reward 1295.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 121\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 4042, reward 1286.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 134\n",
      "Initial State is  [3, 11, 1]\n",
      "episode 4043, reward 1405.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 127\n",
      "Initial State is  [4, 5, 0]\n",
      "episode 4044, reward 1240.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 126\n",
      "Initial State is  [3, 1, 6]\n",
      "episode 4045, reward 1167.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 128\n",
      "Initial State is  [3, 10, 3]\n",
      "episode 4046, reward 1409.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 131\n",
      "Initial State is  [1, 10, 2]\n",
      "episode 4047, reward 1328.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 120\n",
      "Initial State is  [3, 11, 5]\n",
      "episode 4048, reward 1312.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 125\n",
      "Initial State is  [1, 0, 6]\n",
      "episode 4049, reward 955.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 121\n",
      "Initial State is  [4, 23, 4]\n",
      "episode 4050, reward 1402.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 133\n",
      "Initial State is  [2, 23, 4]\n",
      "episode 4051, reward 1257.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 121\n",
      "Initial State is  [4, 21, 2]\n",
      "episode 4052, reward 1109.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 123\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 4053, reward 1238.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 128\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 4054, reward 1374.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 122\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 4055, reward 1428.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 119\n",
      "Initial State is  [4, 3, 4]\n",
      "episode 4056, reward 1199.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 121\n",
      "Initial State is  [1, 23, 2]\n",
      "episode 4057, reward 1177.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 120\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 4058, reward 1153.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 129\n",
      "Initial State is  [0, 5, 5]\n",
      "episode 4059, reward 1295.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 126\n",
      "Initial State is  [3, 7, 3]\n",
      "episode 4060, reward 1243.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 130\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 4061, reward 1212.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 126\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 4062, reward 1378.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 113\n",
      "Initial State is  [1, 14, 5]\n",
      "episode 4063, reward 1099.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 119\n",
      "Initial State is  [2, 3, 5]\n",
      "episode 4064, reward 1250.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 120\n",
      "Initial State is  [0, 18, 3]\n",
      "episode 4065, reward 1237.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 126\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 4066, reward 1675.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 140\n",
      "Initial State is  [4, 0, 6]\n",
      "episode 4067, reward 1378.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 127\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 4068, reward 1085.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 134\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 4069, reward 1358.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 120\n",
      "Initial State is  [2, 6, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4070, reward 1308.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 121\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 4071, reward 972.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 124\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 4072, reward 1209.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 128\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 4073, reward 1223.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 114\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 4074, reward 1214.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 133\n",
      "Initial State is  [1, 1, 1]\n",
      "episode 4075, reward 1328.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 121\n",
      "Initial State is  [1, 16, 5]\n",
      "episode 4076, reward 1640.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 126\n",
      "Initial State is  [4, 10, 6]\n",
      "episode 4077, reward 1256.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 121\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 4078, reward 1210.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 120\n",
      "Initial State is  [1, 21, 4]\n",
      "episode 4079, reward 1012.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 128\n",
      "Initial State is  [0, 2, 1]\n",
      "episode 4080, reward 1026.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 125\n",
      "Initial State is  [2, 2, 5]\n",
      "episode 4081, reward 1372.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 131\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 4082, reward 1201.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 134\n",
      "Initial State is  [4, 15, 0]\n",
      "episode 4083, reward 1450.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 125\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 4084, reward 1234.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 128\n",
      "Initial State is  [0, 1, 4]\n",
      "episode 4085, reward 1449.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 129\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 4086, reward 1086.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 127\n",
      "Initial State is  [0, 6, 3]\n",
      "episode 4087, reward 1293.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 128\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 4088, reward 1288.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 123\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 4089, reward 1264.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 121\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 4090, reward 1388.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 122\n",
      "Initial State is  [1, 11, 5]\n",
      "episode 4091, reward 1018.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 124\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 4092, reward 1141.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 131\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 4093, reward 1265.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 126\n",
      "Initial State is  [0, 11, 6]\n",
      "episode 4094, reward 1344.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 130\n",
      "Initial State is  [2, 18, 5]\n",
      "episode 4095, reward 1098.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 124\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 4096, reward 1157.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 124\n",
      "Initial State is  [3, 12, 4]\n",
      "episode 4097, reward 1419.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 126\n",
      "Initial State is  [0, 19, 5]\n",
      "episode 4098, reward 1394.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 124\n",
      "Initial State is  [0, 2, 0]\n",
      "episode 4099, reward 1286.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 127\n",
      "Initial State is  [1, 6, 0]\n",
      "episode 4100, reward 1299.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 126\n",
      "Initial State is  [0, 17, 2]\n",
      "episode 4101, reward 1009.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 140\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 4102, reward 1304.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 129\n",
      "Initial State is  [1, 4, 1]\n",
      "episode 4103, reward 1251.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 122\n",
      "Initial State is  [4, 3, 3]\n",
      "episode 4104, reward 1451.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 133\n",
      "Initial State is  [0, 12, 5]\n",
      "episode 4105, reward 910.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 118\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 4106, reward 1219.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 123\n",
      "Initial State is  [0, 12, 0]\n",
      "episode 4107, reward 1305.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 131\n",
      "Initial State is  [1, 17, 1]\n",
      "episode 4108, reward 1318.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 119\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 4109, reward 1373.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 124\n",
      "Initial State is  [0, 16, 3]\n",
      "episode 4110, reward 1082.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 127\n",
      "Initial State is  [2, 12, 4]\n",
      "episode 4111, reward 1305.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 135\n",
      "Initial State is  [0, 8, 2]\n",
      "episode 4112, reward 1241.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 128\n",
      "Initial State is  [0, 4, 4]\n",
      "episode 4113, reward 1336.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 125\n",
      "Initial State is  [4, 15, 3]\n",
      "episode 4114, reward 1388.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 118\n",
      "Initial State is  [3, 11, 1]\n",
      "episode 4115, reward 1292.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 136\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 4116, reward 1133.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 130\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 4117, reward 1292.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 123\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 4118, reward 1438.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 133\n",
      "Initial State is  [0, 18, 4]\n",
      "episode 4119, reward 1134.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 134\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 4120, reward 1346.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 130\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 4121, reward 1537.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 132\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 4122, reward 1312.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 125\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 4123, reward 1089.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 132\n",
      "Initial State is  [2, 3, 0]\n",
      "episode 4124, reward 1523.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 125\n",
      "Initial State is  [2, 0, 4]\n",
      "episode 4125, reward 1119.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 129\n",
      "Initial State is  [4, 13, 1]\n",
      "episode 4126, reward 1351.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 128\n",
      "Initial State is  [1, 7, 3]\n",
      "episode 4127, reward 1211.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 123\n",
      "Initial State is  [0, 8, 2]\n",
      "episode 4128, reward 1228.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 128\n",
      "Initial State is  [4, 8, 5]\n",
      "episode 4129, reward 1166.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 121\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 4130, reward 1476.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 126\n",
      "Initial State is  [2, 3, 1]\n",
      "episode 4131, reward 1122.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 119\n",
      "Initial State is  [4, 23, 3]\n",
      "episode 4132, reward 1340.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 125\n",
      "Initial State is  [2, 20, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4133, reward 1163.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 131\n",
      "Initial State is  [2, 6, 5]\n",
      "episode 4134, reward 1240.0, memory_length 2000, epsilon 0.0009954703940636294, time 739.0, rides 116\n",
      "Initial State is  [3, 13, 4]\n",
      "episode 4135, reward 1307.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 128\n",
      "Initial State is  [3, 13, 6]\n",
      "episode 4136, reward 1241.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 123\n",
      "Initial State is  [1, 21, 2]\n",
      "episode 4137, reward 1096.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 126\n",
      "Initial State is  [1, 12, 4]\n",
      "episode 4138, reward 1117.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 131\n",
      "Initial State is  [2, 23, 4]\n",
      "episode 4139, reward 908.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 122\n",
      "Initial State is  [1, 20, 4]\n",
      "episode 4140, reward 1298.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 136\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 4141, reward 1502.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 124\n",
      "Initial State is  [2, 17, 3]\n",
      "episode 4142, reward 1242.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 125\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 4143, reward 1206.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 128\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 4144, reward 1378.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 135\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 4145, reward 1128.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 116\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 4146, reward 1057.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 135\n",
      "Initial State is  [4, 12, 4]\n",
      "episode 4147, reward 1159.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 115\n",
      "Initial State is  [1, 11, 1]\n",
      "episode 4148, reward 1497.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 120\n",
      "Initial State is  [4, 9, 5]\n",
      "episode 4149, reward 1458.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 118\n",
      "Initial State is  [0, 22, 2]\n",
      "episode 4150, reward 1427.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 127\n",
      "Initial State is  [1, 21, 4]\n",
      "episode 4151, reward 1306.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 129\n",
      "Initial State is  [2, 19, 0]\n",
      "episode 4152, reward 1218.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 125\n",
      "Initial State is  [4, 0, 4]\n",
      "episode 4153, reward 1238.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 113\n",
      "Initial State is  [4, 21, 0]\n",
      "episode 4154, reward 1287.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 121\n",
      "Initial State is  [2, 23, 5]\n",
      "episode 4155, reward 1081.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 113\n",
      "Initial State is  [0, 22, 3]\n",
      "episode 4156, reward 1512.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 115\n",
      "Initial State is  [2, 14, 0]\n",
      "episode 4157, reward 1248.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 108\n",
      "Initial State is  [0, 20, 3]\n",
      "episode 4158, reward 1326.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 127\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 4159, reward 1228.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 129\n",
      "Initial State is  [0, 16, 5]\n",
      "episode 4160, reward 1121.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 120\n",
      "Initial State is  [1, 4, 6]\n",
      "episode 4161, reward 1354.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 125\n",
      "Initial State is  [1, 11, 1]\n",
      "episode 4162, reward 1455.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 115\n",
      "Initial State is  [2, 18, 1]\n",
      "episode 4163, reward 1306.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 130\n",
      "Initial State is  [2, 2, 5]\n",
      "episode 4164, reward 1321.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 130\n",
      "Initial State is  [1, 9, 4]\n",
      "episode 4165, reward 1154.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 121\n",
      "Initial State is  [2, 13, 3]\n",
      "episode 4166, reward 1184.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 132\n",
      "Initial State is  [2, 16, 5]\n",
      "episode 4167, reward 1245.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 120\n",
      "Initial State is  [4, 4, 6]\n",
      "episode 4168, reward 1328.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 122\n",
      "Initial State is  [2, 9, 1]\n",
      "episode 4169, reward 1143.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 129\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 4170, reward 1325.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 141\n",
      "Initial State is  [4, 17, 5]\n",
      "episode 4171, reward 1072.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 119\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 4172, reward 1221.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 125\n",
      "Initial State is  [2, 19, 6]\n",
      "episode 4173, reward 1310.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 119\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 4174, reward 1212.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 130\n",
      "Initial State is  [2, 7, 2]\n",
      "episode 4175, reward 1193.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 125\n",
      "Initial State is  [4, 10, 3]\n",
      "episode 4176, reward 1119.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 126\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 4177, reward 842.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 110\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 4178, reward 1330.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 118\n",
      "Initial State is  [3, 22, 6]\n",
      "episode 4179, reward 1102.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 116\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 4180, reward 1517.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 125\n",
      "Initial State is  [1, 4, 4]\n",
      "episode 4181, reward 1450.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 126\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 4182, reward 1316.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 126\n",
      "Initial State is  [1, 0, 4]\n",
      "episode 4183, reward 1169.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 124\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 4184, reward 1043.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 114\n",
      "Initial State is  [1, 3, 1]\n",
      "episode 4185, reward 1034.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 122\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 4186, reward 1249.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 119\n",
      "Initial State is  [3, 7, 5]\n",
      "episode 4187, reward 1285.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 133\n",
      "Initial State is  [2, 23, 0]\n",
      "episode 4188, reward 944.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 131\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 4189, reward 1393.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 121\n",
      "Initial State is  [3, 4, 2]\n",
      "episode 4190, reward 1100.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 120\n",
      "Initial State is  [2, 14, 0]\n",
      "episode 4191, reward 1174.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 120\n",
      "Initial State is  [0, 18, 4]\n",
      "episode 4192, reward 1159.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 121\n",
      "Initial State is  [2, 0, 4]\n",
      "episode 4193, reward 928.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 129\n",
      "Initial State is  [3, 16, 3]\n",
      "episode 4194, reward 929.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 123\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 4195, reward 1321.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 119\n",
      "Initial State is  [0, 17, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4196, reward 1170.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 119\n",
      "Initial State is  [1, 16, 3]\n",
      "episode 4197, reward 1388.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 126\n",
      "Initial State is  [0, 0, 3]\n",
      "episode 4198, reward 1242.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 128\n",
      "Initial State is  [4, 19, 3]\n",
      "episode 4199, reward 1182.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 123\n",
      "Initial State is  [4, 6, 1]\n",
      "episode 4200, reward 1027.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 131\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 4201, reward 1051.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 117\n",
      "Initial State is  [3, 7, 0]\n",
      "episode 4202, reward 1375.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 123\n",
      "Initial State is  [4, 15, 0]\n",
      "episode 4203, reward 1143.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 117\n",
      "Initial State is  [4, 18, 5]\n",
      "episode 4204, reward 1261.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 119\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 4205, reward 927.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 131\n",
      "Initial State is  [3, 0, 3]\n",
      "episode 4206, reward 1315.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 129\n",
      "Initial State is  [0, 8, 2]\n",
      "episode 4207, reward 1268.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 134\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 4208, reward 1403.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 127\n",
      "Initial State is  [4, 6, 5]\n",
      "episode 4209, reward 1183.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 127\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 4210, reward 1460.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 116\n",
      "Initial State is  [1, 7, 3]\n",
      "episode 4211, reward 1434.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 123\n",
      "Initial State is  [0, 19, 5]\n",
      "episode 4212, reward 1438.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 123\n",
      "Initial State is  [3, 11, 2]\n",
      "episode 4213, reward 1039.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 116\n",
      "Initial State is  [2, 3, 0]\n",
      "episode 4214, reward 1259.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 125\n",
      "Initial State is  [1, 7, 5]\n",
      "episode 4215, reward 1068.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 117\n",
      "Initial State is  [2, 23, 1]\n",
      "episode 4216, reward 1320.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 129\n",
      "Initial State is  [4, 14, 1]\n",
      "episode 4217, reward 1258.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 119\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 4218, reward 1233.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 116\n",
      "Initial State is  [0, 3, 6]\n",
      "episode 4219, reward 1181.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 119\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 4220, reward 1432.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 129\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 4221, reward 1124.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 127\n",
      "Initial State is  [0, 5, 5]\n",
      "episode 4222, reward 757.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 117\n",
      "Initial State is  [3, 19, 2]\n",
      "episode 4223, reward 1115.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 122\n",
      "Initial State is  [3, 9, 6]\n",
      "episode 4224, reward 1270.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 123\n",
      "Initial State is  [4, 5, 6]\n",
      "episode 4225, reward 1252.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 127\n",
      "Initial State is  [4, 4, 1]\n",
      "episode 4226, reward 1119.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 116\n",
      "Initial State is  [2, 3, 5]\n",
      "episode 4227, reward 1489.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 131\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 4228, reward 1255.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 124\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 4229, reward 1325.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 123\n",
      "Initial State is  [1, 21, 3]\n",
      "episode 4230, reward 958.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 114\n",
      "Initial State is  [2, 5, 3]\n",
      "episode 4231, reward 1392.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 124\n",
      "Initial State is  [2, 21, 1]\n",
      "episode 4232, reward 1211.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 124\n",
      "Initial State is  [1, 13, 3]\n",
      "episode 4233, reward 1327.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 124\n",
      "Initial State is  [3, 10, 2]\n",
      "episode 4234, reward 1144.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 127\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 4235, reward 1443.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 132\n",
      "Initial State is  [3, 2, 3]\n",
      "episode 4236, reward 1631.0, memory_length 2000, epsilon 0.0009954703940636294, time 741.0, rides 134\n",
      "Initial State is  [0, 1, 1]\n",
      "episode 4237, reward 1287.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 122\n",
      "Initial State is  [0, 15, 0]\n",
      "episode 4238, reward 1402.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 125\n",
      "Initial State is  [3, 22, 0]\n",
      "episode 4239, reward 1358.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 135\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 4240, reward 1179.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 139\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 4241, reward 1006.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 121\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 4242, reward 1302.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 120\n",
      "Initial State is  [4, 22, 4]\n",
      "episode 4243, reward 1170.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 118\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 4244, reward 1250.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 130\n",
      "Initial State is  [3, 7, 6]\n",
      "episode 4245, reward 1171.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 129\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 4246, reward 1246.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 123\n",
      "Initial State is  [2, 12, 2]\n",
      "episode 4247, reward 1240.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 120\n",
      "Initial State is  [3, 10, 5]\n",
      "episode 4248, reward 1125.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 117\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 4249, reward 1272.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 114\n",
      "Initial State is  [2, 6, 1]\n",
      "episode 4250, reward 1214.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 124\n",
      "Initial State is  [0, 9, 3]\n",
      "episode 4251, reward 1457.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 125\n",
      "Initial State is  [3, 19, 0]\n",
      "episode 4252, reward 1071.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 125\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 4253, reward 1391.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 116\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 4254, reward 1380.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 133\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 4255, reward 1308.0, memory_length 2000, epsilon 0.0009954703940636294, time 739.0, rides 137\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 4256, reward 1368.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 131\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 4257, reward 1168.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 127\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 4258, reward 1276.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 126\n",
      "Initial State is  [1, 15, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4259, reward 1489.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 136\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 4260, reward 1277.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 121\n",
      "Initial State is  [2, 18, 3]\n",
      "episode 4261, reward 898.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 115\n",
      "Initial State is  [3, 9, 6]\n",
      "episode 4262, reward 1039.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 113\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 4263, reward 1320.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 120\n",
      "Initial State is  [4, 2, 6]\n",
      "episode 4264, reward 1169.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 117\n",
      "Initial State is  [2, 18, 6]\n",
      "episode 4265, reward 1414.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 125\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 4266, reward 997.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 121\n",
      "Initial State is  [1, 22, 2]\n",
      "episode 4267, reward 1181.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 124\n",
      "Initial State is  [3, 7, 2]\n",
      "episode 4268, reward 1134.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 112\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 4269, reward 1039.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 114\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 4270, reward 1449.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 119\n",
      "Initial State is  [4, 4, 2]\n",
      "episode 4271, reward 1318.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 120\n",
      "Initial State is  [1, 16, 0]\n",
      "episode 4272, reward 1250.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 120\n",
      "Initial State is  [0, 8, 2]\n",
      "episode 4273, reward 1119.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 126\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 4274, reward 1344.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 126\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 4275, reward 1221.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 125\n",
      "Initial State is  [2, 3, 2]\n",
      "episode 4276, reward 1135.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 131\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 4277, reward 1396.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 125\n",
      "Initial State is  [4, 0, 4]\n",
      "episode 4278, reward 1182.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 128\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 4279, reward 949.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 126\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 4280, reward 1093.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 130\n",
      "Initial State is  [0, 15, 3]\n",
      "episode 4281, reward 1344.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 123\n",
      "Initial State is  [1, 21, 2]\n",
      "episode 4282, reward 1312.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 116\n",
      "Initial State is  [2, 3, 0]\n",
      "episode 4283, reward 881.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 119\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 4284, reward 1271.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 120\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 4285, reward 1024.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 114\n",
      "Initial State is  [2, 23, 1]\n",
      "episode 4286, reward 1253.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 115\n",
      "Initial State is  [3, 8, 1]\n",
      "episode 4287, reward 1346.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 122\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 4288, reward 1512.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 132\n",
      "Initial State is  [3, 15, 4]\n",
      "episode 4289, reward 1194.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 118\n",
      "Initial State is  [3, 5, 4]\n",
      "episode 4290, reward 879.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 122\n",
      "Initial State is  [4, 14, 0]\n",
      "episode 4291, reward 1241.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 128\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 4292, reward 1245.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 128\n",
      "Initial State is  [2, 7, 6]\n",
      "episode 4293, reward 1216.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 132\n",
      "Initial State is  [3, 7, 3]\n",
      "episode 4294, reward 1009.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 119\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 4295, reward 1155.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 124\n",
      "Initial State is  [3, 14, 5]\n",
      "episode 4296, reward 1124.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 123\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 4297, reward 962.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 130\n",
      "Initial State is  [3, 22, 6]\n",
      "episode 4298, reward 1235.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 126\n",
      "Initial State is  [3, 20, 4]\n",
      "episode 4299, reward 1245.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 126\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 4300, reward 1078.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 137\n",
      "Initial State is  [0, 21, 4]\n",
      "episode 4301, reward 1383.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 121\n",
      "Initial State is  [4, 4, 0]\n",
      "episode 4302, reward 1163.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 119\n",
      "Initial State is  [3, 20, 0]\n",
      "episode 4303, reward 1291.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 129\n",
      "Initial State is  [0, 20, 1]\n",
      "episode 4304, reward 1157.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 118\n",
      "Initial State is  [4, 19, 5]\n",
      "episode 4305, reward 1347.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 127\n",
      "Initial State is  [3, 19, 4]\n",
      "episode 4306, reward 1222.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 115\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 4307, reward 1305.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 125\n",
      "Initial State is  [3, 17, 1]\n",
      "episode 4308, reward 1267.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 126\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 4309, reward 1177.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 117\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 4310, reward 1546.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 124\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 4311, reward 961.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 132\n",
      "Initial State is  [0, 7, 2]\n",
      "episode 4312, reward 1202.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 118\n",
      "Initial State is  [4, 13, 4]\n",
      "episode 4313, reward 1265.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 120\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 4314, reward 1263.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 123\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 4315, reward 1390.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 121\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 4316, reward 1406.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 120\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 4317, reward 985.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 122\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 4318, reward 1053.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 128\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 4319, reward 1337.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 126\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 4320, reward 1054.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 133\n",
      "Initial State is  [4, 17, 3]\n",
      "episode 4321, reward 1410.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 120\n",
      "Initial State is  [4, 7, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4322, reward 1245.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 129\n",
      "Initial State is  [2, 5, 6]\n",
      "episode 4323, reward 1291.0, memory_length 2000, epsilon 0.0009954703940636294, time 740.0, rides 117\n",
      "Initial State is  [4, 21, 6]\n",
      "episode 4324, reward 1246.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 125\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 4325, reward 1341.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 120\n",
      "Initial State is  [1, 1, 4]\n",
      "episode 4326, reward 1345.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 129\n",
      "Initial State is  [0, 22, 3]\n",
      "episode 4327, reward 1232.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 121\n",
      "Initial State is  [3, 18, 4]\n",
      "episode 4328, reward 1219.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 119\n",
      "Initial State is  [0, 4, 4]\n",
      "episode 4329, reward 1042.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 137\n",
      "Initial State is  [4, 11, 2]\n",
      "episode 4330, reward 1085.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 118\n",
      "Initial State is  [1, 0, 6]\n",
      "episode 4331, reward 1001.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 116\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 4332, reward 1286.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 121\n",
      "Initial State is  [3, 10, 6]\n",
      "episode 4333, reward 1122.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 115\n",
      "Initial State is  [2, 8, 6]\n",
      "episode 4334, reward 950.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 124\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 4335, reward 1241.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 125\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 4336, reward 772.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 116\n",
      "Initial State is  [2, 18, 0]\n",
      "episode 4337, reward 1087.0, memory_length 2000, epsilon 0.0009954703940636294, time 742.0, rides 129\n",
      "Initial State is  [4, 21, 0]\n",
      "episode 4338, reward 1102.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 111\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 4339, reward 980.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 130\n",
      "Initial State is  [3, 20, 4]\n",
      "episode 4340, reward 1248.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 134\n",
      "Initial State is  [3, 14, 0]\n",
      "episode 4341, reward 1203.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 125\n",
      "Initial State is  [4, 10, 3]\n",
      "episode 4342, reward 1277.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 114\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 4343, reward 1225.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 123\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 4344, reward 1279.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 131\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 4345, reward 1198.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 124\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 4346, reward 1101.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 128\n",
      "Initial State is  [3, 3, 2]\n",
      "episode 4347, reward 1391.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 112\n",
      "Initial State is  [3, 7, 0]\n",
      "episode 4348, reward 1096.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 121\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 4349, reward 1061.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 133\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 4350, reward 1232.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 137\n",
      "Initial State is  [1, 17, 2]\n",
      "episode 4351, reward 1145.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 119\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 4352, reward 1186.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 116\n",
      "Initial State is  [1, 4, 3]\n",
      "episode 4353, reward 1247.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 129\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 4354, reward 1254.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 116\n",
      "Initial State is  [1, 18, 0]\n",
      "episode 4355, reward 1292.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 131\n",
      "Initial State is  [0, 1, 2]\n",
      "episode 4356, reward 1146.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 125\n",
      "Initial State is  [4, 8, 5]\n",
      "episode 4357, reward 1313.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 120\n",
      "Initial State is  [1, 5, 2]\n",
      "episode 4358, reward 1238.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 127\n",
      "Initial State is  [4, 13, 5]\n",
      "episode 4359, reward 1200.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 125\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 4360, reward 1379.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 113\n",
      "Initial State is  [3, 21, 0]\n",
      "episode 4361, reward 1312.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 122\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 4362, reward 1044.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 128\n",
      "Initial State is  [0, 2, 6]\n",
      "episode 4363, reward 967.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 117\n",
      "Initial State is  [4, 14, 5]\n",
      "episode 4364, reward 1220.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 126\n",
      "Initial State is  [0, 16, 0]\n",
      "episode 4365, reward 1168.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 131\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 4366, reward 1256.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 127\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 4367, reward 1316.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 122\n",
      "Initial State is  [1, 16, 3]\n",
      "episode 4368, reward 1052.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 121\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 4369, reward 1431.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 134\n",
      "Initial State is  [4, 15, 2]\n",
      "episode 4370, reward 1180.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 125\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 4371, reward 1446.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 131\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 4372, reward 1214.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 127\n",
      "Initial State is  [4, 5, 3]\n",
      "episode 4373, reward 1146.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 122\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 4374, reward 1382.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 124\n",
      "Initial State is  [0, 1, 5]\n",
      "episode 4375, reward 1170.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 123\n",
      "Initial State is  [0, 17, 5]\n",
      "episode 4376, reward 1354.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 135\n",
      "Initial State is  [4, 10, 0]\n",
      "episode 4377, reward 1171.0, memory_length 2000, epsilon 0.0009954703940636294, time 739.0, rides 128\n",
      "Initial State is  [0, 17, 2]\n",
      "episode 4378, reward 1429.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 128\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 4379, reward 1292.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 123\n",
      "Initial State is  [0, 13, 1]\n",
      "episode 4380, reward 1199.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 121\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 4381, reward 1115.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 121\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 4382, reward 1303.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 125\n",
      "Initial State is  [3, 23, 3]\n",
      "episode 4383, reward 1117.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 117\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 4384, reward 1426.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 122\n",
      "Initial State is  [3, 5, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4385, reward 1201.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 124\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 4386, reward 941.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 127\n",
      "Initial State is  [3, 6, 2]\n",
      "episode 4387, reward 1401.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 128\n",
      "Initial State is  [2, 19, 1]\n",
      "episode 4388, reward 1279.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 124\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 4389, reward 1180.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 114\n",
      "Initial State is  [4, 9, 5]\n",
      "episode 4390, reward 1242.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 125\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 4391, reward 899.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 123\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 4392, reward 1013.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 117\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 4393, reward 1238.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 129\n",
      "Initial State is  [2, 1, 1]\n",
      "episode 4394, reward 1116.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 122\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 4395, reward 1328.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 113\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 4396, reward 1225.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 131\n",
      "Initial State is  [1, 5, 6]\n",
      "episode 4397, reward 1428.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 136\n",
      "Initial State is  [2, 16, 0]\n",
      "episode 4398, reward 1446.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 120\n",
      "Initial State is  [2, 12, 5]\n",
      "episode 4399, reward 1202.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 126\n",
      "Initial State is  [3, 16, 5]\n",
      "episode 4400, reward 1435.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 123\n",
      "Initial State is  [4, 9, 4]\n",
      "episode 4401, reward 1376.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 126\n",
      "Initial State is  [3, 3, 6]\n",
      "episode 4402, reward 1448.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 128\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 4403, reward 1385.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 129\n",
      "Initial State is  [4, 7, 3]\n",
      "episode 4404, reward 1147.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 125\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 4405, reward 1110.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 129\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 4406, reward 1175.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 120\n",
      "Initial State is  [4, 12, 4]\n",
      "episode 4407, reward 1269.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 126\n",
      "Initial State is  [0, 4, 3]\n",
      "episode 4408, reward 1178.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 138\n",
      "Initial State is  [2, 9, 1]\n",
      "episode 4409, reward 921.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 115\n",
      "Initial State is  [0, 9, 3]\n",
      "episode 4410, reward 981.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 119\n",
      "Initial State is  [2, 0, 2]\n",
      "episode 4411, reward 1212.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 123\n",
      "Initial State is  [3, 21, 5]\n",
      "episode 4412, reward 1216.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 126\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 4413, reward 1455.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 132\n",
      "Initial State is  [4, 8, 1]\n",
      "episode 4414, reward 1175.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 125\n",
      "Initial State is  [4, 15, 2]\n",
      "episode 4415, reward 1312.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 115\n",
      "Initial State is  [1, 11, 1]\n",
      "episode 4416, reward 1261.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 125\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 4417, reward 1411.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 115\n",
      "Initial State is  [4, 13, 6]\n",
      "episode 4418, reward 1120.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 130\n",
      "Initial State is  [1, 17, 4]\n",
      "episode 4419, reward 1221.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 124\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 4420, reward 1360.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 125\n",
      "Initial State is  [4, 14, 0]\n",
      "episode 4421, reward 1636.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 131\n",
      "Initial State is  [4, 15, 6]\n",
      "episode 4422, reward 1258.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 115\n",
      "Initial State is  [4, 11, 6]\n",
      "episode 4423, reward 1115.0, memory_length 2000, epsilon 0.0009954703940636294, time 739.0, rides 121\n",
      "Initial State is  [0, 21, 4]\n",
      "episode 4424, reward 1572.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 144\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 4425, reward 870.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 123\n",
      "Initial State is  [4, 4, 3]\n",
      "episode 4426, reward 1273.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 120\n",
      "Initial State is  [1, 5, 2]\n",
      "episode 4427, reward 1337.0, memory_length 2000, epsilon 0.0009954703940636294, time 720.0, rides 124\n",
      "Initial State is  [2, 5, 1]\n",
      "episode 4428, reward 1203.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 134\n",
      "Initial State is  [0, 8, 5]\n",
      "episode 4429, reward 1150.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 123\n",
      "Initial State is  [2, 8, 6]\n",
      "episode 4430, reward 1288.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 127\n",
      "Initial State is  [2, 15, 0]\n",
      "episode 4431, reward 1441.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 124\n",
      "Initial State is  [0, 17, 2]\n",
      "episode 4432, reward 944.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 126\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 4433, reward 1096.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 116\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 4434, reward 1291.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 124\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 4435, reward 1344.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 121\n",
      "Initial State is  [0, 17, 1]\n",
      "episode 4436, reward 1371.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 128\n",
      "Initial State is  [4, 15, 5]\n",
      "episode 4437, reward 1172.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 126\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 4438, reward 1098.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 121\n",
      "Initial State is  [4, 5, 2]\n",
      "episode 4439, reward 1032.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 120\n",
      "Initial State is  [2, 6, 4]\n",
      "episode 4440, reward 1108.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 123\n",
      "Initial State is  [1, 7, 6]\n",
      "episode 4441, reward 887.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 126\n",
      "Initial State is  [3, 7, 5]\n",
      "episode 4442, reward 864.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 116\n",
      "Initial State is  [3, 4, 2]\n",
      "episode 4443, reward 1190.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 132\n",
      "Initial State is  [3, 18, 4]\n",
      "episode 4444, reward 1344.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 122\n",
      "Initial State is  [0, 1, 1]\n",
      "episode 4445, reward 1310.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 120\n",
      "Initial State is  [4, 22, 5]\n",
      "episode 4446, reward 1162.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 120\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 4447, reward 1473.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 125\n",
      "Initial State is  [0, 22, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4448, reward 949.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 128\n",
      "Initial State is  [0, 2, 4]\n",
      "episode 4449, reward 1240.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 121\n",
      "Initial State is  [2, 15, 0]\n",
      "episode 4450, reward 1021.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 123\n",
      "Initial State is  [1, 0, 0]\n",
      "episode 4451, reward 1120.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 119\n",
      "Initial State is  [4, 14, 4]\n",
      "episode 4452, reward 1295.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 124\n",
      "Initial State is  [0, 22, 1]\n",
      "episode 4453, reward 1296.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 115\n",
      "Initial State is  [4, 4, 1]\n",
      "episode 4454, reward 1496.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 129\n",
      "Initial State is  [3, 13, 3]\n",
      "episode 4455, reward 1052.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 130\n",
      "Initial State is  [2, 16, 4]\n",
      "episode 4456, reward 1081.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 122\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 4457, reward 1289.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 125\n",
      "Initial State is  [1, 12, 2]\n",
      "episode 4458, reward 1119.0, memory_length 2000, epsilon 0.0009954703940636294, time 740.0, rides 120\n",
      "Initial State is  [3, 18, 6]\n",
      "episode 4459, reward 1275.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 128\n",
      "Initial State is  [0, 6, 1]\n",
      "episode 4460, reward 1285.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 117\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 4461, reward 1126.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 116\n",
      "Initial State is  [2, 18, 5]\n",
      "episode 4462, reward 1190.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 125\n",
      "Initial State is  [1, 15, 0]\n",
      "episode 4463, reward 1521.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 136\n",
      "Initial State is  [0, 20, 0]\n",
      "episode 4464, reward 1283.0, memory_length 2000, epsilon 0.0009954703940636294, time 742.0, rides 120\n",
      "Initial State is  [0, 2, 0]\n",
      "episode 4465, reward 1273.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 120\n",
      "Initial State is  [4, 8, 2]\n",
      "episode 4466, reward 1437.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 134\n",
      "Initial State is  [3, 4, 2]\n",
      "episode 4467, reward 1218.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 128\n",
      "Initial State is  [1, 15, 3]\n",
      "episode 4468, reward 1014.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 131\n",
      "Initial State is  [0, 18, 4]\n",
      "episode 4469, reward 1038.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 117\n",
      "Initial State is  [4, 17, 3]\n",
      "episode 4470, reward 1180.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 137\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 4471, reward 1131.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 122\n",
      "Initial State is  [0, 23, 5]\n",
      "episode 4472, reward 1229.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 116\n",
      "Initial State is  [0, 18, 0]\n",
      "episode 4473, reward 1345.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 131\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 4474, reward 1242.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 124\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 4475, reward 1154.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 127\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 4476, reward 1046.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 140\n",
      "Initial State is  [0, 6, 3]\n",
      "episode 4477, reward 1268.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 129\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 4478, reward 1469.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 118\n",
      "Initial State is  [3, 4, 3]\n",
      "episode 4479, reward 1060.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 133\n",
      "Initial State is  [4, 22, 5]\n",
      "episode 4480, reward 1334.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 115\n",
      "Initial State is  [3, 7, 6]\n",
      "episode 4481, reward 1021.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 120\n",
      "Initial State is  [1, 19, 5]\n",
      "episode 4482, reward 1167.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 133\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 4483, reward 1250.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 141\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 4484, reward 1266.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 131\n",
      "Initial State is  [4, 14, 6]\n",
      "episode 4485, reward 1090.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 128\n",
      "Initial State is  [1, 4, 1]\n",
      "episode 4486, reward 1296.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 117\n",
      "Initial State is  [1, 12, 6]\n",
      "episode 4487, reward 1315.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 124\n",
      "Initial State is  [3, 6, 2]\n",
      "episode 4488, reward 1206.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 136\n",
      "Initial State is  [0, 7, 2]\n",
      "episode 4489, reward 1446.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 127\n",
      "Initial State is  [1, 11, 4]\n",
      "episode 4490, reward 1105.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 123\n",
      "Initial State is  [3, 4, 0]\n",
      "episode 4491, reward 1009.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 118\n",
      "Initial State is  [1, 23, 5]\n",
      "episode 4492, reward 1195.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 112\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 4493, reward 1471.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 119\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 4494, reward 1507.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 125\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 4495, reward 1080.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 126\n",
      "Initial State is  [1, 23, 2]\n",
      "episode 4496, reward 1428.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 119\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 4497, reward 1405.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 134\n",
      "Initial State is  [4, 10, 4]\n",
      "episode 4498, reward 1097.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 124\n",
      "Initial State is  [4, 14, 0]\n",
      "episode 4499, reward 1330.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 124\n",
      "Initial State is  [2, 0, 1]\n",
      "episode 4500, reward 1200.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 124\n",
      "Initial State is  [2, 15, 6]\n",
      "episode 4501, reward 1034.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 115\n",
      "Initial State is  [3, 16, 2]\n",
      "episode 4502, reward 1361.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 120\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 4503, reward 1273.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 117\n",
      "Initial State is  [1, 22, 2]\n",
      "episode 4504, reward 1208.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 120\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 4505, reward 1154.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 120\n",
      "Initial State is  [3, 12, 1]\n",
      "episode 4506, reward 970.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 123\n",
      "Initial State is  [1, 18, 4]\n",
      "episode 4507, reward 1184.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 143\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 4508, reward 995.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 117\n",
      "Initial State is  [0, 14, 1]\n",
      "episode 4509, reward 956.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 114\n",
      "Initial State is  [1, 17, 0]\n",
      "episode 4510, reward 1310.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 123\n",
      "Initial State is  [4, 14, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4511, reward 1191.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 122\n",
      "Initial State is  [4, 10, 5]\n",
      "episode 4512, reward 1296.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 124\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 4513, reward 1131.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 132\n",
      "Initial State is  [4, 15, 6]\n",
      "episode 4514, reward 1292.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 110\n",
      "Initial State is  [4, 21, 1]\n",
      "episode 4515, reward 1153.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 118\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 4516, reward 995.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 121\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 4517, reward 1270.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 123\n",
      "Initial State is  [3, 19, 6]\n",
      "episode 4518, reward 1208.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 130\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 4519, reward 1372.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 118\n",
      "Initial State is  [4, 2, 3]\n",
      "episode 4520, reward 1311.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 120\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 4521, reward 1325.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 126\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 4522, reward 1331.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 117\n",
      "Initial State is  [2, 0, 4]\n",
      "episode 4523, reward 1178.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 117\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 4524, reward 1430.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 128\n",
      "Initial State is  [3, 3, 0]\n",
      "episode 4525, reward 1465.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 128\n",
      "Initial State is  [4, 17, 5]\n",
      "episode 4526, reward 1364.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 135\n",
      "Initial State is  [1, 10, 4]\n",
      "episode 4527, reward 1307.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 114\n",
      "Initial State is  [2, 15, 5]\n",
      "episode 4528, reward 1410.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 126\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 4529, reward 1120.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 123\n",
      "Initial State is  [3, 11, 1]\n",
      "episode 4530, reward 967.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 120\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 4531, reward 1110.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 120\n",
      "Initial State is  [2, 12, 2]\n",
      "episode 4532, reward 1114.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 122\n",
      "Initial State is  [3, 18, 6]\n",
      "episode 4533, reward 1273.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 116\n",
      "Initial State is  [1, 12, 0]\n",
      "episode 4534, reward 1010.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 122\n",
      "Initial State is  [3, 6, 2]\n",
      "episode 4535, reward 1357.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 119\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 4536, reward 1277.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 116\n",
      "Initial State is  [4, 18, 2]\n",
      "episode 4537, reward 983.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 112\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 4538, reward 1304.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 129\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 4539, reward 1056.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 122\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 4540, reward 1197.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 126\n",
      "Initial State is  [1, 10, 6]\n",
      "episode 4541, reward 1251.0, memory_length 2000, epsilon 0.0009954703940636294, time 739.0, rides 116\n",
      "Initial State is  [0, 3, 2]\n",
      "episode 4542, reward 1119.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 120\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 4543, reward 1234.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 124\n",
      "Initial State is  [3, 2, 2]\n",
      "episode 4544, reward 1041.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 123\n",
      "Initial State is  [2, 9, 0]\n",
      "episode 4545, reward 1257.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 120\n",
      "Initial State is  [1, 13, 4]\n",
      "episode 4546, reward 1057.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 120\n",
      "Initial State is  [2, 16, 2]\n",
      "episode 4547, reward 935.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 112\n",
      "Initial State is  [3, 11, 0]\n",
      "episode 4548, reward 1383.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 132\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 4549, reward 1248.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 129\n",
      "Initial State is  [4, 18, 2]\n",
      "episode 4550, reward 1141.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 130\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 4551, reward 1134.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 120\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 4552, reward 1337.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 128\n",
      "Initial State is  [4, 3, 3]\n",
      "episode 4553, reward 1234.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 126\n",
      "Initial State is  [4, 20, 6]\n",
      "episode 4554, reward 1459.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 125\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 4555, reward 1304.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 121\n",
      "Initial State is  [4, 16, 1]\n",
      "episode 4556, reward 1234.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 126\n",
      "Initial State is  [0, 17, 2]\n",
      "episode 4557, reward 1247.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 130\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 4558, reward 1292.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 121\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 4559, reward 1113.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 119\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 4560, reward 1103.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 117\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 4561, reward 1381.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 126\n",
      "Initial State is  [1, 8, 2]\n",
      "episode 4562, reward 1229.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 129\n",
      "Initial State is  [0, 2, 6]\n",
      "episode 4563, reward 1261.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 126\n",
      "Initial State is  [4, 8, 4]\n",
      "episode 4564, reward 1426.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 126\n",
      "Initial State is  [2, 9, 1]\n",
      "episode 4565, reward 1592.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 121\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 4566, reward 1219.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 123\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 4567, reward 1181.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 134\n",
      "Initial State is  [1, 1, 2]\n",
      "episode 4568, reward 1175.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 132\n",
      "Initial State is  [2, 15, 1]\n",
      "episode 4569, reward 1280.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 112\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 4570, reward 1259.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 116\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 4571, reward 1256.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 130\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 4572, reward 1168.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 127\n",
      "Initial State is  [3, 17, 1]\n",
      "episode 4573, reward 1195.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 126\n",
      "Initial State is  [1, 6, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4574, reward 1202.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 129\n",
      "Initial State is  [3, 11, 0]\n",
      "episode 4575, reward 1265.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 130\n",
      "Initial State is  [2, 15, 6]\n",
      "episode 4576, reward 1254.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 142\n",
      "Initial State is  [3, 7, 0]\n",
      "episode 4577, reward 1057.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 109\n",
      "Initial State is  [1, 21, 4]\n",
      "episode 4578, reward 1216.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 127\n",
      "Initial State is  [1, 3, 6]\n",
      "episode 4579, reward 1446.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 132\n",
      "Initial State is  [3, 7, 5]\n",
      "episode 4580, reward 1241.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 133\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 4581, reward 1409.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 127\n",
      "Initial State is  [3, 18, 6]\n",
      "episode 4582, reward 1390.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 126\n",
      "Initial State is  [1, 1, 3]\n",
      "episode 4583, reward 1022.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 114\n",
      "Initial State is  [0, 17, 1]\n",
      "episode 4584, reward 1271.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 126\n",
      "Initial State is  [3, 17, 4]\n",
      "episode 4585, reward 1204.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 126\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 4586, reward 1300.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 128\n",
      "Initial State is  [1, 6, 1]\n",
      "episode 4587, reward 1240.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 120\n",
      "Initial State is  [1, 22, 2]\n",
      "episode 4588, reward 1196.0, memory_length 2000, epsilon 0.0009954703940636294, time 740.0, rides 131\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 4589, reward 1271.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 125\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 4590, reward 1327.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 120\n",
      "Initial State is  [2, 11, 0]\n",
      "episode 4591, reward 1301.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 130\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 4592, reward 1171.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 116\n",
      "Initial State is  [4, 13, 1]\n",
      "episode 4593, reward 1363.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 129\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 4594, reward 1281.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 121\n",
      "Initial State is  [0, 3, 6]\n",
      "episode 4595, reward 1479.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 124\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 4596, reward 1303.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 129\n",
      "Initial State is  [0, 17, 5]\n",
      "episode 4597, reward 1379.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 126\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 4598, reward 1150.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 117\n",
      "Initial State is  [3, 20, 1]\n",
      "episode 4599, reward 1056.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 120\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 4600, reward 1289.0, memory_length 2000, epsilon 0.0009954703940636294, time 739.0, rides 121\n",
      "Initial State is  [1, 15, 4]\n",
      "episode 4601, reward 1133.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 121\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 4602, reward 1101.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 113\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 4603, reward 1223.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 136\n",
      "Initial State is  [3, 8, 6]\n",
      "episode 4604, reward 1237.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 125\n",
      "Initial State is  [0, 23, 5]\n",
      "episode 4605, reward 1368.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 127\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 4606, reward 1357.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 130\n",
      "Initial State is  [3, 20, 1]\n",
      "episode 4607, reward 1116.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 124\n",
      "Initial State is  [0, 19, 0]\n",
      "episode 4608, reward 1159.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 126\n",
      "Initial State is  [4, 0, 5]\n",
      "episode 4609, reward 1343.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 126\n",
      "Initial State is  [0, 13, 3]\n",
      "episode 4610, reward 1161.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 123\n",
      "Initial State is  [2, 22, 2]\n",
      "episode 4611, reward 1176.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 131\n",
      "Initial State is  [3, 21, 2]\n",
      "episode 4612, reward 1256.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 131\n",
      "Initial State is  [4, 16, 5]\n",
      "episode 4613, reward 1105.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 117\n",
      "Initial State is  [4, 22, 2]\n",
      "episode 4614, reward 973.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 116\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 4615, reward 1246.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 129\n",
      "Initial State is  [1, 15, 3]\n",
      "episode 4616, reward 1197.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 129\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 4617, reward 1300.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 123\n",
      "Initial State is  [1, 21, 3]\n",
      "episode 4618, reward 1574.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 119\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 4619, reward 1414.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 122\n",
      "Initial State is  [0, 20, 3]\n",
      "episode 4620, reward 1186.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 125\n",
      "Initial State is  [3, 6, 2]\n",
      "episode 4621, reward 1202.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 120\n",
      "Initial State is  [1, 16, 3]\n",
      "episode 4622, reward 1179.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 116\n",
      "Initial State is  [2, 2, 5]\n",
      "episode 4623, reward 1192.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 130\n",
      "Initial State is  [1, 12, 2]\n",
      "episode 4624, reward 1118.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 120\n",
      "Initial State is  [4, 19, 5]\n",
      "episode 4625, reward 1293.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 123\n",
      "Initial State is  [2, 0, 3]\n",
      "episode 4626, reward 1132.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 135\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 4627, reward 1232.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 121\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 4628, reward 1097.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 115\n",
      "Initial State is  [2, 0, 3]\n",
      "episode 4629, reward 794.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 118\n",
      "Initial State is  [1, 1, 6]\n",
      "episode 4630, reward 1210.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 132\n",
      "Initial State is  [2, 16, 6]\n",
      "episode 4631, reward 1436.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 123\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 4632, reward 1206.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 126\n",
      "Initial State is  [0, 6, 2]\n",
      "episode 4633, reward 1251.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 120\n",
      "Initial State is  [3, 23, 6]\n",
      "episode 4634, reward 1173.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 125\n",
      "Initial State is  [3, 22, 1]\n",
      "episode 4635, reward 1202.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 120\n",
      "Initial State is  [0, 2, 1]\n",
      "episode 4636, reward 915.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 117\n",
      "Initial State is  [4, 16, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4637, reward 1093.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 125\n",
      "Initial State is  [4, 18, 2]\n",
      "episode 4638, reward 1325.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 119\n",
      "Initial State is  [2, 8, 1]\n",
      "episode 4639, reward 1355.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 123\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 4640, reward 1257.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 122\n",
      "Initial State is  [2, 18, 3]\n",
      "episode 4641, reward 1245.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 128\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 4642, reward 1347.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 126\n",
      "Initial State is  [2, 15, 5]\n",
      "episode 4643, reward 1102.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 126\n",
      "Initial State is  [4, 21, 4]\n",
      "episode 4644, reward 1243.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 126\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 4645, reward 1076.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 116\n",
      "Initial State is  [3, 12, 3]\n",
      "episode 4646, reward 1098.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 129\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 4647, reward 1597.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 129\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 4648, reward 1253.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 128\n",
      "Initial State is  [1, 20, 6]\n",
      "episode 4649, reward 1611.0, memory_length 2000, epsilon 0.0009954703940636294, time 739.0, rides 124\n",
      "Initial State is  [3, 23, 6]\n",
      "episode 4650, reward 1385.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 119\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 4651, reward 1426.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 125\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 4652, reward 1279.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 120\n",
      "Initial State is  [3, 17, 4]\n",
      "episode 4653, reward 1233.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 128\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 4654, reward 1436.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 115\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 4655, reward 1328.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 124\n",
      "Initial State is  [0, 12, 5]\n",
      "episode 4656, reward 1304.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 127\n",
      "Initial State is  [4, 16, 2]\n",
      "episode 4657, reward 1102.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 125\n",
      "Initial State is  [3, 22, 4]\n",
      "episode 4658, reward 1619.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 132\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 4659, reward 1313.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 139\n",
      "Initial State is  [0, 22, 6]\n",
      "episode 4660, reward 1304.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 122\n",
      "Initial State is  [3, 8, 6]\n",
      "episode 4661, reward 1279.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 125\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 4662, reward 1398.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 121\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 4663, reward 1211.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 131\n",
      "Initial State is  [1, 16, 1]\n",
      "episode 4664, reward 1105.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 135\n",
      "Initial State is  [2, 12, 6]\n",
      "episode 4665, reward 1341.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 125\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 4666, reward 1088.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 127\n",
      "Initial State is  [4, 16, 6]\n",
      "episode 4667, reward 1354.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 130\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 4668, reward 1258.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 133\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 4669, reward 1179.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 127\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 4670, reward 1150.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 118\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 4671, reward 1145.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 122\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 4672, reward 1108.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 129\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 4673, reward 1264.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 120\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 4674, reward 1437.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 129\n",
      "Initial State is  [3, 1, 2]\n",
      "episode 4675, reward 1153.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 120\n",
      "Initial State is  [2, 14, 1]\n",
      "episode 4676, reward 1227.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 126\n",
      "Initial State is  [1, 8, 6]\n",
      "episode 4677, reward 994.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 118\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 4678, reward 1292.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 121\n",
      "Initial State is  [3, 22, 1]\n",
      "episode 4679, reward 1151.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 131\n",
      "Initial State is  [3, 9, 3]\n",
      "episode 4680, reward 1300.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 123\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 4681, reward 1149.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 125\n",
      "Initial State is  [3, 21, 5]\n",
      "episode 4682, reward 1426.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 121\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 4683, reward 1333.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 133\n",
      "Initial State is  [0, 12, 1]\n",
      "episode 4684, reward 1257.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 123\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 4685, reward 1171.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 122\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 4686, reward 1309.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 121\n",
      "Initial State is  [0, 8, 1]\n",
      "episode 4687, reward 1225.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 130\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 4688, reward 1077.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 125\n",
      "Initial State is  [4, 3, 0]\n",
      "episode 4689, reward 1314.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 124\n",
      "Initial State is  [4, 8, 2]\n",
      "episode 4690, reward 1478.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 128\n",
      "Initial State is  [1, 1, 1]\n",
      "episode 4691, reward 1153.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 117\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 4692, reward 1239.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 118\n",
      "Initial State is  [4, 0, 5]\n",
      "episode 4693, reward 1027.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 122\n",
      "Initial State is  [0, 8, 4]\n",
      "episode 4694, reward 1093.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 124\n",
      "Initial State is  [3, 0, 5]\n",
      "episode 4695, reward 1090.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 130\n",
      "Initial State is  [1, 21, 4]\n",
      "episode 4696, reward 1373.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 123\n",
      "Initial State is  [2, 15, 6]\n",
      "episode 4697, reward 1237.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 136\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 4698, reward 960.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 119\n",
      "Initial State is  [3, 16, 0]\n",
      "episode 4699, reward 1080.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 120\n",
      "Initial State is  [0, 14, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4700, reward 916.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 124\n",
      "Initial State is  [3, 11, 5]\n",
      "episode 4701, reward 1031.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 120\n",
      "Initial State is  [0, 22, 2]\n",
      "episode 4702, reward 1310.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 118\n",
      "Initial State is  [4, 4, 6]\n",
      "episode 4703, reward 1380.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 124\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 4704, reward 1206.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 130\n",
      "Initial State is  [3, 19, 2]\n",
      "episode 4705, reward 1061.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 115\n",
      "Initial State is  [1, 14, 2]\n",
      "episode 4706, reward 1273.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 128\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 4707, reward 1370.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 124\n",
      "Initial State is  [2, 21, 4]\n",
      "episode 4708, reward 1125.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 124\n",
      "Initial State is  [3, 14, 2]\n",
      "episode 4709, reward 1303.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 116\n",
      "Initial State is  [4, 6, 2]\n",
      "episode 4710, reward 1255.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 127\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 4711, reward 1294.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 124\n",
      "Initial State is  [2, 22, 1]\n",
      "episode 4712, reward 1066.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 116\n",
      "Initial State is  [3, 12, 4]\n",
      "episode 4713, reward 894.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 123\n",
      "Initial State is  [1, 8, 3]\n",
      "episode 4714, reward 1257.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 110\n",
      "Initial State is  [2, 18, 1]\n",
      "episode 4715, reward 1243.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 132\n",
      "Initial State is  [3, 4, 4]\n",
      "episode 4716, reward 1008.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 132\n",
      "Initial State is  [4, 20, 6]\n",
      "episode 4717, reward 1067.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 120\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 4718, reward 1467.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 113\n",
      "Initial State is  [0, 14, 5]\n",
      "episode 4719, reward 1209.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 128\n",
      "Initial State is  [1, 12, 6]\n",
      "episode 4720, reward 1214.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 117\n",
      "Initial State is  [0, 0, 0]\n",
      "episode 4721, reward 1085.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 129\n",
      "Initial State is  [2, 5, 5]\n",
      "episode 4722, reward 1010.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 110\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 4723, reward 1017.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 118\n",
      "Initial State is  [2, 4, 3]\n",
      "episode 4724, reward 1250.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 118\n",
      "Initial State is  [4, 0, 5]\n",
      "episode 4725, reward 1205.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 126\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 4726, reward 1203.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 111\n",
      "Initial State is  [2, 3, 1]\n",
      "episode 4727, reward 1131.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 124\n",
      "Initial State is  [1, 0, 3]\n",
      "episode 4728, reward 1248.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 124\n",
      "Initial State is  [1, 21, 3]\n",
      "episode 4729, reward 1213.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 124\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 4730, reward 1253.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 129\n",
      "Initial State is  [2, 13, 5]\n",
      "episode 4731, reward 1244.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 117\n",
      "Initial State is  [4, 10, 5]\n",
      "episode 4732, reward 1099.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 124\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 4733, reward 1261.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 119\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 4734, reward 1614.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 128\n",
      "Initial State is  [4, 11, 4]\n",
      "episode 4735, reward 1208.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 131\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 4736, reward 1036.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 121\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 4737, reward 1235.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 137\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 4738, reward 1161.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 125\n",
      "Initial State is  [2, 16, 2]\n",
      "episode 4739, reward 1213.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 119\n",
      "Initial State is  [0, 20, 1]\n",
      "episode 4740, reward 1247.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 118\n",
      "Initial State is  [3, 9, 3]\n",
      "episode 4741, reward 1226.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 122\n",
      "Initial State is  [4, 20, 3]\n",
      "episode 4742, reward 1206.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 117\n",
      "Initial State is  [1, 12, 2]\n",
      "episode 4743, reward 1177.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 127\n",
      "Initial State is  [3, 0, 0]\n",
      "episode 4744, reward 1253.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 126\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 4745, reward 1305.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 131\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 4746, reward 1175.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 120\n",
      "Initial State is  [3, 3, 5]\n",
      "episode 4747, reward 1153.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 113\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 4748, reward 1186.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 126\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 4749, reward 1014.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 132\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 4750, reward 1135.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 122\n",
      "Initial State is  [3, 20, 2]\n",
      "episode 4751, reward 1011.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 127\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 4752, reward 1320.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 128\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 4753, reward 1212.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 117\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 4754, reward 1694.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 122\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 4755, reward 758.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 111\n",
      "Initial State is  [0, 7, 4]\n",
      "episode 4756, reward 1130.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 135\n",
      "Initial State is  [3, 6, 0]\n",
      "episode 4757, reward 1446.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 128\n",
      "Initial State is  [1, 1, 3]\n",
      "episode 4758, reward 1166.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 127\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 4759, reward 1415.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 116\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 4760, reward 1019.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 134\n",
      "Initial State is  [2, 15, 0]\n",
      "episode 4761, reward 1236.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 123\n",
      "Initial State is  [1, 17, 0]\n",
      "episode 4762, reward 1350.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 113\n",
      "Initial State is  [2, 19, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4763, reward 1335.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 129\n",
      "Initial State is  [0, 0, 5]\n",
      "episode 4764, reward 1253.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 120\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 4765, reward 1734.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 122\n",
      "Initial State is  [3, 9, 0]\n",
      "episode 4766, reward 1041.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 125\n",
      "Initial State is  [2, 14, 0]\n",
      "episode 4767, reward 1184.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 120\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 4768, reward 1134.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 119\n",
      "Initial State is  [0, 16, 2]\n",
      "episode 4769, reward 1365.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 120\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 4770, reward 1178.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 120\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 4771, reward 1276.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 128\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 4772, reward 1269.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 114\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 4773, reward 938.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 123\n",
      "Initial State is  [4, 11, 5]\n",
      "episode 4774, reward 1276.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 140\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 4775, reward 1434.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 126\n",
      "Initial State is  [1, 21, 1]\n",
      "episode 4776, reward 1055.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 120\n",
      "Initial State is  [1, 13, 4]\n",
      "episode 4777, reward 1142.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 111\n",
      "Initial State is  [3, 3, 0]\n",
      "episode 4778, reward 1069.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 124\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 4779, reward 1283.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 128\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 4780, reward 943.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 118\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 4781, reward 1329.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 124\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 4782, reward 1083.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 119\n",
      "Initial State is  [2, 20, 3]\n",
      "episode 4783, reward 1280.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 130\n",
      "Initial State is  [2, 6, 6]\n",
      "episode 4784, reward 1074.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 127\n",
      "Initial State is  [2, 11, 5]\n",
      "episode 4785, reward 998.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 119\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 4786, reward 1381.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 118\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 4787, reward 1124.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 123\n",
      "Initial State is  [4, 22, 5]\n",
      "episode 4788, reward 1211.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 123\n",
      "Initial State is  [3, 23, 5]\n",
      "episode 4789, reward 1308.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 127\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 4790, reward 1192.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 132\n",
      "Initial State is  [1, 5, 6]\n",
      "episode 4791, reward 1311.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 118\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 4792, reward 1440.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 123\n",
      "Initial State is  [4, 20, 6]\n",
      "episode 4793, reward 1105.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 128\n",
      "Initial State is  [2, 7, 5]\n",
      "episode 4794, reward 1053.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 113\n",
      "Initial State is  [0, 8, 1]\n",
      "episode 4795, reward 930.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 124\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 4796, reward 1051.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 114\n",
      "Initial State is  [3, 12, 4]\n",
      "episode 4797, reward 1315.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 130\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 4798, reward 1278.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 126\n",
      "Initial State is  [0, 8, 1]\n",
      "episode 4799, reward 1070.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 123\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 4800, reward 1323.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 122\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 4801, reward 1414.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 123\n",
      "Initial State is  [3, 6, 5]\n",
      "episode 4802, reward 1150.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 126\n",
      "Initial State is  [4, 15, 0]\n",
      "episode 4803, reward 1151.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 120\n",
      "Initial State is  [1, 4, 5]\n",
      "episode 4804, reward 971.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 120\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 4805, reward 1187.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 117\n",
      "Initial State is  [0, 22, 3]\n",
      "episode 4806, reward 1380.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 132\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 4807, reward 1177.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 128\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 4808, reward 1102.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 126\n",
      "Initial State is  [2, 15, 2]\n",
      "episode 4809, reward 1326.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 132\n",
      "Initial State is  [1, 1, 0]\n",
      "episode 4810, reward 1125.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 122\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 4811, reward 1508.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 126\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 4812, reward 1290.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 123\n",
      "Initial State is  [3, 17, 3]\n",
      "episode 4813, reward 1230.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 118\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 4814, reward 1140.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 126\n",
      "Initial State is  [1, 12, 0]\n",
      "episode 4815, reward 1240.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 118\n",
      "Initial State is  [1, 10, 6]\n",
      "episode 4816, reward 1375.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 122\n",
      "Initial State is  [4, 9, 0]\n",
      "episode 4817, reward 1213.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 133\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 4818, reward 1246.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 116\n",
      "Initial State is  [1, 9, 4]\n",
      "episode 4819, reward 1367.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 126\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 4820, reward 1439.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 125\n",
      "Initial State is  [0, 0, 1]\n",
      "episode 4821, reward 1375.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 125\n",
      "Initial State is  [2, 10, 4]\n",
      "episode 4822, reward 1448.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 121\n",
      "Initial State is  [4, 15, 0]\n",
      "episode 4823, reward 1121.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 118\n",
      "Initial State is  [4, 23, 2]\n",
      "episode 4824, reward 1358.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 123\n",
      "Initial State is  [2, 22, 2]\n",
      "episode 4825, reward 1189.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 120\n",
      "Initial State is  [3, 8, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4826, reward 999.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 115\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 4827, reward 1164.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 129\n",
      "Initial State is  [1, 23, 6]\n",
      "episode 4828, reward 1407.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 130\n",
      "Initial State is  [3, 4, 0]\n",
      "episode 4829, reward 1334.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 131\n",
      "Initial State is  [0, 5, 2]\n",
      "episode 4830, reward 1088.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 121\n",
      "Initial State is  [1, 5, 2]\n",
      "episode 4831, reward 1220.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 127\n",
      "Initial State is  [4, 15, 0]\n",
      "episode 4832, reward 1218.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 126\n",
      "Initial State is  [4, 8, 2]\n",
      "episode 4833, reward 1289.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 132\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 4834, reward 1153.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 130\n",
      "Initial State is  [1, 5, 6]\n",
      "episode 4835, reward 1268.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 119\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 4836, reward 1527.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 130\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 4837, reward 1210.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 129\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 4838, reward 1030.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 121\n",
      "Initial State is  [0, 20, 1]\n",
      "episode 4839, reward 1056.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 129\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 4840, reward 1389.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 125\n",
      "Initial State is  [1, 4, 1]\n",
      "episode 4841, reward 1167.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 121\n",
      "Initial State is  [1, 9, 4]\n",
      "episode 4842, reward 1407.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 128\n",
      "Initial State is  [1, 3, 2]\n",
      "episode 4843, reward 1060.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 125\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 4844, reward 1484.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 128\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 4845, reward 1231.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 126\n",
      "Initial State is  [2, 10, 4]\n",
      "episode 4846, reward 869.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 119\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 4847, reward 1393.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 132\n",
      "Initial State is  [1, 0, 3]\n",
      "episode 4848, reward 1256.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 123\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 4849, reward 1087.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 120\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 4850, reward 1541.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 123\n",
      "Initial State is  [4, 2, 6]\n",
      "episode 4851, reward 1449.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 124\n",
      "Initial State is  [0, 2, 6]\n",
      "episode 4852, reward 1271.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 134\n",
      "Initial State is  [1, 2, 3]\n",
      "episode 4853, reward 1583.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 131\n",
      "Initial State is  [0, 5, 3]\n",
      "episode 4854, reward 1202.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 123\n",
      "Initial State is  [1, 8, 2]\n",
      "episode 4855, reward 1109.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 117\n",
      "Initial State is  [2, 1, 0]\n",
      "episode 4856, reward 1133.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 123\n",
      "Initial State is  [1, 22, 1]\n",
      "episode 4857, reward 1257.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 128\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 4858, reward 1481.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 132\n",
      "Initial State is  [2, 6, 1]\n",
      "episode 4859, reward 1194.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 132\n",
      "Initial State is  [4, 1, 6]\n",
      "episode 4860, reward 1141.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 120\n",
      "Initial State is  [1, 11, 4]\n",
      "episode 4861, reward 1024.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 118\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 4862, reward 1344.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 140\n",
      "Initial State is  [4, 2, 4]\n",
      "episode 4863, reward 1066.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 109\n",
      "Initial State is  [2, 12, 2]\n",
      "episode 4864, reward 1266.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 128\n",
      "Initial State is  [4, 9, 4]\n",
      "episode 4865, reward 1217.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 121\n",
      "Initial State is  [3, 14, 2]\n",
      "episode 4866, reward 1162.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 121\n",
      "Initial State is  [2, 14, 2]\n",
      "episode 4867, reward 1163.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 137\n",
      "Initial State is  [1, 0, 0]\n",
      "episode 4868, reward 1395.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 116\n",
      "Initial State is  [4, 19, 6]\n",
      "episode 4869, reward 1137.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 123\n",
      "Initial State is  [4, 8, 0]\n",
      "episode 4870, reward 1123.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 128\n",
      "Initial State is  [1, 14, 6]\n",
      "episode 4871, reward 1028.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 123\n",
      "Initial State is  [3, 7, 0]\n",
      "episode 4872, reward 1383.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 119\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 4873, reward 1565.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 116\n",
      "Initial State is  [3, 6, 5]\n",
      "episode 4874, reward 1312.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 116\n",
      "Initial State is  [2, 0, 3]\n",
      "episode 4875, reward 1113.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 113\n",
      "Initial State is  [3, 2, 4]\n",
      "episode 4876, reward 1057.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 114\n",
      "Initial State is  [1, 13, 6]\n",
      "episode 4877, reward 1232.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 136\n",
      "Initial State is  [4, 7, 1]\n",
      "episode 4878, reward 1352.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 131\n",
      "Initial State is  [1, 5, 5]\n",
      "episode 4879, reward 1397.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 139\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 4880, reward 1311.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 116\n",
      "Initial State is  [2, 12, 4]\n",
      "episode 4881, reward 1167.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 113\n",
      "Initial State is  [2, 14, 0]\n",
      "episode 4882, reward 1398.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 117\n",
      "Initial State is  [2, 11, 2]\n",
      "episode 4883, reward 1273.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 132\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 4884, reward 1551.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 125\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 4885, reward 1120.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 118\n",
      "Initial State is  [4, 12, 1]\n",
      "episode 4886, reward 1026.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 122\n",
      "Initial State is  [4, 12, 2]\n",
      "episode 4887, reward 982.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 129\n",
      "Initial State is  [2, 7, 6]\n",
      "episode 4888, reward 1048.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 126\n",
      "Initial State is  [3, 21, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4889, reward 1269.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 119\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 4890, reward 1329.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 122\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 4891, reward 1396.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 118\n",
      "Initial State is  [2, 20, 4]\n",
      "episode 4892, reward 1213.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 129\n",
      "Initial State is  [3, 7, 2]\n",
      "episode 4893, reward 1349.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 131\n",
      "Initial State is  [4, 12, 0]\n",
      "episode 4894, reward 1264.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 125\n",
      "Initial State is  [3, 9, 6]\n",
      "episode 4895, reward 1156.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 141\n",
      "Initial State is  [2, 2, 2]\n",
      "episode 4896, reward 966.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 135\n",
      "Initial State is  [2, 17, 3]\n",
      "episode 4897, reward 1146.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 118\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 4898, reward 1252.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 118\n",
      "Initial State is  [4, 3, 0]\n",
      "episode 4899, reward 1258.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 118\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 4900, reward 1403.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 123\n",
      "Initial State is  [1, 9, 0]\n",
      "episode 4901, reward 1109.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 131\n",
      "Initial State is  [3, 12, 6]\n",
      "episode 4902, reward 1532.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 132\n",
      "Initial State is  [4, 12, 2]\n",
      "episode 4903, reward 1097.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 116\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 4904, reward 1445.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 128\n",
      "Initial State is  [0, 13, 3]\n",
      "episode 4905, reward 1173.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 128\n",
      "Initial State is  [4, 10, 0]\n",
      "episode 4906, reward 1154.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 124\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 4907, reward 1291.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 122\n",
      "Initial State is  [4, 8, 1]\n",
      "episode 4908, reward 1246.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 139\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 4909, reward 1166.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 129\n",
      "Initial State is  [0, 2, 5]\n",
      "episode 4910, reward 1123.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 113\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 4911, reward 1475.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 128\n",
      "Initial State is  [3, 23, 1]\n",
      "episode 4912, reward 1317.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 122\n",
      "Initial State is  [2, 16, 5]\n",
      "episode 4913, reward 1315.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 113\n",
      "Initial State is  [2, 7, 5]\n",
      "episode 4914, reward 1310.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 125\n",
      "Initial State is  [0, 22, 2]\n",
      "episode 4915, reward 1254.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 142\n",
      "Initial State is  [4, 18, 4]\n",
      "episode 4916, reward 1558.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 126\n",
      "Initial State is  [3, 9, 1]\n",
      "episode 4917, reward 1166.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 119\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 4918, reward 1395.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 134\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 4919, reward 1182.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 119\n",
      "Initial State is  [3, 19, 3]\n",
      "episode 4920, reward 1214.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 124\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 4921, reward 1026.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 112\n",
      "Initial State is  [0, 7, 4]\n",
      "episode 4922, reward 858.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 119\n",
      "Initial State is  [2, 16, 5]\n",
      "episode 4923, reward 1252.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 128\n",
      "Initial State is  [4, 11, 6]\n",
      "episode 4924, reward 1209.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 128\n",
      "Initial State is  [2, 23, 2]\n",
      "episode 4925, reward 1278.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 116\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 4926, reward 1313.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 127\n",
      "Initial State is  [1, 2, 0]\n",
      "episode 4927, reward 1216.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 128\n",
      "Initial State is  [2, 1, 1]\n",
      "episode 4928, reward 1107.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 121\n",
      "Initial State is  [3, 16, 1]\n",
      "episode 4929, reward 1350.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 130\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 4930, reward 1237.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 117\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 4931, reward 1190.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 128\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 4932, reward 1058.0, memory_length 2000, epsilon 0.0009954703940636294, time 739.0, rides 114\n",
      "Initial State is  [3, 11, 3]\n",
      "episode 4933, reward 1389.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 118\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 4934, reward 1364.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 135\n",
      "Initial State is  [4, 15, 4]\n",
      "episode 4935, reward 1388.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 126\n",
      "Initial State is  [4, 6, 2]\n",
      "episode 4936, reward 1304.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 137\n",
      "Initial State is  [4, 8, 2]\n",
      "episode 4937, reward 1290.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 124\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 4938, reward 1310.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 125\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 4939, reward 1339.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 130\n",
      "Initial State is  [0, 21, 5]\n",
      "episode 4940, reward 1112.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 123\n",
      "Initial State is  [0, 23, 2]\n",
      "episode 4941, reward 1391.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 131\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 4942, reward 1366.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 129\n",
      "Initial State is  [2, 12, 2]\n",
      "episode 4943, reward 1253.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 117\n",
      "Initial State is  [4, 3, 3]\n",
      "episode 4944, reward 1293.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 132\n",
      "Initial State is  [3, 23, 5]\n",
      "episode 4945, reward 1281.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 122\n",
      "Initial State is  [4, 15, 4]\n",
      "episode 4946, reward 1356.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 127\n",
      "Initial State is  [2, 12, 6]\n",
      "episode 4947, reward 1295.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 127\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 4948, reward 1035.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 123\n",
      "Initial State is  [1, 13, 3]\n",
      "episode 4949, reward 1467.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 127\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 4950, reward 1227.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 125\n",
      "Initial State is  [0, 16, 3]\n",
      "episode 4951, reward 1447.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 135\n",
      "Initial State is  [0, 22, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4952, reward 1123.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 122\n",
      "Initial State is  [3, 9, 5]\n",
      "episode 4953, reward 1362.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 130\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 4954, reward 1289.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 124\n",
      "Initial State is  [2, 2, 5]\n",
      "episode 4955, reward 1494.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 133\n",
      "Initial State is  [1, 22, 4]\n",
      "episode 4956, reward 999.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 125\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 4957, reward 1355.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 125\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 4958, reward 1272.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 134\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 4959, reward 1398.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 130\n",
      "Initial State is  [2, 20, 1]\n",
      "episode 4960, reward 1126.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 124\n",
      "Initial State is  [4, 22, 1]\n",
      "episode 4961, reward 1135.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 128\n",
      "Initial State is  [3, 12, 6]\n",
      "episode 4962, reward 1081.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 125\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 4963, reward 1045.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 129\n",
      "Initial State is  [3, 17, 6]\n",
      "episode 4964, reward 905.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 143\n",
      "Initial State is  [1, 12, 2]\n",
      "episode 4965, reward 1067.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 146\n",
      "Initial State is  [3, 14, 0]\n",
      "episode 4966, reward 964.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 123\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 4967, reward 1291.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 134\n",
      "Initial State is  [2, 4, 1]\n",
      "episode 4968, reward 1200.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 128\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 4969, reward 1292.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 128\n",
      "Initial State is  [2, 6, 1]\n",
      "episode 4970, reward 1157.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 129\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 4971, reward 1369.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 117\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 4972, reward 1170.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 117\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 4973, reward 1546.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 129\n",
      "Initial State is  [3, 3, 3]\n",
      "episode 4974, reward 1127.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 126\n",
      "Initial State is  [0, 21, 5]\n",
      "episode 4975, reward 1277.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 117\n",
      "Initial State is  [1, 23, 6]\n",
      "episode 4976, reward 1070.0, memory_length 2000, epsilon 0.0009954703940636294, time 739.0, rides 122\n",
      "Initial State is  [3, 16, 2]\n",
      "episode 4977, reward 1586.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 119\n",
      "Initial State is  [4, 5, 6]\n",
      "episode 4978, reward 1191.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 124\n",
      "Initial State is  [4, 13, 1]\n",
      "episode 4979, reward 1101.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 122\n",
      "Initial State is  [4, 14, 0]\n",
      "episode 4980, reward 1342.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 119\n",
      "Initial State is  [4, 15, 5]\n",
      "episode 4981, reward 1298.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 129\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 4982, reward 1216.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 118\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 4983, reward 1184.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 116\n",
      "Initial State is  [4, 5, 0]\n",
      "episode 4984, reward 1277.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 128\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 4985, reward 1410.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 128\n",
      "Initial State is  [0, 18, 4]\n",
      "episode 4986, reward 1152.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 112\n",
      "Initial State is  [4, 21, 0]\n",
      "episode 4987, reward 1277.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 138\n",
      "Initial State is  [4, 19, 5]\n",
      "episode 4988, reward 1261.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 135\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 4989, reward 1191.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 122\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 4990, reward 1021.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 126\n",
      "Initial State is  [2, 3, 6]\n",
      "episode 4991, reward 1386.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 128\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 4992, reward 1454.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 118\n",
      "Initial State is  [1, 3, 3]\n",
      "episode 4993, reward 1234.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 120\n",
      "Initial State is  [4, 21, 2]\n",
      "episode 4994, reward 1153.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 130\n",
      "Initial State is  [4, 21, 4]\n",
      "episode 4995, reward 1098.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 122\n",
      "Initial State is  [2, 11, 2]\n",
      "episode 4996, reward 1439.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 123\n",
      "Initial State is  [3, 10, 5]\n",
      "episode 4997, reward 1239.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 129\n",
      "Initial State is  [4, 22, 5]\n",
      "episode 4998, reward 1404.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 116\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 4999, reward 1329.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 116\n"
     ]
    }
   ],
   "source": [
    "#Under Test\n",
    "agent = DQNAgent(36,21)\n",
    "rewards_per_episode, episodes = [], []\n",
    "\n",
    "for episode in range(Episodes):\n",
    "\n",
    "    # Write code here\n",
    "    # Call the environment\n",
    "    env = CabDriver()\n",
    "    # Call all the initialised variables of the environment\n",
    "    state_space = env.state_space\n",
    "    action_space = env.action_space\n",
    "    state = env.state_init\n",
    "    print(\"Initial State is \",state)\n",
    "    time = 0\n",
    "    #Call the DQN agent\n",
    "    terminal_state = False\n",
    "    score = 0\n",
    "    action = agent.get_action(env.state_encod_arch1(state),env)\n",
    "    score += env.reward_func(state,action_space[action],Time_matrix)\n",
    "    next_state,ride_time = env.next_state_func(state,action_space[action],Time_matrix)\n",
    "    time += ride_time\n",
    "    if time >= 24*30:\n",
    "        agent.append_sample(env.state_encod_arch1(state),action,score,env.state_encod_arch1(next_state),True)\n",
    "    else:\n",
    "        agent.append_sample(env.state_encod_arch1(state),action,score,env.state_encod_arch1(next_state),False)\n",
    "    loop = 0\n",
    "    while not terminal_state:\n",
    "        \n",
    "        # Write your code here\n",
    "        \n",
    "        if time >= 24*30:\n",
    "            terminal_state = True\n",
    "            pass\n",
    "        state = next_state\n",
    "        # 1. Pick epsilon-greedy action from possible actions for the current state\n",
    "        action = agent.get_action(env.state_encod_arch1(state),env)\n",
    "        # 2. Evaluate your reward and next state\n",
    "        reward_curr_ride = env.reward_func(state,action_space[action],Time_matrix)\n",
    "        score+= reward_curr_ride\n",
    "        next_state,ride_time = env.next_state_func(next_state,action_space[action],Time_matrix)\n",
    "        time += ride_time\n",
    "        # 3. Append the experience to the memory\n",
    "        if time >= 24*30:\n",
    "            agent.append_sample(env.state_encod_arch1(state),action,reward_curr_ride,env.state_encod_arch1(next_state),True)\n",
    "        else:\n",
    "            agent.append_sample(env.state_encod_arch1(state),action,reward_curr_ride,env.state_encod_arch1(next_state),False)\n",
    "        # 4. Train the model by calling function agent.train_model\n",
    "        agent.train_model(env)\n",
    "        #print('Time elapsed {} and current loop {}'.format(time,loop))\n",
    "        loop+= 1\n",
    "        # 5. Keep a track of rewards, Q-values, loss\n",
    "    \n",
    "    rewards_per_episode.append(score)   \n",
    "    episodes.append(episode)\n",
    "    \n",
    "    if agent.epsilon > agent.epsilon_min:\n",
    "        agent.epsilon *= agent.epsilon_decay\n",
    "# every episode:\n",
    "    print(\"episode {0}, reward {1}, memory_length {2}, epsilon {3}, time {4}, rides {5}\".format(episode,\n",
    "                                                                         score,\n",
    "                                                                         len(agent.memory),\n",
    "                                                                         agent.epsilon,time,loop))\n",
    "    # every few episodes:\n",
    "    if episode % 1000 == 0:\n",
    "        # store q-values of some prespecified state-action pairs\n",
    "        # q_dict = agent.store_q_values()\n",
    "\n",
    "        # save model weights\n",
    "        agent.save(name=\"model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State is  [4, 14, 4]\n",
      "episode 0, reward 55.0, memory_length 136, epsilon 0.995, time 729.0, rides 135\n",
      "Initial State is  [1, 22, 4]\n",
      "episode 1, reward -220.0, memory_length 239, epsilon 0.990025, time 724.0, rides 102\n",
      "Initial State is  [2, 4, 3]\n",
      "episode 2, reward -172.0, memory_length 361, epsilon 0.985074875, time 738.0, rides 121\n",
      "Initial State is  [2, 20, 2]\n",
      "episode 3, reward -146.0, memory_length 490, epsilon 0.9801495006250001, time 729.0, rides 128\n",
      "Initial State is  [3, 11, 1]\n",
      "episode 4, reward -173.0, memory_length 610, epsilon 0.9752487531218751, time 721.0, rides 119\n",
      "Initial State is  [1, 6, 4]\n",
      "episode 5, reward -389.0, memory_length 732, epsilon 0.9703725093562657, time 734.0, rides 121\n",
      "Initial State is  [0, 21, 1]\n",
      "episode 6, reward -75.0, memory_length 858, epsilon 0.9655206468094844, time 735.0, rides 125\n",
      "Initial State is  [2, 16, 3]\n",
      "episode 7, reward -263.0, memory_length 990, epsilon 0.960693043575437, time 734.0, rides 131\n",
      "Initial State is  [4, 3, 5]\n",
      "episode 8, reward -188.0, memory_length 1121, epsilon 0.9558895783575597, time 729.0, rides 130\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 9, reward -253.0, memory_length 1248, epsilon 0.9511101304657719, time 731.0, rides 126\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 10, reward 6.0, memory_length 1381, epsilon 0.946354579813443, time 724.0, rides 132\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 11, reward -458.0, memory_length 1500, epsilon 0.9416228069143757, time 730.0, rides 118\n",
      "Initial State is  [3, 3, 2]\n",
      "episode 12, reward -66.0, memory_length 1624, epsilon 0.9369146928798039, time 724.0, rides 123\n",
      "Initial State is  [4, 7, 0]\n",
      "episode 13, reward -299.0, memory_length 1749, epsilon 0.9322301194154049, time 721.0, rides 124\n",
      "Initial State is  [4, 17, 5]\n",
      "episode 14, reward -156.0, memory_length 1882, epsilon 0.9275689688183278, time 732.0, rides 132\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 15, reward -96.0, memory_length 1998, epsilon 0.9229311239742362, time 727.0, rides 115\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 16, reward -287.0, memory_length 2000, epsilon 0.918316468354365, time 721.0, rides 127\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 17, reward -77.0, memory_length 2000, epsilon 0.9137248860125932, time 730.0, rides 123\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 18, reward -35.0, memory_length 2000, epsilon 0.9091562615825302, time 726.0, rides 111\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 19, reward -244.0, memory_length 2000, epsilon 0.9046104802746175, time 730.0, rides 128\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 20, reward -137.0, memory_length 2000, epsilon 0.9000874278732445, time 735.0, rides 130\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 21, reward -297.0, memory_length 2000, epsilon 0.8955869907338783, time 727.0, rides 121\n",
      "Initial State is  [2, 22, 2]\n",
      "episode 22, reward -99.0, memory_length 2000, epsilon 0.8911090557802088, time 726.0, rides 118\n",
      "Initial State is  [4, 0, 5]\n",
      "episode 23, reward -38.0, memory_length 2000, epsilon 0.8866535105013078, time 727.0, rides 129\n",
      "Initial State is  [1, 21, 5]\n",
      "episode 24, reward -81.0, memory_length 2000, epsilon 0.8822202429488013, time 728.0, rides 120\n",
      "Initial State is  [3, 3, 2]\n",
      "episode 25, reward -100.0, memory_length 2000, epsilon 0.8778091417340573, time 726.0, rides 125\n",
      "Initial State is  [1, 8, 2]\n",
      "episode 26, reward -185.0, memory_length 2000, epsilon 0.8734200960253871, time 733.0, rides 118\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 27, reward -82.0, memory_length 2000, epsilon 0.8690529955452602, time 729.0, rides 126\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 28, reward -87.0, memory_length 2000, epsilon 0.8647077305675338, time 727.0, rides 130\n",
      "Initial State is  [1, 4, 4]\n",
      "episode 29, reward 167.0, memory_length 2000, epsilon 0.8603841919146962, time 733.0, rides 120\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 30, reward 89.0, memory_length 2000, epsilon 0.8560822709551227, time 727.0, rides 127\n",
      "Initial State is  [3, 19, 6]\n",
      "episode 31, reward -5.0, memory_length 2000, epsilon 0.851801859600347, time 725.0, rides 117\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 32, reward -164.0, memory_length 2000, epsilon 0.8475428503023453, time 725.0, rides 130\n",
      "Initial State is  [3, 7, 6]\n",
      "episode 33, reward -91.0, memory_length 2000, epsilon 0.8433051360508336, time 734.0, rides 124\n",
      "Initial State is  [4, 5, 5]\n",
      "episode 34, reward -236.0, memory_length 2000, epsilon 0.8390886103705794, time 737.0, rides 120\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 35, reward 212.0, memory_length 2000, epsilon 0.8348931673187264, time 728.0, rides 112\n",
      "Initial State is  [1, 18, 4]\n",
      "episode 36, reward 27.0, memory_length 2000, epsilon 0.8307187014821328, time 731.0, rides 136\n",
      "Initial State is  [3, 21, 3]\n",
      "episode 37, reward 103.0, memory_length 2000, epsilon 0.8265651079747222, time 727.0, rides 131\n",
      "Initial State is  [4, 13, 1]\n",
      "episode 38, reward 50.0, memory_length 2000, epsilon 0.8224322824348486, time 727.0, rides 133\n",
      "Initial State is  [4, 18, 5]\n",
      "episode 39, reward -207.0, memory_length 2000, epsilon 0.8183201210226743, time 729.0, rides 128\n",
      "Initial State is  [4, 23, 0]\n",
      "episode 40, reward -147.0, memory_length 2000, epsilon 0.8142285204175609, time 728.0, rides 120\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 41, reward -8.0, memory_length 2000, epsilon 0.810157377815473, time 735.0, rides 123\n",
      "Initial State is  [1, 11, 5]\n",
      "episode 42, reward 24.0, memory_length 2000, epsilon 0.8061065909263957, time 732.0, rides 129\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 43, reward -80.0, memory_length 2000, epsilon 0.8020760579717637, time 734.0, rides 140\n",
      "Initial State is  [0, 11, 2]\n",
      "episode 44, reward 35.0, memory_length 2000, epsilon 0.798065677681905, time 732.0, rides 113\n",
      "Initial State is  [4, 18, 6]\n",
      "episode 45, reward 37.0, memory_length 2000, epsilon 0.7940753492934954, time 733.0, rides 125\n",
      "Initial State is  [3, 8, 1]\n",
      "episode 46, reward -134.0, memory_length 2000, epsilon 0.7901049725470279, time 736.0, rides 130\n",
      "Initial State is  [3, 20, 1]\n",
      "episode 47, reward -96.0, memory_length 2000, epsilon 0.7861544476842928, time 735.0, rides 118\n",
      "Initial State is  [2, 18, 5]\n",
      "episode 48, reward -293.0, memory_length 2000, epsilon 0.7822236754458713, time 729.0, rides 139\n",
      "Initial State is  [1, 4, 3]\n",
      "episode 49, reward -9.0, memory_length 2000, epsilon 0.778312557068642, time 737.0, rides 120\n",
      "Initial State is  [3, 5, 4]\n",
      "episode 50, reward 160.0, memory_length 2000, epsilon 0.7744209942832988, time 736.0, rides 115\n",
      "Initial State is  [2, 20, 2]\n",
      "episode 51, reward 284.0, memory_length 2000, epsilon 0.7705488893118823, time 724.0, rides 129\n",
      "Initial State is  [0, 3, 6]\n",
      "episode 52, reward -4.0, memory_length 2000, epsilon 0.7666961448653229, time 729.0, rides 129\n",
      "Initial State is  [2, 1, 0]\n",
      "episode 53, reward -169.0, memory_length 2000, epsilon 0.7628626641409962, time 726.0, rides 124\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 54, reward -32.0, memory_length 2000, epsilon 0.7590483508202912, time 727.0, rides 114\n",
      "Initial State is  [3, 7, 0]\n",
      "episode 55, reward 108.0, memory_length 2000, epsilon 0.7552531090661897, time 721.0, rides 131\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 56, reward 38.0, memory_length 2000, epsilon 0.7514768435208588, time 726.0, rides 133\n",
      "Initial State is  [1, 17, 4]\n",
      "episode 57, reward 27.0, memory_length 2000, epsilon 0.7477194593032545, time 734.0, rides 117\n",
      "Initial State is  [1, 14, 4]\n",
      "episode 58, reward 34.0, memory_length 2000, epsilon 0.7439808620067382, time 729.0, rides 136\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 59, reward -230.0, memory_length 2000, epsilon 0.7402609576967045, time 732.0, rides 133\n",
      "Initial State is  [0, 8, 2]\n",
      "episode 60, reward 10.0, memory_length 2000, epsilon 0.736559652908221, time 730.0, rides 127\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 61, reward -33.0, memory_length 2000, epsilon 0.7328768546436799, time 727.0, rides 127\n",
      "Initial State is  [1, 11, 6]\n",
      "episode 62, reward -80.0, memory_length 2000, epsilon 0.7292124703704616, time 728.0, rides 133\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 63, reward 124.0, memory_length 2000, epsilon 0.7255664080186093, time 737.0, rides 128\n",
      "Initial State is  [1, 11, 0]\n",
      "episode 64, reward 0.0, memory_length 2000, epsilon 0.7219385759785162, time 735.0, rides 126\n",
      "Initial State is  [1, 21, 6]\n",
      "episode 65, reward -63.0, memory_length 2000, epsilon 0.7183288830986236, time 723.0, rides 134\n",
      "Initial State is  [2, 8, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 66, reward 108.0, memory_length 2000, epsilon 0.7147372386831305, time 729.0, rides 127\n",
      "Initial State is  [0, 3, 3]\n",
      "episode 67, reward 301.0, memory_length 2000, epsilon 0.7111635524897149, time 728.0, rides 116\n",
      "Initial State is  [4, 18, 5]\n",
      "episode 68, reward 188.0, memory_length 2000, epsilon 0.7076077347272662, time 728.0, rides 140\n",
      "Initial State is  [1, 11, 1]\n",
      "episode 69, reward 46.0, memory_length 2000, epsilon 0.7040696960536299, time 729.0, rides 124\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 70, reward 61.0, memory_length 2000, epsilon 0.7005493475733617, time 725.0, rides 121\n",
      "Initial State is  [4, 14, 6]\n",
      "episode 71, reward 129.0, memory_length 2000, epsilon 0.697046600835495, time 734.0, rides 137\n",
      "Initial State is  [2, 15, 5]\n",
      "episode 72, reward 215.0, memory_length 2000, epsilon 0.6935613678313175, time 732.0, rides 120\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 73, reward 49.0, memory_length 2000, epsilon 0.6900935609921609, time 736.0, rides 129\n",
      "Initial State is  [2, 4, 4]\n",
      "episode 74, reward 56.0, memory_length 2000, epsilon 0.6866430931872001, time 731.0, rides 129\n",
      "Initial State is  [2, 21, 0]\n",
      "episode 75, reward 206.0, memory_length 2000, epsilon 0.6832098777212641, time 729.0, rides 133\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 76, reward 296.0, memory_length 2000, epsilon 0.6797938283326578, time 733.0, rides 128\n",
      "Initial State is  [3, 9, 2]\n",
      "episode 77, reward -56.0, memory_length 2000, epsilon 0.6763948591909945, time 725.0, rides 130\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 78, reward 224.0, memory_length 2000, epsilon 0.6730128848950395, time 728.0, rides 128\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 79, reward 87.0, memory_length 2000, epsilon 0.6696478204705644, time 727.0, rides 123\n",
      "Initial State is  [2, 15, 6]\n",
      "episode 80, reward 86.0, memory_length 2000, epsilon 0.6662995813682115, time 734.0, rides 111\n",
      "Initial State is  [4, 14, 1]\n",
      "episode 81, reward -58.0, memory_length 2000, epsilon 0.6629680834613705, time 728.0, rides 123\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 82, reward 15.0, memory_length 2000, epsilon 0.6596532430440636, time 731.0, rides 128\n",
      "Initial State is  [1, 5, 6]\n",
      "episode 83, reward 81.0, memory_length 2000, epsilon 0.6563549768288433, time 724.0, rides 131\n",
      "Initial State is  [2, 23, 0]\n",
      "episode 84, reward 95.0, memory_length 2000, epsilon 0.653073201944699, time 739.0, rides 135\n",
      "Initial State is  [0, 4, 3]\n",
      "episode 85, reward -142.0, memory_length 2000, epsilon 0.6498078359349755, time 740.0, rides 137\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 86, reward 307.0, memory_length 2000, epsilon 0.6465587967553006, time 728.0, rides 126\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 87, reward 172.0, memory_length 2000, epsilon 0.6433260027715241, time 721.0, rides 126\n",
      "Initial State is  [0, 4, 3]\n",
      "episode 88, reward 331.0, memory_length 2000, epsilon 0.6401093727576664, time 734.0, rides 143\n",
      "Initial State is  [2, 19, 1]\n",
      "episode 89, reward 240.0, memory_length 2000, epsilon 0.6369088258938781, time 737.0, rides 116\n",
      "Initial State is  [0, 7, 2]\n",
      "episode 90, reward -26.0, memory_length 2000, epsilon 0.6337242817644086, time 731.0, rides 124\n",
      "Initial State is  [3, 14, 5]\n",
      "episode 91, reward 301.0, memory_length 2000, epsilon 0.6305556603555866, time 722.0, rides 127\n",
      "Initial State is  [1, 2, 3]\n",
      "episode 92, reward 264.0, memory_length 2000, epsilon 0.6274028820538087, time 726.0, rides 124\n",
      "Initial State is  [4, 14, 6]\n",
      "episode 93, reward 168.0, memory_length 2000, epsilon 0.6242658676435396, time 730.0, rides 112\n",
      "Initial State is  [0, 12, 1]\n",
      "episode 94, reward 16.0, memory_length 2000, epsilon 0.6211445383053219, time 726.0, rides 124\n",
      "Initial State is  [1, 2, 0]\n",
      "episode 95, reward 106.0, memory_length 2000, epsilon 0.6180388156137953, time 730.0, rides 125\n",
      "Initial State is  [2, 4, 1]\n",
      "episode 96, reward 231.0, memory_length 2000, epsilon 0.6149486215357263, time 723.0, rides 117\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 97, reward 344.0, memory_length 2000, epsilon 0.6118738784280476, time 739.0, rides 118\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 98, reward -37.0, memory_length 2000, epsilon 0.6088145090359074, time 724.0, rides 125\n",
      "Initial State is  [3, 10, 2]\n",
      "episode 99, reward 263.0, memory_length 2000, epsilon 0.6057704364907278, time 739.0, rides 114\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 100, reward 96.0, memory_length 2000, epsilon 0.6027415843082742, time 726.0, rides 113\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 101, reward 502.0, memory_length 2000, epsilon 0.5997278763867329, time 727.0, rides 137\n",
      "Initial State is  [4, 2, 2]\n",
      "episode 102, reward 544.0, memory_length 2000, epsilon 0.5967292370047992, time 724.0, rides 136\n",
      "Initial State is  [4, 15, 4]\n",
      "episode 103, reward -116.0, memory_length 2000, epsilon 0.5937455908197752, time 728.0, rides 119\n",
      "Initial State is  [4, 19, 6]\n",
      "episode 104, reward 122.0, memory_length 2000, epsilon 0.5907768628656763, time 736.0, rides 119\n",
      "Initial State is  [2, 19, 1]\n",
      "episode 105, reward 184.0, memory_length 2000, epsilon 0.5878229785513479, time 728.0, rides 120\n",
      "Initial State is  [4, 22, 1]\n",
      "episode 106, reward 228.0, memory_length 2000, epsilon 0.5848838636585911, time 734.0, rides 117\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 107, reward 451.0, memory_length 2000, epsilon 0.5819594443402982, time 723.0, rides 125\n",
      "Initial State is  [4, 11, 2]\n",
      "episode 108, reward 80.0, memory_length 2000, epsilon 0.5790496471185967, time 721.0, rides 115\n",
      "Initial State is  [1, 18, 4]\n",
      "episode 109, reward 326.0, memory_length 2000, epsilon 0.5761543988830038, time 741.0, rides 125\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 110, reward 553.0, memory_length 2000, epsilon 0.5732736268885887, time 733.0, rides 120\n",
      "Initial State is  [3, 22, 3]\n",
      "episode 111, reward 19.0, memory_length 2000, epsilon 0.5704072587541458, time 731.0, rides 118\n",
      "Initial State is  [2, 9, 3]\n",
      "episode 112, reward 88.0, memory_length 2000, epsilon 0.567555222460375, time 732.0, rides 128\n",
      "Initial State is  [1, 6, 1]\n",
      "episode 113, reward 152.0, memory_length 2000, epsilon 0.5647174463480732, time 737.0, rides 135\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 114, reward 51.0, memory_length 2000, epsilon 0.5618938591163328, time 725.0, rides 124\n",
      "Initial State is  [3, 9, 3]\n",
      "episode 115, reward 268.0, memory_length 2000, epsilon 0.5590843898207511, time 722.0, rides 131\n",
      "Initial State is  [2, 20, 0]\n",
      "episode 116, reward 175.0, memory_length 2000, epsilon 0.5562889678716474, time 727.0, rides 121\n",
      "Initial State is  [3, 7, 0]\n",
      "episode 117, reward 244.0, memory_length 2000, epsilon 0.5535075230322891, time 730.0, rides 131\n",
      "Initial State is  [3, 10, 1]\n",
      "episode 118, reward 343.0, memory_length 2000, epsilon 0.5507399854171277, time 734.0, rides 119\n",
      "Initial State is  [4, 11, 5]\n",
      "episode 119, reward 300.0, memory_length 2000, epsilon 0.547986285490042, time 721.0, rides 131\n",
      "Initial State is  [2, 3, 5]\n",
      "episode 120, reward 215.0, memory_length 2000, epsilon 0.5452463540625918, time 732.0, rides 108\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 121, reward 292.0, memory_length 2000, epsilon 0.5425201222922789, time 729.0, rides 125\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 122, reward 175.0, memory_length 2000, epsilon 0.5398075216808175, time 724.0, rides 126\n",
      "Initial State is  [3, 20, 4]\n",
      "episode 123, reward 383.0, memory_length 2000, epsilon 0.5371084840724134, time 725.0, rides 134\n",
      "Initial State is  [1, 21, 0]\n",
      "episode 124, reward 377.0, memory_length 2000, epsilon 0.5344229416520513, time 733.0, rides 128\n",
      "Initial State is  [0, 2, 1]\n",
      "episode 125, reward 455.0, memory_length 2000, epsilon 0.531750826943791, time 729.0, rides 132\n",
      "Initial State is  [3, 7, 5]\n",
      "episode 126, reward 469.0, memory_length 2000, epsilon 0.5290920728090721, time 732.0, rides 131\n",
      "Initial State is  [3, 5, 0]\n",
      "episode 127, reward 256.0, memory_length 2000, epsilon 0.5264466124450268, time 730.0, rides 130\n",
      "Initial State is  [2, 16, 1]\n",
      "episode 128, reward 342.0, memory_length 2000, epsilon 0.5238143793828016, time 725.0, rides 133\n",
      "Initial State is  [3, 5, 2]\n",
      "episode 129, reward 312.0, memory_length 2000, epsilon 0.5211953074858876, time 728.0, rides 114\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 130, reward 156.0, memory_length 2000, epsilon 0.5185893309484582, time 725.0, rides 134\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 131, reward 200.0, memory_length 2000, epsilon 0.5159963842937159, time 731.0, rides 126\n",
      "Initial State is  [0, 22, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 132, reward 433.0, memory_length 2000, epsilon 0.5134164023722473, time 730.0, rides 126\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 133, reward 226.0, memory_length 2000, epsilon 0.510849320360386, time 741.0, rides 130\n",
      "Initial State is  [4, 19, 6]\n",
      "episode 134, reward 482.0, memory_length 2000, epsilon 0.5082950737585841, time 733.0, rides 125\n",
      "Initial State is  [3, 7, 6]\n",
      "episode 135, reward 431.0, memory_length 2000, epsilon 0.5057535983897912, time 726.0, rides 132\n",
      "Initial State is  [0, 1, 5]\n",
      "episode 136, reward 672.0, memory_length 2000, epsilon 0.5032248303978422, time 726.0, rides 133\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 137, reward 569.0, memory_length 2000, epsilon 0.500708706245853, time 727.0, rides 123\n",
      "Initial State is  [4, 11, 2]\n",
      "episode 138, reward 459.0, memory_length 2000, epsilon 0.4982051627146237, time 737.0, rides 121\n",
      "Initial State is  [1, 10, 4]\n",
      "episode 139, reward 322.0, memory_length 2000, epsilon 0.49571413690105054, time 728.0, rides 124\n",
      "Initial State is  [3, 0, 5]\n",
      "episode 140, reward 215.0, memory_length 2000, epsilon 0.4932355662165453, time 739.0, rides 123\n",
      "Initial State is  [3, 15, 6]\n",
      "episode 141, reward 551.0, memory_length 2000, epsilon 0.4907693883854626, time 726.0, rides 131\n",
      "Initial State is  [2, 18, 3]\n",
      "episode 142, reward 276.0, memory_length 2000, epsilon 0.4883155414435353, time 725.0, rides 122\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 143, reward 489.0, memory_length 2000, epsilon 0.4858739637363176, time 723.0, rides 125\n",
      "Initial State is  [2, 15, 6]\n",
      "episode 144, reward 539.0, memory_length 2000, epsilon 0.483444593917636, time 725.0, rides 142\n",
      "Initial State is  [2, 13, 1]\n",
      "episode 145, reward 184.0, memory_length 2000, epsilon 0.4810273709480478, time 729.0, rides 129\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 146, reward 720.0, memory_length 2000, epsilon 0.47862223409330756, time 721.0, rides 126\n",
      "Initial State is  [3, 4, 2]\n",
      "episode 147, reward 111.0, memory_length 2000, epsilon 0.47622912292284103, time 732.0, rides 135\n",
      "Initial State is  [0, 6, 3]\n",
      "episode 148, reward 436.0, memory_length 2000, epsilon 0.4738479773082268, time 726.0, rides 130\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 149, reward 394.0, memory_length 2000, epsilon 0.47147873742168567, time 731.0, rides 125\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 150, reward 436.0, memory_length 2000, epsilon 0.46912134373457726, time 724.0, rides 117\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 151, reward 708.0, memory_length 2000, epsilon 0.46677573701590436, time 729.0, rides 122\n",
      "Initial State is  [1, 14, 4]\n",
      "episode 152, reward 266.0, memory_length 2000, epsilon 0.46444185833082485, time 729.0, rides 127\n",
      "Initial State is  [0, 10, 2]\n",
      "episode 153, reward 446.0, memory_length 2000, epsilon 0.46211964903917074, time 723.0, rides 129\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 154, reward 373.0, memory_length 2000, epsilon 0.4598090507939749, time 733.0, rides 126\n",
      "Initial State is  [1, 21, 4]\n",
      "episode 155, reward 697.0, memory_length 2000, epsilon 0.457510005540005, time 730.0, rides 129\n",
      "Initial State is  [4, 13, 3]\n",
      "episode 156, reward 192.0, memory_length 2000, epsilon 0.45522245551230495, time 729.0, rides 130\n",
      "Initial State is  [0, 12, 6]\n",
      "episode 157, reward 670.0, memory_length 2000, epsilon 0.4529463432347434, time 735.0, rides 110\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 158, reward 644.0, memory_length 2000, epsilon 0.4506816115185697, time 727.0, rides 116\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 159, reward 481.0, memory_length 2000, epsilon 0.4484282034609769, time 724.0, rides 124\n",
      "Initial State is  [4, 20, 3]\n",
      "episode 160, reward 360.0, memory_length 2000, epsilon 0.446186062443672, time 730.0, rides 126\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 161, reward 503.0, memory_length 2000, epsilon 0.4439551321314536, time 726.0, rides 147\n",
      "Initial State is  [3, 9, 3]\n",
      "episode 162, reward 532.0, memory_length 2000, epsilon 0.4417353564707963, time 734.0, rides 123\n",
      "Initial State is  [2, 3, 5]\n",
      "episode 163, reward 509.0, memory_length 2000, epsilon 0.43952667968844233, time 732.0, rides 110\n",
      "Initial State is  [2, 4, 4]\n",
      "episode 164, reward 804.0, memory_length 2000, epsilon 0.43732904629000013, time 725.0, rides 117\n",
      "Initial State is  [1, 18, 0]\n",
      "episode 165, reward 483.0, memory_length 2000, epsilon 0.4351424010585501, time 735.0, rides 118\n",
      "Initial State is  [0, 6, 3]\n",
      "episode 166, reward 739.0, memory_length 2000, epsilon 0.43296668905325736, time 729.0, rides 121\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 167, reward 374.0, memory_length 2000, epsilon 0.43080185560799106, time 723.0, rides 122\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 168, reward 253.0, memory_length 2000, epsilon 0.4286478463299511, time 728.0, rides 107\n",
      "Initial State is  [4, 7, 2]\n",
      "episode 169, reward 485.0, memory_length 2000, epsilon 0.42650460709830135, time 733.0, rides 120\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 170, reward 563.0, memory_length 2000, epsilon 0.42437208406280985, time 740.0, rides 115\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 171, reward 247.0, memory_length 2000, epsilon 0.4222502236424958, time 734.0, rides 129\n",
      "Initial State is  [4, 14, 6]\n",
      "episode 172, reward 479.0, memory_length 2000, epsilon 0.42013897252428334, time 732.0, rides 139\n",
      "Initial State is  [0, 21, 4]\n",
      "episode 173, reward 283.0, memory_length 2000, epsilon 0.4180382776616619, time 725.0, rides 126\n",
      "Initial State is  [4, 0, 0]\n",
      "episode 174, reward 548.0, memory_length 2000, epsilon 0.4159480862733536, time 727.0, rides 135\n",
      "Initial State is  [2, 2, 0]\n",
      "episode 175, reward 92.0, memory_length 2000, epsilon 0.41386834584198684, time 727.0, rides 124\n",
      "Initial State is  [4, 19, 6]\n",
      "episode 176, reward 408.0, memory_length 2000, epsilon 0.4117990041127769, time 739.0, rides 124\n",
      "Initial State is  [3, 14, 0]\n",
      "episode 177, reward 455.0, memory_length 2000, epsilon 0.40974000909221303, time 739.0, rides 115\n",
      "Initial State is  [1, 20, 5]\n",
      "episode 178, reward 434.0, memory_length 2000, epsilon 0.40769130904675194, time 730.0, rides 125\n",
      "Initial State is  [3, 17, 4]\n",
      "episode 179, reward 718.0, memory_length 2000, epsilon 0.40565285250151817, time 731.0, rides 120\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 180, reward 378.0, memory_length 2000, epsilon 0.4036245882390106, time 730.0, rides 119\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 181, reward 586.0, memory_length 2000, epsilon 0.4016064652978155, time 729.0, rides 122\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 182, reward 567.0, memory_length 2000, epsilon 0.3995984329713264, time 732.0, rides 127\n",
      "Initial State is  [1, 16, 4]\n",
      "episode 183, reward 513.0, memory_length 2000, epsilon 0.3976004408064698, time 725.0, rides 118\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 184, reward 617.0, memory_length 2000, epsilon 0.39561243860243744, time 737.0, rides 126\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 185, reward 392.0, memory_length 2000, epsilon 0.3936343764094253, time 726.0, rides 122\n",
      "Initial State is  [4, 21, 0]\n",
      "episode 186, reward 529.0, memory_length 2000, epsilon 0.39166620452737816, time 731.0, rides 124\n",
      "Initial State is  [0, 12, 3]\n",
      "episode 187, reward 541.0, memory_length 2000, epsilon 0.3897078735047413, time 726.0, rides 125\n",
      "Initial State is  [4, 7, 3]\n",
      "episode 188, reward 452.0, memory_length 2000, epsilon 0.3877593341372176, time 728.0, rides 133\n",
      "Initial State is  [0, 13, 3]\n",
      "episode 189, reward 302.0, memory_length 2000, epsilon 0.3858205374665315, time 729.0, rides 126\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 190, reward 616.0, memory_length 2000, epsilon 0.38389143477919885, time 735.0, rides 123\n",
      "Initial State is  [3, 2, 3]\n",
      "episode 191, reward 575.0, memory_length 2000, epsilon 0.3819719776053028, time 730.0, rides 114\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 192, reward 572.0, memory_length 2000, epsilon 0.3800621177172763, time 743.0, rides 130\n",
      "Initial State is  [0, 20, 1]\n",
      "episode 193, reward 517.0, memory_length 2000, epsilon 0.37816180712868996, time 725.0, rides 119\n",
      "Initial State is  [0, 18, 1]\n",
      "episode 194, reward 542.0, memory_length 2000, epsilon 0.37627099809304654, time 724.0, rides 132\n",
      "Initial State is  [1, 11, 6]\n",
      "episode 195, reward 278.0, memory_length 2000, epsilon 0.3743896431025813, time 731.0, rides 116\n",
      "Initial State is  [1, 0, 1]\n",
      "episode 196, reward 409.0, memory_length 2000, epsilon 0.37251769488706843, time 726.0, rides 126\n",
      "Initial State is  [3, 2, 6]\n",
      "episode 197, reward 475.0, memory_length 2000, epsilon 0.3706551064126331, time 729.0, rides 129\n",
      "Initial State is  [4, 19, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 198, reward 443.0, memory_length 2000, epsilon 0.36880183088056995, time 726.0, rides 128\n",
      "Initial State is  [3, 12, 0]\n",
      "episode 199, reward 262.0, memory_length 2000, epsilon 0.3669578217261671, time 728.0, rides 125\n",
      "Initial State is  [1, 23, 6]\n",
      "episode 200, reward 491.0, memory_length 2000, epsilon 0.36512303261753626, time 742.0, rides 119\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 201, reward 454.0, memory_length 2000, epsilon 0.3632974174544486, time 731.0, rides 130\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 202, reward 493.0, memory_length 2000, epsilon 0.3614809303671764, time 735.0, rides 125\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 203, reward 422.0, memory_length 2000, epsilon 0.3596735257153405, time 728.0, rides 135\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 204, reward 791.0, memory_length 2000, epsilon 0.3578751580867638, time 738.0, rides 134\n",
      "Initial State is  [2, 10, 4]\n",
      "episode 205, reward 751.0, memory_length 2000, epsilon 0.35608578229633, time 733.0, rides 123\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 206, reward 513.0, memory_length 2000, epsilon 0.3543053533848483, time 726.0, rides 113\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 207, reward 603.0, memory_length 2000, epsilon 0.35253382661792404, time 729.0, rides 126\n",
      "Initial State is  [4, 20, 1]\n",
      "episode 208, reward 540.0, memory_length 2000, epsilon 0.3507711574848344, time 734.0, rides 129\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 209, reward 427.0, memory_length 2000, epsilon 0.34901730169741024, time 727.0, rides 124\n",
      "Initial State is  [4, 14, 4]\n",
      "episode 210, reward 515.0, memory_length 2000, epsilon 0.3472722151889232, time 723.0, rides 127\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 211, reward 522.0, memory_length 2000, epsilon 0.3455358541129786, time 730.0, rides 129\n",
      "Initial State is  [1, 21, 3]\n",
      "episode 212, reward 695.0, memory_length 2000, epsilon 0.3438081748424137, time 730.0, rides 119\n",
      "Initial State is  [2, 18, 3]\n",
      "episode 213, reward 644.0, memory_length 2000, epsilon 0.3420891339682016, time 730.0, rides 126\n",
      "Initial State is  [2, 7, 6]\n",
      "episode 214, reward 941.0, memory_length 2000, epsilon 0.3403786882983606, time 730.0, rides 126\n",
      "Initial State is  [2, 8, 4]\n",
      "episode 215, reward 528.0, memory_length 2000, epsilon 0.3386767948568688, time 726.0, rides 119\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 216, reward 483.0, memory_length 2000, epsilon 0.33698341088258443, time 731.0, rides 136\n",
      "Initial State is  [2, 0, 5]\n",
      "episode 217, reward 537.0, memory_length 2000, epsilon 0.3352984938281715, time 733.0, rides 124\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 218, reward 693.0, memory_length 2000, epsilon 0.33362200135903064, time 726.0, rides 118\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 219, reward 575.0, memory_length 2000, epsilon 0.33195389135223546, time 735.0, rides 121\n",
      "Initial State is  [0, 18, 3]\n",
      "episode 220, reward 517.0, memory_length 2000, epsilon 0.3302941218954743, time 731.0, rides 126\n",
      "Initial State is  [3, 8, 1]\n",
      "episode 221, reward 776.0, memory_length 2000, epsilon 0.32864265128599696, time 726.0, rides 118\n",
      "Initial State is  [0, 21, 6]\n",
      "episode 222, reward 637.0, memory_length 2000, epsilon 0.326999438029567, time 725.0, rides 127\n",
      "Initial State is  [3, 10, 6]\n",
      "episode 223, reward 459.0, memory_length 2000, epsilon 0.3253644408394192, time 724.0, rides 122\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 224, reward 756.0, memory_length 2000, epsilon 0.3237376186352221, time 735.0, rides 122\n",
      "Initial State is  [4, 9, 4]\n",
      "episode 225, reward 552.0, memory_length 2000, epsilon 0.322118930542046, time 738.0, rides 131\n",
      "Initial State is  [0, 16, 0]\n",
      "episode 226, reward 755.0, memory_length 2000, epsilon 0.32050833588933575, time 722.0, rides 132\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 227, reward 733.0, memory_length 2000, epsilon 0.31890579420988907, time 727.0, rides 119\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 228, reward 522.0, memory_length 2000, epsilon 0.3173112652388396, time 726.0, rides 119\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 229, reward 306.0, memory_length 2000, epsilon 0.3157247089126454, time 725.0, rides 134\n",
      "Initial State is  [4, 8, 3]\n",
      "episode 230, reward 268.0, memory_length 2000, epsilon 0.3141460853680822, time 725.0, rides 120\n",
      "Initial State is  [3, 3, 1]\n",
      "episode 231, reward 580.0, memory_length 2000, epsilon 0.3125753549412418, time 728.0, rides 121\n",
      "Initial State is  [3, 15, 2]\n",
      "episode 232, reward 585.0, memory_length 2000, epsilon 0.31101247816653554, time 725.0, rides 116\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 233, reward 596.0, memory_length 2000, epsilon 0.30945741577570285, time 737.0, rides 119\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 234, reward 384.0, memory_length 2000, epsilon 0.3079101286968243, time 735.0, rides 116\n",
      "Initial State is  [1, 20, 6]\n",
      "episode 235, reward 613.0, memory_length 2000, epsilon 0.3063705780533402, time 730.0, rides 130\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 236, reward 675.0, memory_length 2000, epsilon 0.30483872516307353, time 728.0, rides 133\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 237, reward 391.0, memory_length 2000, epsilon 0.3033145315372582, time 732.0, rides 100\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 238, reward 486.0, memory_length 2000, epsilon 0.3017979588795719, time 730.0, rides 120\n",
      "Initial State is  [3, 5, 1]\n",
      "episode 239, reward 876.0, memory_length 2000, epsilon 0.30028896908517405, time 738.0, rides 122\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 240, reward 554.0, memory_length 2000, epsilon 0.2987875242397482, time 736.0, rides 111\n",
      "Initial State is  [0, 2, 0]\n",
      "episode 241, reward 791.0, memory_length 2000, epsilon 0.29729358661854943, time 733.0, rides 126\n",
      "Initial State is  [1, 2, 0]\n",
      "episode 242, reward 551.0, memory_length 2000, epsilon 0.29580711868545667, time 731.0, rides 125\n",
      "Initial State is  [3, 5, 4]\n",
      "episode 243, reward 1020.0, memory_length 2000, epsilon 0.2943280830920294, time 737.0, rides 123\n",
      "Initial State is  [0, 5, 2]\n",
      "episode 244, reward 994.0, memory_length 2000, epsilon 0.29285644267656924, time 728.0, rides 125\n",
      "Initial State is  [2, 4, 2]\n",
      "episode 245, reward 620.0, memory_length 2000, epsilon 0.2913921604631864, time 729.0, rides 124\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 246, reward 518.0, memory_length 2000, epsilon 0.28993519966087045, time 732.0, rides 121\n",
      "Initial State is  [4, 23, 4]\n",
      "episode 247, reward 670.0, memory_length 2000, epsilon 0.2884855236625661, time 729.0, rides 130\n",
      "Initial State is  [1, 3, 2]\n",
      "episode 248, reward 563.0, memory_length 2000, epsilon 0.28704309604425327, time 734.0, rides 115\n",
      "Initial State is  [0, 9, 2]\n",
      "episode 249, reward 440.0, memory_length 2000, epsilon 0.285607880564032, time 728.0, rides 124\n",
      "Initial State is  [3, 18, 1]\n",
      "episode 250, reward 874.0, memory_length 2000, epsilon 0.28417984116121187, time 726.0, rides 129\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 251, reward 510.0, memory_length 2000, epsilon 0.2827589419554058, time 731.0, rides 129\n",
      "Initial State is  [2, 5, 4]\n",
      "episode 252, reward 834.0, memory_length 2000, epsilon 0.28134514724562876, time 723.0, rides 127\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 253, reward 379.0, memory_length 2000, epsilon 0.2799384215094006, time 730.0, rides 103\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 254, reward 786.0, memory_length 2000, epsilon 0.27853872940185365, time 727.0, rides 113\n",
      "Initial State is  [4, 15, 3]\n",
      "episode 255, reward 701.0, memory_length 2000, epsilon 0.27714603575484437, time 730.0, rides 127\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 256, reward 314.0, memory_length 2000, epsilon 0.2757603055760701, time 731.0, rides 116\n",
      "Initial State is  [4, 19, 1]\n",
      "episode 257, reward 602.0, memory_length 2000, epsilon 0.2743815040481898, time 725.0, rides 124\n",
      "Initial State is  [3, 17, 2]\n",
      "episode 258, reward 616.0, memory_length 2000, epsilon 0.2730095965279488, time 731.0, rides 129\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 259, reward 813.0, memory_length 2000, epsilon 0.27164454854530906, time 730.0, rides 120\n",
      "Initial State is  [1, 22, 5]\n",
      "episode 260, reward 788.0, memory_length 2000, epsilon 0.2702863258025825, time 728.0, rides 119\n",
      "Initial State is  [0, 6, 5]\n",
      "episode 261, reward 837.0, memory_length 2000, epsilon 0.2689348941735696, time 730.0, rides 124\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 262, reward 763.0, memory_length 2000, epsilon 0.26759021970270175, time 732.0, rides 129\n",
      "Initial State is  [2, 8, 1]\n",
      "episode 263, reward 698.0, memory_length 2000, epsilon 0.2662522686041882, time 726.0, rides 119\n",
      "Initial State is  [2, 7, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 264, reward 593.0, memory_length 2000, epsilon 0.2649210072611673, time 733.0, rides 128\n",
      "Initial State is  [3, 19, 6]\n",
      "episode 265, reward 943.0, memory_length 2000, epsilon 0.26359640222486147, time 727.0, rides 136\n",
      "Initial State is  [0, 5, 0]\n",
      "episode 266, reward 898.0, memory_length 2000, epsilon 0.26227842021373715, time 736.0, rides 131\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 267, reward 588.0, memory_length 2000, epsilon 0.2609670281126685, time 734.0, rides 117\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 268, reward 254.0, memory_length 2000, epsilon 0.25966219297210513, time 737.0, rides 131\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 269, reward 919.0, memory_length 2000, epsilon 0.2583638820072446, time 738.0, rides 129\n",
      "Initial State is  [4, 13, 3]\n",
      "episode 270, reward 890.0, memory_length 2000, epsilon 0.2570720625972084, time 730.0, rides 125\n",
      "Initial State is  [2, 20, 2]\n",
      "episode 271, reward 899.0, memory_length 2000, epsilon 0.25578670228422234, time 725.0, rides 125\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 272, reward 826.0, memory_length 2000, epsilon 0.25450776877280124, time 725.0, rides 113\n",
      "Initial State is  [4, 17, 6]\n",
      "episode 273, reward 917.0, memory_length 2000, epsilon 0.2532352299289372, time 728.0, rides 120\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 274, reward 859.0, memory_length 2000, epsilon 0.2519690537792925, time 729.0, rides 125\n",
      "Initial State is  [4, 11, 0]\n",
      "episode 275, reward 817.0, memory_length 2000, epsilon 0.2507092085103961, time 735.0, rides 124\n",
      "Initial State is  [2, 5, 4]\n",
      "episode 276, reward 675.0, memory_length 2000, epsilon 0.2494556624678441, time 724.0, rides 131\n",
      "Initial State is  [4, 22, 1]\n",
      "episode 277, reward 885.0, memory_length 2000, epsilon 0.24820838415550486, time 730.0, rides 125\n",
      "Initial State is  [4, 5, 6]\n",
      "episode 278, reward 657.0, memory_length 2000, epsilon 0.24696734223472733, time 734.0, rides 116\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 279, reward 745.0, memory_length 2000, epsilon 0.2457325055235537, time 732.0, rides 125\n",
      "Initial State is  [4, 0, 5]\n",
      "episode 280, reward 630.0, memory_length 2000, epsilon 0.24450384299593592, time 726.0, rides 119\n",
      "Initial State is  [2, 22, 2]\n",
      "episode 281, reward 999.0, memory_length 2000, epsilon 0.24328132378095624, time 727.0, rides 134\n",
      "Initial State is  [4, 13, 6]\n",
      "episode 282, reward 612.0, memory_length 2000, epsilon 0.24206491716205145, time 741.0, rides 124\n",
      "Initial State is  [4, 13, 4]\n",
      "episode 283, reward 696.0, memory_length 2000, epsilon 0.2408545925762412, time 734.0, rides 133\n",
      "Initial State is  [3, 2, 2]\n",
      "episode 284, reward 928.0, memory_length 2000, epsilon 0.23965031961336, time 731.0, rides 127\n",
      "Initial State is  [4, 19, 1]\n",
      "episode 285, reward 643.0, memory_length 2000, epsilon 0.2384520680152932, time 737.0, rides 140\n",
      "Initial State is  [2, 3, 5]\n",
      "episode 286, reward 506.0, memory_length 2000, epsilon 0.23725980767521673, time 737.0, rides 116\n",
      "Initial State is  [1, 1, 6]\n",
      "episode 287, reward 844.0, memory_length 2000, epsilon 0.23607350863684065, time 730.0, rides 135\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 288, reward 1050.0, memory_length 2000, epsilon 0.23489314109365644, time 725.0, rides 133\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 289, reward 791.0, memory_length 2000, epsilon 0.23371867538818816, time 732.0, rides 123\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 290, reward 850.0, memory_length 2000, epsilon 0.23255008201124722, time 734.0, rides 128\n",
      "Initial State is  [2, 4, 1]\n",
      "episode 291, reward 659.0, memory_length 2000, epsilon 0.231387331601191, time 729.0, rides 112\n",
      "Initial State is  [4, 20, 5]\n",
      "episode 292, reward 857.0, memory_length 2000, epsilon 0.23023039494318503, time 723.0, rides 119\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 293, reward 921.0, memory_length 2000, epsilon 0.2290792429684691, time 734.0, rides 119\n",
      "Initial State is  [4, 23, 1]\n",
      "episode 294, reward 872.0, memory_length 2000, epsilon 0.22793384675362674, time 729.0, rides 127\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 295, reward 878.0, memory_length 2000, epsilon 0.22679417751985861, time 739.0, rides 120\n",
      "Initial State is  [3, 20, 5]\n",
      "episode 296, reward 977.0, memory_length 2000, epsilon 0.22566020663225933, time 734.0, rides 127\n",
      "Initial State is  [4, 2, 4]\n",
      "episode 297, reward 618.0, memory_length 2000, epsilon 0.22453190559909803, time 722.0, rides 129\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 298, reward 817.0, memory_length 2000, epsilon 0.22340924607110255, time 729.0, rides 133\n",
      "Initial State is  [0, 4, 4]\n",
      "episode 299, reward 809.0, memory_length 2000, epsilon 0.22229219984074702, time 728.0, rides 125\n",
      "Initial State is  [0, 4, 0]\n",
      "episode 300, reward 494.0, memory_length 2000, epsilon 0.2211807388415433, time 725.0, rides 134\n",
      "Initial State is  [1, 4, 6]\n",
      "episode 301, reward 646.0, memory_length 2000, epsilon 0.22007483514733558, time 732.0, rides 123\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 302, reward 868.0, memory_length 2000, epsilon 0.2189744609715989, time 733.0, rides 118\n",
      "Initial State is  [2, 1, 3]\n",
      "episode 303, reward 988.0, memory_length 2000, epsilon 0.2178795886667409, time 733.0, rides 130\n",
      "Initial State is  [4, 2, 3]\n",
      "episode 304, reward 647.0, memory_length 2000, epsilon 0.2167901907234072, time 730.0, rides 128\n",
      "Initial State is  [1, 16, 3]\n",
      "episode 305, reward 647.0, memory_length 2000, epsilon 0.21570623976979014, time 730.0, rides 126\n",
      "Initial State is  [3, 5, 4]\n",
      "episode 306, reward 603.0, memory_length 2000, epsilon 0.21462770857094118, time 727.0, rides 131\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 307, reward 505.0, memory_length 2000, epsilon 0.21355457002808648, time 726.0, rides 117\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 308, reward 1004.0, memory_length 2000, epsilon 0.21248679717794605, time 732.0, rides 123\n",
      "Initial State is  [4, 0, 4]\n",
      "episode 309, reward 702.0, memory_length 2000, epsilon 0.21142436319205632, time 722.0, rides 120\n",
      "Initial State is  [0, 0, 5]\n",
      "episode 310, reward 1050.0, memory_length 2000, epsilon 0.21036724137609603, time 727.0, rides 122\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 311, reward 672.0, memory_length 2000, epsilon 0.20931540516921554, time 736.0, rides 119\n",
      "Initial State is  [3, 19, 3]\n",
      "episode 312, reward 799.0, memory_length 2000, epsilon 0.20826882814336947, time 731.0, rides 124\n",
      "Initial State is  [3, 10, 6]\n",
      "episode 313, reward 704.0, memory_length 2000, epsilon 0.20722748400265262, time 728.0, rides 116\n",
      "Initial State is  [4, 16, 5]\n",
      "episode 314, reward 771.0, memory_length 2000, epsilon 0.20619134658263935, time 724.0, rides 119\n",
      "Initial State is  [2, 19, 0]\n",
      "episode 315, reward 592.0, memory_length 2000, epsilon 0.20516038984972615, time 732.0, rides 128\n",
      "Initial State is  [0, 0, 6]\n",
      "episode 316, reward 709.0, memory_length 2000, epsilon 0.2041345879004775, time 734.0, rides 130\n",
      "Initial State is  [4, 15, 5]\n",
      "episode 317, reward 825.0, memory_length 2000, epsilon 0.2031139149609751, time 729.0, rides 118\n",
      "Initial State is  [0, 9, 2]\n",
      "episode 318, reward 758.0, memory_length 2000, epsilon 0.20209834538617025, time 729.0, rides 111\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 319, reward 976.0, memory_length 2000, epsilon 0.2010878536592394, time 728.0, rides 115\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 320, reward 763.0, memory_length 2000, epsilon 0.2000824143909432, time 725.0, rides 126\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 321, reward 671.0, memory_length 2000, epsilon 0.19908200231898848, time 727.0, rides 117\n",
      "Initial State is  [1, 20, 6]\n",
      "episode 322, reward 775.0, memory_length 2000, epsilon 0.19808659230739353, time 737.0, rides 114\n",
      "Initial State is  [3, 14, 2]\n",
      "episode 323, reward 744.0, memory_length 2000, epsilon 0.19709615934585656, time 739.0, rides 126\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 324, reward 837.0, memory_length 2000, epsilon 0.19611067854912728, time 728.0, rides 118\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 325, reward 1071.0, memory_length 2000, epsilon 0.19513012515638165, time 726.0, rides 125\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 326, reward 857.0, memory_length 2000, epsilon 0.19415447453059972, time 735.0, rides 110\n",
      "Initial State is  [2, 8, 0]\n",
      "episode 327, reward 650.0, memory_length 2000, epsilon 0.19318370215794672, time 729.0, rides 122\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 328, reward 660.0, memory_length 2000, epsilon 0.192217783647157, time 743.0, rides 123\n",
      "Initial State is  [2, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 329, reward 360.0, memory_length 2000, epsilon 0.1912566947289212, time 731.0, rides 121\n",
      "Initial State is  [0, 7, 2]\n",
      "episode 330, reward 749.0, memory_length 2000, epsilon 0.1903004112552766, time 734.0, rides 128\n",
      "Initial State is  [4, 1, 6]\n",
      "episode 331, reward 718.0, memory_length 2000, epsilon 0.18934890919900021, time 729.0, rides 131\n",
      "Initial State is  [1, 22, 5]\n",
      "episode 332, reward 883.0, memory_length 2000, epsilon 0.18840216465300522, time 725.0, rides 122\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 333, reward 773.0, memory_length 2000, epsilon 0.18746015382974018, time 741.0, rides 126\n",
      "Initial State is  [2, 16, 6]\n",
      "episode 334, reward 844.0, memory_length 2000, epsilon 0.1865228530605915, time 729.0, rides 134\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 335, reward 1030.0, memory_length 2000, epsilon 0.18559023879528855, time 732.0, rides 135\n",
      "Initial State is  [3, 9, 0]\n",
      "episode 336, reward 695.0, memory_length 2000, epsilon 0.1846622876013121, time 730.0, rides 139\n",
      "Initial State is  [1, 19, 5]\n",
      "episode 337, reward 759.0, memory_length 2000, epsilon 0.18373897616330553, time 725.0, rides 117\n",
      "Initial State is  [0, 11, 2]\n",
      "episode 338, reward 674.0, memory_length 2000, epsilon 0.182820281282489, time 738.0, rides 128\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 339, reward 651.0, memory_length 2000, epsilon 0.18190617987607657, time 733.0, rides 126\n",
      "Initial State is  [3, 16, 2]\n",
      "episode 340, reward 881.0, memory_length 2000, epsilon 0.18099664897669618, time 729.0, rides 129\n",
      "Initial State is  [1, 13, 4]\n",
      "episode 341, reward 1219.0, memory_length 2000, epsilon 0.1800916657318127, time 725.0, rides 133\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 342, reward 878.0, memory_length 2000, epsilon 0.17919120740315364, time 728.0, rides 117\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 343, reward 778.0, memory_length 2000, epsilon 0.17829525136613786, time 728.0, rides 132\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 344, reward 1168.0, memory_length 2000, epsilon 0.17740377510930716, time 733.0, rides 129\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 345, reward 815.0, memory_length 2000, epsilon 0.17651675623376062, time 727.0, rides 123\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 346, reward 869.0, memory_length 2000, epsilon 0.1756341724525918, time 739.0, rides 114\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 347, reward 961.0, memory_length 2000, epsilon 0.17475600159032884, time 729.0, rides 124\n",
      "Initial State is  [3, 3, 1]\n",
      "episode 348, reward 910.0, memory_length 2000, epsilon 0.17388222158237718, time 725.0, rides 111\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 349, reward 791.0, memory_length 2000, epsilon 0.1730128104744653, time 727.0, rides 124\n",
      "Initial State is  [3, 4, 4]\n",
      "episode 350, reward 814.0, memory_length 2000, epsilon 0.17214774642209296, time 735.0, rides 123\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 351, reward 953.0, memory_length 2000, epsilon 0.1712870076899825, time 725.0, rides 121\n",
      "Initial State is  [1, 18, 1]\n",
      "episode 352, reward 733.0, memory_length 2000, epsilon 0.17043057265153258, time 737.0, rides 127\n",
      "Initial State is  [4, 12, 6]\n",
      "episode 353, reward 690.0, memory_length 2000, epsilon 0.16957841978827493, time 731.0, rides 117\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 354, reward 765.0, memory_length 2000, epsilon 0.16873052768933355, time 733.0, rides 124\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 355, reward 819.0, memory_length 2000, epsilon 0.1678868750508869, time 731.0, rides 125\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 356, reward 651.0, memory_length 2000, epsilon 0.16704744067563246, time 729.0, rides 120\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 357, reward 621.0, memory_length 2000, epsilon 0.1662122034722543, time 723.0, rides 126\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 358, reward 877.0, memory_length 2000, epsilon 0.16538114245489302, time 727.0, rides 117\n",
      "Initial State is  [1, 1, 1]\n",
      "episode 359, reward 610.0, memory_length 2000, epsilon 0.16455423674261854, time 731.0, rides 120\n",
      "Initial State is  [1, 14, 6]\n",
      "episode 360, reward 931.0, memory_length 2000, epsilon 0.16373146555890544, time 732.0, rides 117\n",
      "Initial State is  [3, 14, 2]\n",
      "episode 361, reward 682.0, memory_length 2000, epsilon 0.16291280823111093, time 741.0, rides 128\n",
      "Initial State is  [4, 16, 4]\n",
      "episode 362, reward 649.0, memory_length 2000, epsilon 0.16209824418995536, time 728.0, rides 125\n",
      "Initial State is  [4, 9, 3]\n",
      "episode 363, reward 454.0, memory_length 2000, epsilon 0.16128775296900558, time 739.0, rides 129\n",
      "Initial State is  [0, 6, 3]\n",
      "episode 364, reward 1033.0, memory_length 2000, epsilon 0.16048131420416054, time 728.0, rides 112\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 365, reward 600.0, memory_length 2000, epsilon 0.15967890763313974, time 731.0, rides 121\n",
      "Initial State is  [0, 0, 3]\n",
      "episode 366, reward 709.0, memory_length 2000, epsilon 0.15888051309497406, time 732.0, rides 113\n",
      "Initial State is  [3, 11, 5]\n",
      "episode 367, reward 782.0, memory_length 2000, epsilon 0.1580861105294992, time 730.0, rides 132\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 368, reward 982.0, memory_length 2000, epsilon 0.1572956799768517, time 723.0, rides 122\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 369, reward 857.0, memory_length 2000, epsilon 0.15650920157696743, time 727.0, rides 112\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 370, reward 1068.0, memory_length 2000, epsilon 0.1557266555690826, time 730.0, rides 123\n",
      "Initial State is  [1, 12, 4]\n",
      "episode 371, reward 815.0, memory_length 2000, epsilon 0.1549480222912372, time 725.0, rides 121\n",
      "Initial State is  [1, 1, 2]\n",
      "episode 372, reward 816.0, memory_length 2000, epsilon 0.15417328217978102, time 734.0, rides 125\n",
      "Initial State is  [0, 22, 1]\n",
      "episode 373, reward 1038.0, memory_length 2000, epsilon 0.1534024157688821, time 731.0, rides 118\n",
      "Initial State is  [1, 23, 5]\n",
      "episode 374, reward 549.0, memory_length 2000, epsilon 0.1526354036900377, time 731.0, rides 125\n",
      "Initial State is  [1, 8, 2]\n",
      "episode 375, reward 918.0, memory_length 2000, epsilon 0.1518722266715875, time 730.0, rides 127\n",
      "Initial State is  [2, 15, 6]\n",
      "episode 376, reward 672.0, memory_length 2000, epsilon 0.15111286553822956, time 729.0, rides 125\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 377, reward 965.0, memory_length 2000, epsilon 0.15035730121053842, time 727.0, rides 128\n",
      "Initial State is  [1, 11, 4]\n",
      "episode 378, reward 1136.0, memory_length 2000, epsilon 0.14960551470448571, time 730.0, rides 119\n",
      "Initial State is  [3, 19, 0]\n",
      "episode 379, reward 756.0, memory_length 2000, epsilon 0.14885748713096328, time 733.0, rides 124\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 380, reward 1110.0, memory_length 2000, epsilon 0.14811319969530845, time 721.0, rides 128\n",
      "Initial State is  [0, 16, 5]\n",
      "episode 381, reward 701.0, memory_length 2000, epsilon 0.1473726336968319, time 732.0, rides 126\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 382, reward 939.0, memory_length 2000, epsilon 0.14663577052834775, time 727.0, rides 118\n",
      "Initial State is  [0, 5, 2]\n",
      "episode 383, reward 916.0, memory_length 2000, epsilon 0.14590259167570602, time 735.0, rides 120\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 384, reward 999.0, memory_length 2000, epsilon 0.1451730787173275, time 731.0, rides 125\n",
      "Initial State is  [0, 7, 2]\n",
      "episode 385, reward 851.0, memory_length 2000, epsilon 0.14444721332374086, time 746.0, rides 124\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 386, reward 866.0, memory_length 2000, epsilon 0.14372497725712216, time 727.0, rides 127\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 387, reward 976.0, memory_length 2000, epsilon 0.14300635237083656, time 727.0, rides 132\n",
      "Initial State is  [3, 21, 0]\n",
      "episode 388, reward 886.0, memory_length 2000, epsilon 0.14229132060898236, time 728.0, rides 131\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 389, reward 1013.0, memory_length 2000, epsilon 0.14157986400593744, time 728.0, rides 121\n",
      "Initial State is  [4, 7, 1]\n",
      "episode 390, reward 800.0, memory_length 2000, epsilon 0.14087196468590776, time 734.0, rides 124\n",
      "Initial State is  [2, 23, 1]\n",
      "episode 391, reward 631.0, memory_length 2000, epsilon 0.14016760486247823, time 728.0, rides 122\n",
      "Initial State is  [2, 15, 1]\n",
      "episode 392, reward 748.0, memory_length 2000, epsilon 0.13946676683816583, time 736.0, rides 117\n",
      "Initial State is  [2, 7, 2]\n",
      "episode 393, reward 926.0, memory_length 2000, epsilon 0.138769433003975, time 727.0, rides 125\n",
      "Initial State is  [3, 16, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 394, reward 677.0, memory_length 2000, epsilon 0.13807558583895513, time 728.0, rides 135\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 395, reward 865.0, memory_length 2000, epsilon 0.13738520790976036, time 730.0, rides 126\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 396, reward 890.0, memory_length 2000, epsilon 0.13669828187021155, time 726.0, rides 124\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 397, reward 987.0, memory_length 2000, epsilon 0.13601479046086049, time 728.0, rides 123\n",
      "Initial State is  [4, 0, 0]\n",
      "episode 398, reward 901.0, memory_length 2000, epsilon 0.1353347165085562, time 728.0, rides 127\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 399, reward 1155.0, memory_length 2000, epsilon 0.1346580429260134, time 737.0, rides 131\n",
      "Initial State is  [0, 2, 6]\n",
      "episode 400, reward 1129.0, memory_length 2000, epsilon 0.13398475271138335, time 726.0, rides 119\n",
      "Initial State is  [2, 23, 1]\n",
      "episode 401, reward 952.0, memory_length 2000, epsilon 0.13331482894782642, time 733.0, rides 133\n",
      "Initial State is  [0, 8, 1]\n",
      "episode 402, reward 934.0, memory_length 2000, epsilon 0.13264825480308728, time 723.0, rides 126\n",
      "Initial State is  [1, 7, 3]\n",
      "episode 403, reward 756.0, memory_length 2000, epsilon 0.13198501352907185, time 736.0, rides 121\n",
      "Initial State is  [1, 5, 5]\n",
      "episode 404, reward 878.0, memory_length 2000, epsilon 0.1313250884614265, time 723.0, rides 115\n",
      "Initial State is  [0, 17, 5]\n",
      "episode 405, reward 905.0, memory_length 2000, epsilon 0.13066846301911936, time 734.0, rides 121\n",
      "Initial State is  [1, 5, 5]\n",
      "episode 406, reward 1098.0, memory_length 2000, epsilon 0.13001512070402377, time 731.0, rides 121\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 407, reward 1009.0, memory_length 2000, epsilon 0.12936504510050365, time 729.0, rides 122\n",
      "Initial State is  [3, 22, 4]\n",
      "episode 408, reward 826.0, memory_length 2000, epsilon 0.12871821987500112, time 728.0, rides 118\n",
      "Initial State is  [1, 20, 4]\n",
      "episode 409, reward 1118.0, memory_length 2000, epsilon 0.12807462877562611, time 733.0, rides 117\n",
      "Initial State is  [0, 8, 5]\n",
      "episode 410, reward 758.0, memory_length 2000, epsilon 0.12743425563174798, time 730.0, rides 109\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 411, reward 918.0, memory_length 2000, epsilon 0.12679708435358925, time 730.0, rides 109\n",
      "Initial State is  [2, 8, 1]\n",
      "episode 412, reward 1061.0, memory_length 2000, epsilon 0.1261630989318213, time 729.0, rides 131\n",
      "Initial State is  [1, 18, 3]\n",
      "episode 413, reward 742.0, memory_length 2000, epsilon 0.1255322834371622, time 724.0, rides 122\n",
      "Initial State is  [1, 1, 3]\n",
      "episode 414, reward 905.0, memory_length 2000, epsilon 0.12490462201997637, time 728.0, rides 120\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 415, reward 716.0, memory_length 2000, epsilon 0.1242800989098765, time 734.0, rides 118\n",
      "Initial State is  [2, 12, 5]\n",
      "episode 416, reward 780.0, memory_length 2000, epsilon 0.12365869841532712, time 723.0, rides 128\n",
      "Initial State is  [2, 17, 0]\n",
      "episode 417, reward 980.0, memory_length 2000, epsilon 0.12304040492325048, time 734.0, rides 123\n",
      "Initial State is  [2, 5, 3]\n",
      "episode 418, reward 790.0, memory_length 2000, epsilon 0.12242520289863423, time 723.0, rides 117\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 419, reward 1005.0, memory_length 2000, epsilon 0.12181307688414106, time 734.0, rides 123\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 420, reward 917.0, memory_length 2000, epsilon 0.12120401149972035, time 741.0, rides 134\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 421, reward 822.0, memory_length 2000, epsilon 0.12059799144222175, time 728.0, rides 108\n",
      "Initial State is  [3, 5, 6]\n",
      "episode 422, reward 817.0, memory_length 2000, epsilon 0.11999500148501063, time 721.0, rides 125\n",
      "Initial State is  [3, 1, 1]\n",
      "episode 423, reward 948.0, memory_length 2000, epsilon 0.11939502647758558, time 731.0, rides 129\n",
      "Initial State is  [0, 20, 3]\n",
      "episode 424, reward 853.0, memory_length 2000, epsilon 0.11879805134519765, time 730.0, rides 123\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 425, reward 1162.0, memory_length 2000, epsilon 0.11820406108847166, time 728.0, rides 120\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 426, reward 1257.0, memory_length 2000, epsilon 0.1176130407830293, time 725.0, rides 136\n",
      "Initial State is  [0, 3, 5]\n",
      "episode 427, reward 897.0, memory_length 2000, epsilon 0.11702497557911415, time 733.0, rides 127\n",
      "Initial State is  [0, 0, 6]\n",
      "episode 428, reward 994.0, memory_length 2000, epsilon 0.11643985070121858, time 724.0, rides 123\n",
      "Initial State is  [4, 15, 6]\n",
      "episode 429, reward 968.0, memory_length 2000, epsilon 0.11585765144771248, time 726.0, rides 119\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 430, reward 913.0, memory_length 2000, epsilon 0.11527836319047392, time 729.0, rides 114\n",
      "Initial State is  [4, 16, 4]\n",
      "episode 431, reward 931.0, memory_length 2000, epsilon 0.11470197137452155, time 733.0, rides 140\n",
      "Initial State is  [4, 4, 0]\n",
      "episode 432, reward 958.0, memory_length 2000, epsilon 0.11412846151764894, time 736.0, rides 120\n",
      "Initial State is  [2, 1, 3]\n",
      "episode 433, reward 660.0, memory_length 2000, epsilon 0.1135578192100607, time 728.0, rides 119\n",
      "Initial State is  [2, 2, 2]\n",
      "episode 434, reward 781.0, memory_length 2000, epsilon 0.11299003011401039, time 729.0, rides 112\n",
      "Initial State is  [4, 6, 5]\n",
      "episode 435, reward 849.0, memory_length 2000, epsilon 0.11242507996344034, time 733.0, rides 128\n",
      "Initial State is  [0, 18, 3]\n",
      "episode 436, reward 586.0, memory_length 2000, epsilon 0.11186295456362313, time 730.0, rides 128\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 437, reward 909.0, memory_length 2000, epsilon 0.11130363979080501, time 731.0, rides 113\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 438, reward 940.0, memory_length 2000, epsilon 0.11074712159185099, time 724.0, rides 120\n",
      "Initial State is  [2, 21, 1]\n",
      "episode 439, reward 915.0, memory_length 2000, epsilon 0.11019338598389174, time 721.0, rides 120\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 440, reward 845.0, memory_length 2000, epsilon 0.10964241905397228, time 743.0, rides 120\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 441, reward 847.0, memory_length 2000, epsilon 0.10909420695870241, time 724.0, rides 128\n",
      "Initial State is  [1, 17, 2]\n",
      "episode 442, reward 1221.0, memory_length 2000, epsilon 0.1085487359239089, time 732.0, rides 123\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 443, reward 509.0, memory_length 2000, epsilon 0.10800599224428936, time 728.0, rides 114\n",
      "Initial State is  [1, 17, 4]\n",
      "episode 444, reward 910.0, memory_length 2000, epsilon 0.10746596228306791, time 737.0, rides 127\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 445, reward 626.0, memory_length 2000, epsilon 0.10692863247165257, time 729.0, rides 127\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 446, reward 1004.0, memory_length 2000, epsilon 0.1063939893092943, time 730.0, rides 119\n",
      "Initial State is  [4, 20, 1]\n",
      "episode 447, reward 1390.0, memory_length 2000, epsilon 0.10586201936274783, time 729.0, rides 131\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 448, reward 1170.0, memory_length 2000, epsilon 0.10533270926593409, time 728.0, rides 119\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 449, reward 991.0, memory_length 2000, epsilon 0.10480604571960442, time 736.0, rides 128\n",
      "Initial State is  [4, 13, 5]\n",
      "episode 450, reward 1173.0, memory_length 2000, epsilon 0.1042820154910064, time 730.0, rides 130\n",
      "Initial State is  [4, 23, 2]\n",
      "episode 451, reward 1157.0, memory_length 2000, epsilon 0.10376060541355137, time 724.0, rides 117\n",
      "Initial State is  [1, 1, 2]\n",
      "episode 452, reward 805.0, memory_length 2000, epsilon 0.1032418023864836, time 734.0, rides 117\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 453, reward 772.0, memory_length 2000, epsilon 0.10272559337455119, time 737.0, rides 118\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 454, reward 983.0, memory_length 2000, epsilon 0.10221196540767843, time 732.0, rides 138\n",
      "Initial State is  [3, 6, 1]\n",
      "episode 455, reward 493.0, memory_length 2000, epsilon 0.10170090558064004, time 729.0, rides 127\n",
      "Initial State is  [3, 8, 3]\n",
      "episode 456, reward 755.0, memory_length 2000, epsilon 0.10119240105273684, time 730.0, rides 121\n",
      "Initial State is  [3, 2, 4]\n",
      "episode 457, reward 1066.0, memory_length 2000, epsilon 0.10068643904747315, time 730.0, rides 118\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 458, reward 613.0, memory_length 2000, epsilon 0.10018300685223579, time 734.0, rides 125\n",
      "Initial State is  [4, 5, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 459, reward 910.0, memory_length 2000, epsilon 0.0996820918179746, time 727.0, rides 123\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 460, reward 1031.0, memory_length 2000, epsilon 0.09918368135888474, time 727.0, rides 124\n",
      "Initial State is  [2, 2, 2]\n",
      "episode 461, reward 1078.0, memory_length 2000, epsilon 0.09868776295209031, time 731.0, rides 126\n",
      "Initial State is  [0, 21, 2]\n",
      "episode 462, reward 1082.0, memory_length 2000, epsilon 0.09819432413732986, time 724.0, rides 121\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 463, reward 949.0, memory_length 2000, epsilon 0.09770335251664321, time 730.0, rides 122\n",
      "Initial State is  [0, 8, 2]\n",
      "episode 464, reward 933.0, memory_length 2000, epsilon 0.09721483575406, time 722.0, rides 121\n",
      "Initial State is  [1, 0, 1]\n",
      "episode 465, reward 811.0, memory_length 2000, epsilon 0.09672876157528969, time 725.0, rides 125\n",
      "Initial State is  [2, 3, 6]\n",
      "episode 466, reward 742.0, memory_length 2000, epsilon 0.09624511776741324, time 729.0, rides 117\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 467, reward 781.0, memory_length 2000, epsilon 0.09576389217857617, time 731.0, rides 123\n",
      "Initial State is  [4, 16, 4]\n",
      "episode 468, reward 1039.0, memory_length 2000, epsilon 0.09528507271768329, time 727.0, rides 121\n",
      "Initial State is  [3, 1, 1]\n",
      "episode 469, reward 613.0, memory_length 2000, epsilon 0.09480864735409487, time 726.0, rides 120\n",
      "Initial State is  [3, 18, 1]\n",
      "episode 470, reward 841.0, memory_length 2000, epsilon 0.0943346041173244, time 734.0, rides 123\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 471, reward 897.0, memory_length 2000, epsilon 0.09386293109673778, time 726.0, rides 122\n",
      "Initial State is  [1, 5, 2]\n",
      "episode 472, reward 892.0, memory_length 2000, epsilon 0.09339361644125409, time 727.0, rides 128\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 473, reward 995.0, memory_length 2000, epsilon 0.09292664835904782, time 730.0, rides 124\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 474, reward 1244.0, memory_length 2000, epsilon 0.09246201511725258, time 725.0, rides 117\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 475, reward 1077.0, memory_length 2000, epsilon 0.09199970504166631, time 727.0, rides 130\n",
      "Initial State is  [2, 12, 5]\n",
      "episode 476, reward 883.0, memory_length 2000, epsilon 0.09153970651645797, time 733.0, rides 121\n",
      "Initial State is  [1, 5, 3]\n",
      "episode 477, reward 1042.0, memory_length 2000, epsilon 0.09108200798387568, time 731.0, rides 127\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 478, reward 942.0, memory_length 2000, epsilon 0.0906265979439563, time 732.0, rides 125\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 479, reward 731.0, memory_length 2000, epsilon 0.09017346495423652, time 732.0, rides 123\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 480, reward 813.0, memory_length 2000, epsilon 0.08972259762946533, time 724.0, rides 115\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 481, reward 941.0, memory_length 2000, epsilon 0.089273984641318, time 732.0, rides 124\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 482, reward 812.0, memory_length 2000, epsilon 0.0888276147181114, time 727.0, rides 120\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 483, reward 1117.0, memory_length 2000, epsilon 0.08838347664452084, time 725.0, rides 120\n",
      "Initial State is  [4, 10, 1]\n",
      "episode 484, reward 1108.0, memory_length 2000, epsilon 0.08794155926129824, time 734.0, rides 126\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 485, reward 1052.0, memory_length 2000, epsilon 0.08750185146499175, time 732.0, rides 116\n",
      "Initial State is  [1, 8, 6]\n",
      "episode 486, reward 1167.0, memory_length 2000, epsilon 0.08706434220766679, time 745.0, rides 127\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 487, reward 907.0, memory_length 2000, epsilon 0.08662902049662846, time 725.0, rides 110\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 488, reward 956.0, memory_length 2000, epsilon 0.08619587539414532, time 729.0, rides 111\n",
      "Initial State is  [1, 18, 1]\n",
      "episode 489, reward 992.0, memory_length 2000, epsilon 0.08576489601717459, time 732.0, rides 133\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 490, reward 994.0, memory_length 2000, epsilon 0.08533607153708872, time 729.0, rides 120\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 491, reward 979.0, memory_length 2000, epsilon 0.08490939117940327, time 728.0, rides 125\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 492, reward 1248.0, memory_length 2000, epsilon 0.08448484422350626, time 730.0, rides 118\n",
      "Initial State is  [3, 15, 0]\n",
      "episode 493, reward 704.0, memory_length 2000, epsilon 0.08406242000238873, time 734.0, rides 114\n",
      "Initial State is  [4, 1, 6]\n",
      "episode 494, reward 1127.0, memory_length 2000, epsilon 0.08364210790237678, time 727.0, rides 128\n",
      "Initial State is  [0, 18, 3]\n",
      "episode 495, reward 1227.0, memory_length 2000, epsilon 0.0832238973628649, time 725.0, rides 135\n",
      "Initial State is  [4, 0, 5]\n",
      "episode 496, reward 1128.0, memory_length 2000, epsilon 0.08280777787605056, time 724.0, rides 118\n",
      "Initial State is  [3, 8, 0]\n",
      "episode 497, reward 824.0, memory_length 2000, epsilon 0.08239373898667031, time 726.0, rides 119\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 498, reward 1065.0, memory_length 2000, epsilon 0.08198177029173696, time 728.0, rides 123\n",
      "Initial State is  [3, 16, 0]\n",
      "episode 499, reward 1143.0, memory_length 2000, epsilon 0.08157186144027828, time 723.0, rides 119\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 500, reward 1130.0, memory_length 2000, epsilon 0.0811640021330769, time 728.0, rides 129\n",
      "Initial State is  [2, 9, 3]\n",
      "episode 501, reward 1116.0, memory_length 2000, epsilon 0.08075818212241151, time 735.0, rides 130\n",
      "Initial State is  [3, 6, 1]\n",
      "episode 502, reward 1293.0, memory_length 2000, epsilon 0.08035439121179945, time 734.0, rides 121\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 503, reward 1026.0, memory_length 2000, epsilon 0.07995261925574046, time 730.0, rides 127\n",
      "Initial State is  [4, 18, 6]\n",
      "episode 504, reward 750.0, memory_length 2000, epsilon 0.07955285615946175, time 731.0, rides 116\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 505, reward 797.0, memory_length 2000, epsilon 0.07915509187866444, time 733.0, rides 121\n",
      "Initial State is  [0, 1, 6]\n",
      "episode 506, reward 867.0, memory_length 2000, epsilon 0.07875931641927113, time 731.0, rides 126\n",
      "Initial State is  [1, 19, 5]\n",
      "episode 507, reward 707.0, memory_length 2000, epsilon 0.07836551983717477, time 727.0, rides 119\n",
      "Initial State is  [3, 23, 1]\n",
      "episode 508, reward 849.0, memory_length 2000, epsilon 0.07797369223798889, time 731.0, rides 120\n",
      "Initial State is  [4, 13, 3]\n",
      "episode 509, reward 850.0, memory_length 2000, epsilon 0.07758382377679894, time 733.0, rides 125\n",
      "Initial State is  [4, 8, 2]\n",
      "episode 510, reward 908.0, memory_length 2000, epsilon 0.07719590465791494, time 733.0, rides 125\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 511, reward 1083.0, memory_length 2000, epsilon 0.07680992513462537, time 730.0, rides 126\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 512, reward 628.0, memory_length 2000, epsilon 0.07642587550895225, time 725.0, rides 121\n",
      "Initial State is  [1, 8, 6]\n",
      "episode 513, reward 828.0, memory_length 2000, epsilon 0.07604374613140748, time 738.0, rides 122\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 514, reward 876.0, memory_length 2000, epsilon 0.07566352740075044, time 724.0, rides 122\n",
      "Initial State is  [2, 20, 6]\n",
      "episode 515, reward 1132.0, memory_length 2000, epsilon 0.07528520976374668, time 729.0, rides 135\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 516, reward 991.0, memory_length 2000, epsilon 0.07490878371492794, time 730.0, rides 116\n",
      "Initial State is  [1, 1, 5]\n",
      "episode 517, reward 1040.0, memory_length 2000, epsilon 0.0745342397963533, time 739.0, rides 123\n",
      "Initial State is  [3, 5, 3]\n",
      "episode 518, reward 931.0, memory_length 2000, epsilon 0.07416156859737154, time 731.0, rides 125\n",
      "Initial State is  [4, 8, 0]\n",
      "episode 519, reward 1320.0, memory_length 2000, epsilon 0.07379076075438468, time 738.0, rides 121\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 520, reward 790.0, memory_length 2000, epsilon 0.07342180695061275, time 727.0, rides 132\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 521, reward 661.0, memory_length 2000, epsilon 0.07305469791585968, time 723.0, rides 123\n",
      "Initial State is  [0, 6, 2]\n",
      "episode 522, reward 842.0, memory_length 2000, epsilon 0.07268942442628039, time 724.0, rides 121\n",
      "Initial State is  [3, 1, 0]\n",
      "episode 523, reward 765.0, memory_length 2000, epsilon 0.07232597730414898, time 728.0, rides 121\n",
      "Initial State is  [0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 524, reward 921.0, memory_length 2000, epsilon 0.07196434741762824, time 728.0, rides 117\n",
      "Initial State is  [3, 12, 2]\n",
      "episode 525, reward 909.0, memory_length 2000, epsilon 0.0716045256805401, time 730.0, rides 127\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 526, reward 1044.0, memory_length 2000, epsilon 0.0712465030521374, time 726.0, rides 132\n",
      "Initial State is  [3, 23, 1]\n",
      "episode 527, reward 1082.0, memory_length 2000, epsilon 0.0708902705368767, time 729.0, rides 129\n",
      "Initial State is  [4, 4, 0]\n",
      "episode 528, reward 1153.0, memory_length 2000, epsilon 0.07053581918419231, time 725.0, rides 116\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 529, reward 1175.0, memory_length 2000, epsilon 0.07018314008827135, time 722.0, rides 127\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 530, reward 759.0, memory_length 2000, epsilon 0.06983222438783, time 726.0, rides 123\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 531, reward 1136.0, memory_length 2000, epsilon 0.06948306326589085, time 730.0, rides 125\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 532, reward 1462.0, memory_length 2000, epsilon 0.0691356479495614, time 728.0, rides 121\n",
      "Initial State is  [3, 1, 0]\n",
      "episode 533, reward 1025.0, memory_length 2000, epsilon 0.06878996970981359, time 724.0, rides 122\n",
      "Initial State is  [1, 3, 6]\n",
      "episode 534, reward 1119.0, memory_length 2000, epsilon 0.06844601986126451, time 737.0, rides 123\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 535, reward 1042.0, memory_length 2000, epsilon 0.06810378976195819, time 731.0, rides 140\n",
      "Initial State is  [3, 5, 3]\n",
      "episode 536, reward 1362.0, memory_length 2000, epsilon 0.0677632708131484, time 728.0, rides 133\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 537, reward 1210.0, memory_length 2000, epsilon 0.06742445445908266, time 730.0, rides 121\n",
      "Initial State is  [2, 2, 4]\n",
      "episode 538, reward 1048.0, memory_length 2000, epsilon 0.06708733218678724, time 732.0, rides 117\n",
      "Initial State is  [1, 9, 4]\n",
      "episode 539, reward 1203.0, memory_length 2000, epsilon 0.0667518955258533, time 726.0, rides 117\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 540, reward 901.0, memory_length 2000, epsilon 0.06641813604822402, time 732.0, rides 110\n",
      "Initial State is  [1, 5, 4]\n",
      "episode 541, reward 837.0, memory_length 2000, epsilon 0.0660860453679829, time 727.0, rides 109\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 542, reward 783.0, memory_length 2000, epsilon 0.06575561514114299, time 727.0, rides 118\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 543, reward 1044.0, memory_length 2000, epsilon 0.06542683706543727, time 728.0, rides 127\n",
      "Initial State is  [3, 15, 2]\n",
      "episode 544, reward 1198.0, memory_length 2000, epsilon 0.06509970288011008, time 729.0, rides 126\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 545, reward 1268.0, memory_length 2000, epsilon 0.06477420436570952, time 726.0, rides 132\n",
      "Initial State is  [0, 10, 2]\n",
      "episode 546, reward 891.0, memory_length 2000, epsilon 0.06445033334388098, time 735.0, rides 122\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 547, reward 1076.0, memory_length 2000, epsilon 0.06412808167716157, time 725.0, rides 123\n",
      "Initial State is  [4, 20, 1]\n",
      "episode 548, reward 822.0, memory_length 2000, epsilon 0.06380744126877576, time 724.0, rides 111\n",
      "Initial State is  [3, 10, 6]\n",
      "episode 549, reward 994.0, memory_length 2000, epsilon 0.06348840406243188, time 726.0, rides 127\n",
      "Initial State is  [3, 2, 3]\n",
      "episode 550, reward 728.0, memory_length 2000, epsilon 0.06317096204211972, time 725.0, rides 118\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 551, reward 998.0, memory_length 2000, epsilon 0.06285510723190912, time 728.0, rides 134\n",
      "Initial State is  [3, 8, 2]\n",
      "episode 552, reward 1288.0, memory_length 2000, epsilon 0.06254083169574957, time 724.0, rides 121\n",
      "Initial State is  [2, 18, 5]\n",
      "episode 553, reward 1198.0, memory_length 2000, epsilon 0.062228127537270826, time 737.0, rides 118\n",
      "Initial State is  [3, 17, 5]\n",
      "episode 554, reward 1185.0, memory_length 2000, epsilon 0.06191698689958447, time 728.0, rides 123\n",
      "Initial State is  [3, 11, 5]\n",
      "episode 555, reward 1301.0, memory_length 2000, epsilon 0.061607401965086545, time 732.0, rides 133\n",
      "Initial State is  [4, 4, 0]\n",
      "episode 556, reward 995.0, memory_length 2000, epsilon 0.06129936495526111, time 721.0, rides 123\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 557, reward 1051.0, memory_length 2000, epsilon 0.0609928681304848, time 728.0, rides 120\n",
      "Initial State is  [1, 9, 0]\n",
      "episode 558, reward 945.0, memory_length 2000, epsilon 0.060687903789832374, time 722.0, rides 124\n",
      "Initial State is  [4, 16, 4]\n",
      "episode 559, reward 1093.0, memory_length 2000, epsilon 0.06038446427088321, time 724.0, rides 127\n",
      "Initial State is  [3, 21, 2]\n",
      "episode 560, reward 1320.0, memory_length 2000, epsilon 0.06008254194952879, time 737.0, rides 133\n",
      "Initial State is  [3, 4, 5]\n",
      "episode 561, reward 1159.0, memory_length 2000, epsilon 0.05978212923978115, time 722.0, rides 122\n",
      "Initial State is  [4, 17, 6]\n",
      "episode 562, reward 636.0, memory_length 2000, epsilon 0.05948321859358224, time 728.0, rides 116\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 563, reward 1036.0, memory_length 2000, epsilon 0.05918580250061433, time 727.0, rides 121\n",
      "Initial State is  [2, 18, 3]\n",
      "episode 564, reward 1033.0, memory_length 2000, epsilon 0.058889873488111255, time 732.0, rides 122\n",
      "Initial State is  [3, 17, 1]\n",
      "episode 565, reward 1181.0, memory_length 2000, epsilon 0.058595424120670696, time 731.0, rides 123\n",
      "Initial State is  [3, 11, 2]\n",
      "episode 566, reward 904.0, memory_length 2000, epsilon 0.05830244700006734, time 729.0, rides 130\n",
      "Initial State is  [0, 14, 5]\n",
      "episode 567, reward 1008.0, memory_length 2000, epsilon 0.058010934765067, time 728.0, rides 126\n",
      "Initial State is  [4, 17, 2]\n",
      "episode 568, reward 1062.0, memory_length 2000, epsilon 0.05772088009124167, time 730.0, rides 129\n",
      "Initial State is  [1, 11, 1]\n",
      "episode 569, reward 1161.0, memory_length 2000, epsilon 0.05743227569078546, time 730.0, rides 122\n",
      "Initial State is  [1, 10, 4]\n",
      "episode 570, reward 1271.0, memory_length 2000, epsilon 0.05714511431233153, time 731.0, rides 127\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 571, reward 960.0, memory_length 2000, epsilon 0.05685938874076987, time 732.0, rides 114\n",
      "Initial State is  [2, 5, 5]\n",
      "episode 572, reward 937.0, memory_length 2000, epsilon 0.056575091797066025, time 727.0, rides 125\n",
      "Initial State is  [1, 18, 3]\n",
      "episode 573, reward 1252.0, memory_length 2000, epsilon 0.056292216338080694, time 734.0, rides 124\n",
      "Initial State is  [3, 5, 1]\n",
      "episode 574, reward 1122.0, memory_length 2000, epsilon 0.05601075525639029, time 739.0, rides 120\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 575, reward 1168.0, memory_length 2000, epsilon 0.05573070148010834, time 734.0, rides 123\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 576, reward 1067.0, memory_length 2000, epsilon 0.0554520479727078, time 724.0, rides 116\n",
      "Initial State is  [1, 21, 4]\n",
      "episode 577, reward 1147.0, memory_length 2000, epsilon 0.05517478773284426, time 728.0, rides 134\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 578, reward 993.0, memory_length 2000, epsilon 0.05489891379418004, time 732.0, rides 123\n",
      "Initial State is  [4, 10, 3]\n",
      "episode 579, reward 870.0, memory_length 2000, epsilon 0.05462441922520914, time 727.0, rides 126\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 580, reward 863.0, memory_length 2000, epsilon 0.0543512971290831, time 728.0, rides 120\n",
      "Initial State is  [0, 16, 3]\n",
      "episode 581, reward 828.0, memory_length 2000, epsilon 0.05407954064343768, time 730.0, rides 122\n",
      "Initial State is  [1, 11, 0]\n",
      "episode 582, reward 1052.0, memory_length 2000, epsilon 0.05380914294022049, time 739.0, rides 120\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 583, reward 1011.0, memory_length 2000, epsilon 0.05354009722551939, time 732.0, rides 119\n",
      "Initial State is  [2, 19, 4]\n",
      "episode 584, reward 904.0, memory_length 2000, epsilon 0.05327239673939179, time 725.0, rides 120\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 585, reward 857.0, memory_length 2000, epsilon 0.053006034755694834, time 724.0, rides 118\n",
      "Initial State is  [3, 13, 0]\n",
      "episode 586, reward 965.0, memory_length 2000, epsilon 0.052741004581916356, time 740.0, rides 131\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 587, reward 903.0, memory_length 2000, epsilon 0.052477299559006776, time 726.0, rides 129\n",
      "Initial State is  [4, 14, 5]\n",
      "episode 588, reward 707.0, memory_length 2000, epsilon 0.052214913061211746, time 723.0, rides 111\n",
      "Initial State is  [0, 2, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 589, reward 807.0, memory_length 2000, epsilon 0.05195383849590569, time 728.0, rides 140\n",
      "Initial State is  [3, 10, 4]\n",
      "episode 590, reward 1150.0, memory_length 2000, epsilon 0.05169406930342616, time 724.0, rides 128\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 591, reward 1083.0, memory_length 2000, epsilon 0.05143559895690903, time 734.0, rides 120\n",
      "Initial State is  [0, 17, 2]\n",
      "episode 592, reward 1113.0, memory_length 2000, epsilon 0.051178420962124486, time 728.0, rides 122\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 593, reward 1068.0, memory_length 2000, epsilon 0.05092252885731386, time 733.0, rides 122\n",
      "Initial State is  [1, 21, 2]\n",
      "episode 594, reward 1207.0, memory_length 2000, epsilon 0.05066791621302729, time 728.0, rides 123\n",
      "Initial State is  [1, 20, 3]\n",
      "episode 595, reward 781.0, memory_length 2000, epsilon 0.05041457663196215, time 728.0, rides 116\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 596, reward 817.0, memory_length 2000, epsilon 0.050162503748802344, time 728.0, rides 123\n",
      "Initial State is  [4, 10, 6]\n",
      "episode 597, reward 1035.0, memory_length 2000, epsilon 0.049911691230058335, time 730.0, rides 123\n",
      "Initial State is  [0, 23, 5]\n",
      "episode 598, reward 1114.0, memory_length 2000, epsilon 0.04966213277390804, time 724.0, rides 118\n",
      "Initial State is  [1, 12, 4]\n",
      "episode 599, reward 1109.0, memory_length 2000, epsilon 0.0494138221100385, time 728.0, rides 127\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 600, reward 1060.0, memory_length 2000, epsilon 0.04916675299948831, time 727.0, rides 125\n",
      "Initial State is  [3, 3, 6]\n",
      "episode 601, reward 936.0, memory_length 2000, epsilon 0.04892091923449087, time 730.0, rides 125\n",
      "Initial State is  [4, 10, 3]\n",
      "episode 602, reward 1028.0, memory_length 2000, epsilon 0.04867631463831842, time 730.0, rides 120\n",
      "Initial State is  [3, 10, 6]\n",
      "episode 603, reward 1019.0, memory_length 2000, epsilon 0.048432933065126825, time 736.0, rides 129\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 604, reward 793.0, memory_length 2000, epsilon 0.048190768399801194, time 729.0, rides 122\n",
      "Initial State is  [2, 21, 0]\n",
      "episode 605, reward 1215.0, memory_length 2000, epsilon 0.04794981455780219, time 730.0, rides 125\n",
      "Initial State is  [0, 9, 3]\n",
      "episode 606, reward 1168.0, memory_length 2000, epsilon 0.04771006548501318, time 730.0, rides 134\n",
      "Initial State is  [2, 18, 1]\n",
      "episode 607, reward 749.0, memory_length 2000, epsilon 0.047471515157588115, time 730.0, rides 119\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 608, reward 1112.0, memory_length 2000, epsilon 0.047234157581800176, time 725.0, rides 124\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 609, reward 999.0, memory_length 2000, epsilon 0.046997986793891174, time 731.0, rides 122\n",
      "Initial State is  [3, 5, 3]\n",
      "episode 610, reward 983.0, memory_length 2000, epsilon 0.04676299685992172, time 729.0, rides 125\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 611, reward 751.0, memory_length 2000, epsilon 0.04652918187562211, time 733.0, rides 121\n",
      "Initial State is  [4, 19, 3]\n",
      "episode 612, reward 1042.0, memory_length 2000, epsilon 0.046296535966244, time 728.0, rides 116\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 613, reward 1179.0, memory_length 2000, epsilon 0.046065053286412784, time 740.0, rides 123\n",
      "Initial State is  [2, 11, 1]\n",
      "episode 614, reward 1050.0, memory_length 2000, epsilon 0.04583472801998072, time 726.0, rides 133\n",
      "Initial State is  [2, 0, 0]\n",
      "episode 615, reward 1249.0, memory_length 2000, epsilon 0.045605554379880814, time 731.0, rides 130\n",
      "Initial State is  [1, 5, 1]\n",
      "episode 616, reward 986.0, memory_length 2000, epsilon 0.04537752660798141, time 727.0, rides 127\n",
      "Initial State is  [0, 0, 1]\n",
      "episode 617, reward 821.0, memory_length 2000, epsilon 0.0451506389749415, time 736.0, rides 125\n",
      "Initial State is  [4, 11, 4]\n",
      "episode 618, reward 894.0, memory_length 2000, epsilon 0.044924885780066794, time 726.0, rides 123\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 619, reward 1046.0, memory_length 2000, epsilon 0.04470026135116646, time 738.0, rides 126\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 620, reward 1056.0, memory_length 2000, epsilon 0.04447676004441063, time 727.0, rides 123\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 621, reward 636.0, memory_length 2000, epsilon 0.04425437624418858, time 731.0, rides 125\n",
      "Initial State is  [3, 13, 0]\n",
      "episode 622, reward 1280.0, memory_length 2000, epsilon 0.04403310436296763, time 730.0, rides 126\n",
      "Initial State is  [2, 2, 5]\n",
      "episode 623, reward 1025.0, memory_length 2000, epsilon 0.043812938841152796, time 734.0, rides 118\n",
      "Initial State is  [2, 5, 1]\n",
      "episode 624, reward 1250.0, memory_length 2000, epsilon 0.04359387414694703, time 732.0, rides 141\n",
      "Initial State is  [0, 1, 5]\n",
      "episode 625, reward 970.0, memory_length 2000, epsilon 0.043375904776212296, time 727.0, rides 124\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 626, reward 1038.0, memory_length 2000, epsilon 0.043159025252331236, time 728.0, rides 123\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 627, reward 934.0, memory_length 2000, epsilon 0.04294323012606958, time 727.0, rides 118\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 628, reward 732.0, memory_length 2000, epsilon 0.04272851397543923, time 730.0, rides 128\n",
      "Initial State is  [3, 19, 6]\n",
      "episode 629, reward 845.0, memory_length 2000, epsilon 0.04251487140556204, time 737.0, rides 121\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 630, reward 1110.0, memory_length 2000, epsilon 0.04230229704853423, time 732.0, rides 135\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 631, reward 1190.0, memory_length 2000, epsilon 0.04209078556329156, time 724.0, rides 131\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 632, reward 933.0, memory_length 2000, epsilon 0.0418803316354751, time 729.0, rides 115\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 633, reward 984.0, memory_length 2000, epsilon 0.041670929977297724, time 733.0, rides 121\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 634, reward 1013.0, memory_length 2000, epsilon 0.04146257532741124, time 734.0, rides 116\n",
      "Initial State is  [1, 9, 2]\n",
      "episode 635, reward 1159.0, memory_length 2000, epsilon 0.04125526245077418, time 734.0, rides 133\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 636, reward 1204.0, memory_length 2000, epsilon 0.04104898613852031, time 734.0, rides 125\n",
      "Initial State is  [0, 4, 0]\n",
      "episode 637, reward 1074.0, memory_length 2000, epsilon 0.04084374120782771, time 726.0, rides 122\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 638, reward 1036.0, memory_length 2000, epsilon 0.04063952250178857, time 727.0, rides 125\n",
      "Initial State is  [2, 23, 2]\n",
      "episode 639, reward 1141.0, memory_length 2000, epsilon 0.04043632488927963, time 730.0, rides 126\n",
      "Initial State is  [0, 20, 3]\n",
      "episode 640, reward 1080.0, memory_length 2000, epsilon 0.04023414326483323, time 729.0, rides 124\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 641, reward 1361.0, memory_length 2000, epsilon 0.040032972548509065, time 727.0, rides 130\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 642, reward 996.0, memory_length 2000, epsilon 0.03983280768576652, time 733.0, rides 122\n",
      "Initial State is  [1, 20, 4]\n",
      "episode 643, reward 1154.0, memory_length 2000, epsilon 0.03963364364733769, time 729.0, rides 125\n",
      "Initial State is  [3, 17, 2]\n",
      "episode 644, reward 974.0, memory_length 2000, epsilon 0.039435475429100995, time 723.0, rides 122\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 645, reward 1130.0, memory_length 2000, epsilon 0.03923829805195549, time 726.0, rides 121\n",
      "Initial State is  [4, 20, 5]\n",
      "episode 646, reward 1078.0, memory_length 2000, epsilon 0.03904210656169572, time 737.0, rides 125\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 647, reward 1193.0, memory_length 2000, epsilon 0.03884689602888724, time 727.0, rides 124\n",
      "Initial State is  [2, 23, 6]\n",
      "episode 648, reward 1290.0, memory_length 2000, epsilon 0.0386526615487428, time 728.0, rides 117\n",
      "Initial State is  [0, 21, 2]\n",
      "episode 649, reward 925.0, memory_length 2000, epsilon 0.03845939824099909, time 739.0, rides 117\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 650, reward 799.0, memory_length 2000, epsilon 0.03826710124979409, time 724.0, rides 121\n",
      "Initial State is  [4, 19, 5]\n",
      "episode 651, reward 1082.0, memory_length 2000, epsilon 0.038075765743545126, time 728.0, rides 116\n",
      "Initial State is  [1, 5, 3]\n",
      "episode 652, reward 968.0, memory_length 2000, epsilon 0.0378853869148274, time 732.0, rides 126\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 653, reward 792.0, memory_length 2000, epsilon 0.03769595998025326, time 733.0, rides 113\n",
      "Initial State is  [0, 3, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 654, reward 725.0, memory_length 2000, epsilon 0.03750748018035199, time 727.0, rides 121\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 655, reward 900.0, memory_length 2000, epsilon 0.037319942779450235, time 734.0, rides 124\n",
      "Initial State is  [3, 7, 3]\n",
      "episode 656, reward 1056.0, memory_length 2000, epsilon 0.037133343065552986, time 730.0, rides 124\n",
      "Initial State is  [1, 21, 3]\n",
      "episode 657, reward 1062.0, memory_length 2000, epsilon 0.03694767635022522, time 731.0, rides 123\n",
      "Initial State is  [4, 4, 5]\n",
      "episode 658, reward 957.0, memory_length 2000, epsilon 0.036762937968474095, time 744.0, rides 119\n",
      "Initial State is  [0, 21, 6]\n",
      "episode 659, reward 1191.0, memory_length 2000, epsilon 0.03657912327863173, time 726.0, rides 119\n",
      "Initial State is  [0, 3, 2]\n",
      "episode 660, reward 1266.0, memory_length 2000, epsilon 0.036396227662238566, time 728.0, rides 121\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 661, reward 1177.0, memory_length 2000, epsilon 0.03621424652392737, time 732.0, rides 133\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 662, reward 921.0, memory_length 2000, epsilon 0.036033175291307735, time 722.0, rides 112\n",
      "Initial State is  [3, 10, 1]\n",
      "episode 663, reward 965.0, memory_length 2000, epsilon 0.03585300941485119, time 739.0, rides 137\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 664, reward 1091.0, memory_length 2000, epsilon 0.035673744367776934, time 726.0, rides 132\n",
      "Initial State is  [4, 21, 1]\n",
      "episode 665, reward 1263.0, memory_length 2000, epsilon 0.03549537564593805, time 733.0, rides 123\n",
      "Initial State is  [0, 2, 1]\n",
      "episode 666, reward 1159.0, memory_length 2000, epsilon 0.035317898767708356, time 725.0, rides 125\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 667, reward 829.0, memory_length 2000, epsilon 0.03514130927386981, time 726.0, rides 131\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 668, reward 1286.0, memory_length 2000, epsilon 0.03496560272750046, time 735.0, rides 132\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 669, reward 839.0, memory_length 2000, epsilon 0.03479077471386296, time 724.0, rides 121\n",
      "Initial State is  [3, 17, 5]\n",
      "episode 670, reward 1082.0, memory_length 2000, epsilon 0.03461682084029365, time 730.0, rides 126\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 671, reward 1123.0, memory_length 2000, epsilon 0.034443736736092176, time 733.0, rides 120\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 672, reward 902.0, memory_length 2000, epsilon 0.034271518052411715, time 732.0, rides 123\n",
      "Initial State is  [0, 19, 0]\n",
      "episode 673, reward 1082.0, memory_length 2000, epsilon 0.034100160462149656, time 729.0, rides 128\n",
      "Initial State is  [0, 0, 1]\n",
      "episode 674, reward 1244.0, memory_length 2000, epsilon 0.03392965965983891, time 726.0, rides 120\n",
      "Initial State is  [4, 19, 5]\n",
      "episode 675, reward 1454.0, memory_length 2000, epsilon 0.033760011361539714, time 729.0, rides 125\n",
      "Initial State is  [3, 17, 6]\n",
      "episode 676, reward 1206.0, memory_length 2000, epsilon 0.03359121130473201, time 732.0, rides 128\n",
      "Initial State is  [3, 19, 2]\n",
      "episode 677, reward 914.0, memory_length 2000, epsilon 0.033423255248208356, time 730.0, rides 119\n",
      "Initial State is  [0, 3, 5]\n",
      "episode 678, reward 1116.0, memory_length 2000, epsilon 0.03325613897196732, time 731.0, rides 133\n",
      "Initial State is  [0, 7, 4]\n",
      "episode 679, reward 1092.0, memory_length 2000, epsilon 0.03308985827710748, time 722.0, rides 120\n",
      "Initial State is  [2, 0, 1]\n",
      "episode 680, reward 846.0, memory_length 2000, epsilon 0.032924408985721944, time 725.0, rides 122\n",
      "Initial State is  [0, 12, 1]\n",
      "episode 681, reward 992.0, memory_length 2000, epsilon 0.03275978694079333, time 737.0, rides 129\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 682, reward 1057.0, memory_length 2000, epsilon 0.032595988006089364, time 735.0, rides 121\n",
      "Initial State is  [0, 9, 2]\n",
      "episode 683, reward 903.0, memory_length 2000, epsilon 0.032433008066058915, time 728.0, rides 121\n",
      "Initial State is  [0, 0, 6]\n",
      "episode 684, reward 1134.0, memory_length 2000, epsilon 0.03227084302572862, time 726.0, rides 114\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 685, reward 1056.0, memory_length 2000, epsilon 0.032109488810599975, time 735.0, rides 122\n",
      "Initial State is  [2, 1, 0]\n",
      "episode 686, reward 1047.0, memory_length 2000, epsilon 0.031948941366546975, time 734.0, rides 110\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 687, reward 1189.0, memory_length 2000, epsilon 0.03178919665971424, time 733.0, rides 129\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 688, reward 1077.0, memory_length 2000, epsilon 0.03163025067641567, time 739.0, rides 127\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 689, reward 899.0, memory_length 2000, epsilon 0.03147209942303359, time 733.0, rides 114\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 690, reward 1017.0, memory_length 2000, epsilon 0.03131473892591842, time 721.0, rides 120\n",
      "Initial State is  [0, 9, 0]\n",
      "episode 691, reward 1016.0, memory_length 2000, epsilon 0.031158165231288826, time 734.0, rides 123\n",
      "Initial State is  [0, 23, 0]\n",
      "episode 692, reward 952.0, memory_length 2000, epsilon 0.03100237440513238, time 727.0, rides 113\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 693, reward 1358.0, memory_length 2000, epsilon 0.030847362533106718, time 729.0, rides 127\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 694, reward 1267.0, memory_length 2000, epsilon 0.030693125720441184, time 726.0, rides 124\n",
      "Initial State is  [4, 20, 5]\n",
      "episode 695, reward 1091.0, memory_length 2000, epsilon 0.030539660091838977, time 746.0, rides 127\n",
      "Initial State is  [3, 16, 1]\n",
      "episode 696, reward 1091.0, memory_length 2000, epsilon 0.03038696179137978, time 726.0, rides 120\n",
      "Initial State is  [2, 23, 1]\n",
      "episode 697, reward 965.0, memory_length 2000, epsilon 0.030235026982422884, time 724.0, rides 120\n",
      "Initial State is  [0, 0, 2]\n",
      "episode 698, reward 1398.0, memory_length 2000, epsilon 0.030083851847510768, time 733.0, rides 128\n",
      "Initial State is  [2, 16, 4]\n",
      "episode 699, reward 1147.0, memory_length 2000, epsilon 0.029933432588273214, time 734.0, rides 121\n",
      "Initial State is  [3, 17, 3]\n",
      "episode 700, reward 880.0, memory_length 2000, epsilon 0.029783765425331846, time 723.0, rides 131\n",
      "Initial State is  [2, 22, 5]\n",
      "episode 701, reward 988.0, memory_length 2000, epsilon 0.029634846598205186, time 729.0, rides 118\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 702, reward 1197.0, memory_length 2000, epsilon 0.02948667236521416, time 731.0, rides 123\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 703, reward 1241.0, memory_length 2000, epsilon 0.029339239003388088, time 731.0, rides 143\n",
      "Initial State is  [1, 17, 1]\n",
      "episode 704, reward 809.0, memory_length 2000, epsilon 0.029192542808371146, time 728.0, rides 121\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 705, reward 845.0, memory_length 2000, epsilon 0.02904658009432929, time 730.0, rides 124\n",
      "Initial State is  [4, 7, 1]\n",
      "episode 706, reward 1137.0, memory_length 2000, epsilon 0.028901347193857643, time 733.0, rides 141\n",
      "Initial State is  [0, 21, 5]\n",
      "episode 707, reward 1138.0, memory_length 2000, epsilon 0.028756840457888354, time 732.0, rides 124\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 708, reward 1113.0, memory_length 2000, epsilon 0.02861305625559891, time 728.0, rides 135\n",
      "Initial State is  [4, 7, 2]\n",
      "episode 709, reward 906.0, memory_length 2000, epsilon 0.028469990974320916, time 725.0, rides 116\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 710, reward 1193.0, memory_length 2000, epsilon 0.02832764101944931, time 726.0, rides 125\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 711, reward 935.0, memory_length 2000, epsilon 0.028186002814352063, time 734.0, rides 125\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 712, reward 976.0, memory_length 2000, epsilon 0.0280450728002803, time 730.0, rides 121\n",
      "Initial State is  [2, 19, 3]\n",
      "episode 713, reward 906.0, memory_length 2000, epsilon 0.0279048474362789, time 726.0, rides 123\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 714, reward 1050.0, memory_length 2000, epsilon 0.027765323199097504, time 733.0, rides 120\n",
      "Initial State is  [3, 3, 1]\n",
      "episode 715, reward 1030.0, memory_length 2000, epsilon 0.027626496583102015, time 730.0, rides 126\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 716, reward 1153.0, memory_length 2000, epsilon 0.027488364100186506, time 730.0, rides 127\n",
      "Initial State is  [1, 17, 1]\n",
      "episode 717, reward 816.0, memory_length 2000, epsilon 0.027350922279685573, time 728.0, rides 113\n",
      "Initial State is  [3, 1, 2]\n",
      "episode 718, reward 1286.0, memory_length 2000, epsilon 0.027214167668287145, time 734.0, rides 127\n",
      "Initial State is  [2, 1, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 719, reward 1278.0, memory_length 2000, epsilon 0.02707809682994571, time 726.0, rides 120\n",
      "Initial State is  [2, 22, 5]\n",
      "episode 720, reward 1219.0, memory_length 2000, epsilon 0.02694270634579598, time 734.0, rides 122\n",
      "Initial State is  [3, 7, 3]\n",
      "episode 721, reward 709.0, memory_length 2000, epsilon 0.026807992814067, time 740.0, rides 125\n",
      "Initial State is  [0, 20, 5]\n",
      "episode 722, reward 886.0, memory_length 2000, epsilon 0.026673952849996664, time 732.0, rides 111\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 723, reward 1334.0, memory_length 2000, epsilon 0.02654058308574668, time 736.0, rides 118\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 724, reward 1191.0, memory_length 2000, epsilon 0.026407880170317945, time 729.0, rides 118\n",
      "Initial State is  [3, 7, 4]\n",
      "episode 725, reward 1074.0, memory_length 2000, epsilon 0.026275840769466357, time 726.0, rides 117\n",
      "Initial State is  [2, 19, 4]\n",
      "episode 726, reward 1046.0, memory_length 2000, epsilon 0.026144461565619025, time 727.0, rides 129\n",
      "Initial State is  [3, 18, 4]\n",
      "episode 727, reward 1154.0, memory_length 2000, epsilon 0.02601373925779093, time 732.0, rides 132\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 728, reward 1260.0, memory_length 2000, epsilon 0.025883670561501974, time 723.0, rides 120\n",
      "Initial State is  [3, 3, 3]\n",
      "episode 729, reward 1016.0, memory_length 2000, epsilon 0.025754252208694463, time 735.0, rides 116\n",
      "Initial State is  [3, 5, 2]\n",
      "episode 730, reward 1330.0, memory_length 2000, epsilon 0.02562548094765099, time 725.0, rides 123\n",
      "Initial State is  [1, 21, 6]\n",
      "episode 731, reward 1012.0, memory_length 2000, epsilon 0.025497353542912736, time 729.0, rides 115\n",
      "Initial State is  [3, 2, 3]\n",
      "episode 732, reward 645.0, memory_length 2000, epsilon 0.02536986677519817, time 727.0, rides 113\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 733, reward 1106.0, memory_length 2000, epsilon 0.02524301744132218, time 733.0, rides 137\n",
      "Initial State is  [4, 12, 1]\n",
      "episode 734, reward 1094.0, memory_length 2000, epsilon 0.025116802354115567, time 722.0, rides 112\n",
      "Initial State is  [1, 9, 6]\n",
      "episode 735, reward 1039.0, memory_length 2000, epsilon 0.024991218342344988, time 732.0, rides 119\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 736, reward 892.0, memory_length 2000, epsilon 0.024866262250633264, time 728.0, rides 114\n",
      "Initial State is  [3, 15, 0]\n",
      "episode 737, reward 1120.0, memory_length 2000, epsilon 0.024741930939380097, time 724.0, rides 127\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 738, reward 1021.0, memory_length 2000, epsilon 0.024618221284683196, time 730.0, rides 126\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 739, reward 1303.0, memory_length 2000, epsilon 0.02449513017825978, time 736.0, rides 128\n",
      "Initial State is  [4, 8, 3]\n",
      "episode 740, reward 1031.0, memory_length 2000, epsilon 0.02437265452736848, time 734.0, rides 127\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 741, reward 849.0, memory_length 2000, epsilon 0.024250791254731636, time 725.0, rides 131\n",
      "Initial State is  [0, 22, 3]\n",
      "episode 742, reward 884.0, memory_length 2000, epsilon 0.024129537298457977, time 726.0, rides 118\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 743, reward 1327.0, memory_length 2000, epsilon 0.024008889611965685, time 734.0, rides 125\n",
      "Initial State is  [1, 13, 1]\n",
      "episode 744, reward 1120.0, memory_length 2000, epsilon 0.023888845163905856, time 727.0, rides 128\n",
      "Initial State is  [1, 5, 3]\n",
      "episode 745, reward 948.0, memory_length 2000, epsilon 0.023769400938086327, time 730.0, rides 133\n",
      "Initial State is  [0, 0, 6]\n",
      "episode 746, reward 883.0, memory_length 2000, epsilon 0.023650553933395897, time 738.0, rides 118\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 747, reward 1165.0, memory_length 2000, epsilon 0.023532301163728918, time 728.0, rides 118\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 748, reward 1179.0, memory_length 2000, epsilon 0.023414639657910272, time 729.0, rides 126\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 749, reward 1154.0, memory_length 2000, epsilon 0.023297566459620722, time 725.0, rides 127\n",
      "Initial State is  [0, 15, 0]\n",
      "episode 750, reward 983.0, memory_length 2000, epsilon 0.023181078627322618, time 729.0, rides 127\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 751, reward 1442.0, memory_length 2000, epsilon 0.023065173234186005, time 727.0, rides 125\n",
      "Initial State is  [4, 15, 4]\n",
      "episode 752, reward 1217.0, memory_length 2000, epsilon 0.022949847368015076, time 730.0, rides 109\n",
      "Initial State is  [3, 6, 1]\n",
      "episode 753, reward 967.0, memory_length 2000, epsilon 0.022835098131175, time 729.0, rides 129\n",
      "Initial State is  [1, 3, 3]\n",
      "episode 754, reward 1325.0, memory_length 2000, epsilon 0.022720922640519125, time 731.0, rides 123\n",
      "Initial State is  [2, 5, 4]\n",
      "episode 755, reward 1259.0, memory_length 2000, epsilon 0.02260731802731653, time 725.0, rides 117\n",
      "Initial State is  [0, 19, 4]\n",
      "episode 756, reward 1242.0, memory_length 2000, epsilon 0.022494281437179946, time 725.0, rides 124\n",
      "Initial State is  [1, 15, 1]\n",
      "episode 757, reward 1251.0, memory_length 2000, epsilon 0.022381810029994047, time 724.0, rides 119\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 758, reward 1121.0, memory_length 2000, epsilon 0.022269900979844076, time 733.0, rides 117\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 759, reward 1078.0, memory_length 2000, epsilon 0.022158551474944856, time 724.0, rides 122\n",
      "Initial State is  [3, 2, 6]\n",
      "episode 760, reward 1294.0, memory_length 2000, epsilon 0.022047758717570132, time 726.0, rides 124\n",
      "Initial State is  [3, 21, 0]\n",
      "episode 761, reward 1321.0, memory_length 2000, epsilon 0.021937519923982282, time 731.0, rides 125\n",
      "Initial State is  [4, 10, 6]\n",
      "episode 762, reward 1159.0, memory_length 2000, epsilon 0.021827832324362372, time 722.0, rides 125\n",
      "Initial State is  [4, 12, 2]\n",
      "episode 763, reward 1015.0, memory_length 2000, epsilon 0.02171869316274056, time 727.0, rides 119\n",
      "Initial State is  [1, 11, 0]\n",
      "episode 764, reward 1316.0, memory_length 2000, epsilon 0.021610099696926857, time 723.0, rides 123\n",
      "Initial State is  [3, 0, 0]\n",
      "episode 765, reward 1213.0, memory_length 2000, epsilon 0.021502049198442223, time 733.0, rides 125\n",
      "Initial State is  [1, 5, 5]\n",
      "episode 766, reward 917.0, memory_length 2000, epsilon 0.021394538952450012, time 728.0, rides 116\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 767, reward 1082.0, memory_length 2000, epsilon 0.02128756625768776, time 731.0, rides 119\n",
      "Initial State is  [0, 14, 5]\n",
      "episode 768, reward 785.0, memory_length 2000, epsilon 0.021181128426399323, time 732.0, rides 120\n",
      "Initial State is  [2, 1, 4]\n",
      "episode 769, reward 966.0, memory_length 2000, epsilon 0.021075222784267326, time 728.0, rides 128\n",
      "Initial State is  [2, 23, 1]\n",
      "episode 770, reward 1122.0, memory_length 2000, epsilon 0.020969846670345987, time 734.0, rides 125\n",
      "Initial State is  [3, 5, 2]\n",
      "episode 771, reward 976.0, memory_length 2000, epsilon 0.020864997436994256, time 723.0, rides 130\n",
      "Initial State is  [3, 2, 2]\n",
      "episode 772, reward 984.0, memory_length 2000, epsilon 0.020760672449809284, time 733.0, rides 117\n",
      "Initial State is  [1, 10, 1]\n",
      "episode 773, reward 780.0, memory_length 2000, epsilon 0.020656869087560238, time 736.0, rides 130\n",
      "Initial State is  [3, 13, 0]\n",
      "episode 774, reward 922.0, memory_length 2000, epsilon 0.020553584742122436, time 724.0, rides 109\n",
      "Initial State is  [4, 16, 1]\n",
      "episode 775, reward 1078.0, memory_length 2000, epsilon 0.020450816818411825, time 728.0, rides 125\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 776, reward 1048.0, memory_length 2000, epsilon 0.020348562734319765, time 728.0, rides 111\n",
      "Initial State is  [0, 8, 2]\n",
      "episode 777, reward 1013.0, memory_length 2000, epsilon 0.020246819920648168, time 727.0, rides 122\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 778, reward 1101.0, memory_length 2000, epsilon 0.020145585821044927, time 727.0, rides 117\n",
      "Initial State is  [0, 17, 5]\n",
      "episode 779, reward 881.0, memory_length 2000, epsilon 0.020044857891939702, time 732.0, rides 114\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 780, reward 794.0, memory_length 2000, epsilon 0.019944633602480003, time 726.0, rides 118\n",
      "Initial State is  [2, 5, 1]\n",
      "episode 781, reward 1148.0, memory_length 2000, epsilon 0.019844910434467605, time 732.0, rides 123\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 782, reward 1168.0, memory_length 2000, epsilon 0.019745685882295267, time 737.0, rides 122\n",
      "Initial State is  [1, 6, 0]\n",
      "episode 783, reward 977.0, memory_length 2000, epsilon 0.01964695745288379, time 729.0, rides 121\n",
      "Initial State is  [2, 19, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 784, reward 1296.0, memory_length 2000, epsilon 0.01954872266561937, time 723.0, rides 129\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 785, reward 1047.0, memory_length 2000, epsilon 0.019450979052291272, time 736.0, rides 117\n",
      "Initial State is  [4, 12, 0]\n",
      "episode 786, reward 1086.0, memory_length 2000, epsilon 0.019353724157029815, time 733.0, rides 122\n",
      "Initial State is  [0, 20, 0]\n",
      "episode 787, reward 678.0, memory_length 2000, epsilon 0.019256955536244666, time 726.0, rides 124\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 788, reward 1147.0, memory_length 2000, epsilon 0.019160670758563442, time 731.0, rides 122\n",
      "Initial State is  [4, 1, 2]\n",
      "episode 789, reward 992.0, memory_length 2000, epsilon 0.019064867404770626, time 726.0, rides 123\n",
      "Initial State is  [0, 16, 0]\n",
      "episode 790, reward 1163.0, memory_length 2000, epsilon 0.018969543067746772, time 727.0, rides 122\n",
      "Initial State is  [0, 23, 0]\n",
      "episode 791, reward 1122.0, memory_length 2000, epsilon 0.018874695352408037, time 725.0, rides 125\n",
      "Initial State is  [3, 22, 6]\n",
      "episode 792, reward 1074.0, memory_length 2000, epsilon 0.018780321875645996, time 734.0, rides 122\n",
      "Initial State is  [4, 20, 4]\n",
      "episode 793, reward 1259.0, memory_length 2000, epsilon 0.018686420266267767, time 732.0, rides 129\n",
      "Initial State is  [1, 15, 3]\n",
      "episode 794, reward 959.0, memory_length 2000, epsilon 0.018592988164936427, time 733.0, rides 112\n",
      "Initial State is  [2, 4, 2]\n",
      "episode 795, reward 1199.0, memory_length 2000, epsilon 0.018500023224111744, time 728.0, rides 127\n",
      "Initial State is  [0, 1, 5]\n",
      "episode 796, reward 974.0, memory_length 2000, epsilon 0.018407523107991184, time 730.0, rides 128\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 797, reward 1095.0, memory_length 2000, epsilon 0.01831548549245123, time 725.0, rides 120\n",
      "Initial State is  [2, 8, 2]\n",
      "episode 798, reward 796.0, memory_length 2000, epsilon 0.018223908064988973, time 727.0, rides 128\n",
      "Initial State is  [0, 4, 4]\n",
      "episode 799, reward 1499.0, memory_length 2000, epsilon 0.018132788524664028, time 730.0, rides 124\n",
      "Initial State is  [3, 23, 3]\n",
      "episode 800, reward 989.0, memory_length 2000, epsilon 0.018042124582040707, time 731.0, rides 120\n",
      "Initial State is  [1, 14, 6]\n",
      "episode 801, reward 933.0, memory_length 2000, epsilon 0.017951913959130504, time 739.0, rides 132\n",
      "Initial State is  [1, 3, 6]\n",
      "episode 802, reward 1207.0, memory_length 2000, epsilon 0.01786215438933485, time 726.0, rides 128\n",
      "Initial State is  [4, 14, 3]\n",
      "episode 803, reward 1157.0, memory_length 2000, epsilon 0.017772843617388175, time 723.0, rides 121\n",
      "Initial State is  [1, 2, 3]\n",
      "episode 804, reward 1335.0, memory_length 2000, epsilon 0.017683979399301233, time 729.0, rides 129\n",
      "Initial State is  [0, 17, 0]\n",
      "episode 805, reward 1095.0, memory_length 2000, epsilon 0.017595559502304726, time 728.0, rides 116\n",
      "Initial State is  [4, 7, 3]\n",
      "episode 806, reward 838.0, memory_length 2000, epsilon 0.0175075817047932, time 732.0, rides 118\n",
      "Initial State is  [1, 21, 3]\n",
      "episode 807, reward 1204.0, memory_length 2000, epsilon 0.017420043796269234, time 731.0, rides 106\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 808, reward 1096.0, memory_length 2000, epsilon 0.017332943577287888, time 729.0, rides 120\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 809, reward 1270.0, memory_length 2000, epsilon 0.01724627885940145, time 735.0, rides 139\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 810, reward 1322.0, memory_length 2000, epsilon 0.017160047465104442, time 732.0, rides 122\n",
      "Initial State is  [0, 20, 1]\n",
      "episode 811, reward 887.0, memory_length 2000, epsilon 0.01707424722777892, time 723.0, rides 124\n",
      "Initial State is  [4, 15, 2]\n",
      "episode 812, reward 1317.0, memory_length 2000, epsilon 0.016988875991640028, time 728.0, rides 125\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 813, reward 1262.0, memory_length 2000, epsilon 0.016903931611681827, time 728.0, rides 125\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 814, reward 702.0, memory_length 2000, epsilon 0.01681941195362342, time 737.0, rides 116\n",
      "Initial State is  [3, 6, 3]\n",
      "episode 815, reward 1212.0, memory_length 2000, epsilon 0.016735314893855303, time 731.0, rides 132\n",
      "Initial State is  [4, 20, 5]\n",
      "episode 816, reward 1156.0, memory_length 2000, epsilon 0.016651638319386028, time 729.0, rides 126\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 817, reward 1234.0, memory_length 2000, epsilon 0.0165683801277891, time 733.0, rides 110\n",
      "Initial State is  [4, 14, 0]\n",
      "episode 818, reward 916.0, memory_length 2000, epsilon 0.016485538227150154, time 728.0, rides 122\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 819, reward 1079.0, memory_length 2000, epsilon 0.0164031105360144, time 730.0, rides 120\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 820, reward 1300.0, memory_length 2000, epsilon 0.01632109498333433, time 727.0, rides 123\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 821, reward 1040.0, memory_length 2000, epsilon 0.016239489508417658, time 728.0, rides 125\n",
      "Initial State is  [4, 15, 4]\n",
      "episode 822, reward 1523.0, memory_length 2000, epsilon 0.01615829206087557, time 723.0, rides 119\n",
      "Initial State is  [2, 21, 0]\n",
      "episode 823, reward 1050.0, memory_length 2000, epsilon 0.01607750060057119, time 732.0, rides 122\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 824, reward 1134.0, memory_length 2000, epsilon 0.015997113097568336, time 733.0, rides 121\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 825, reward 1070.0, memory_length 2000, epsilon 0.015917127532080494, time 731.0, rides 122\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 826, reward 1004.0, memory_length 2000, epsilon 0.01583754189442009, time 731.0, rides 114\n",
      "Initial State is  [3, 3, 3]\n",
      "episode 827, reward 1261.0, memory_length 2000, epsilon 0.01575835418494799, time 729.0, rides 120\n",
      "Initial State is  [0, 16, 0]\n",
      "episode 828, reward 1057.0, memory_length 2000, epsilon 0.01567956241402325, time 729.0, rides 126\n",
      "Initial State is  [3, 22, 0]\n",
      "episode 829, reward 885.0, memory_length 2000, epsilon 0.015601164601953134, time 729.0, rides 130\n",
      "Initial State is  [1, 12, 0]\n",
      "episode 830, reward 1058.0, memory_length 2000, epsilon 0.015523158778943369, time 735.0, rides 119\n",
      "Initial State is  [4, 7, 3]\n",
      "episode 831, reward 1419.0, memory_length 2000, epsilon 0.015445542985048652, time 731.0, rides 130\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 832, reward 1107.0, memory_length 2000, epsilon 0.015368315270123408, time 735.0, rides 112\n",
      "Initial State is  [2, 18, 0]\n",
      "episode 833, reward 1061.0, memory_length 2000, epsilon 0.01529147369377279, time 725.0, rides 115\n",
      "Initial State is  [4, 23, 0]\n",
      "episode 834, reward 880.0, memory_length 2000, epsilon 0.015215016325303928, time 726.0, rides 126\n",
      "Initial State is  [0, 2, 4]\n",
      "episode 835, reward 861.0, memory_length 2000, epsilon 0.015138941243677408, time 730.0, rides 123\n",
      "Initial State is  [1, 18, 4]\n",
      "episode 836, reward 934.0, memory_length 2000, epsilon 0.01506324653745902, time 729.0, rides 135\n",
      "Initial State is  [4, 21, 4]\n",
      "episode 837, reward 1128.0, memory_length 2000, epsilon 0.014987930304771725, time 726.0, rides 125\n",
      "Initial State is  [4, 22, 1]\n",
      "episode 838, reward 748.0, memory_length 2000, epsilon 0.014912990653247866, time 721.0, rides 107\n",
      "Initial State is  [1, 3, 5]\n",
      "episode 839, reward 1159.0, memory_length 2000, epsilon 0.014838425699981627, time 726.0, rides 136\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 840, reward 1182.0, memory_length 2000, epsilon 0.01476423357148172, time 729.0, rides 122\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 841, reward 829.0, memory_length 2000, epsilon 0.014690412403624311, time 723.0, rides 127\n",
      "Initial State is  [2, 10, 4]\n",
      "episode 842, reward 949.0, memory_length 2000, epsilon 0.01461696034160619, time 730.0, rides 118\n",
      "Initial State is  [3, 11, 2]\n",
      "episode 843, reward 988.0, memory_length 2000, epsilon 0.014543875539898159, time 735.0, rides 116\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 844, reward 1174.0, memory_length 2000, epsilon 0.014471156162198668, time 723.0, rides 119\n",
      "Initial State is  [0, 0, 5]\n",
      "episode 845, reward 1430.0, memory_length 2000, epsilon 0.014398800381387675, time 731.0, rides 129\n",
      "Initial State is  [1, 12, 3]\n",
      "episode 846, reward 1196.0, memory_length 2000, epsilon 0.014326806379480736, time 726.0, rides 125\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 847, reward 1057.0, memory_length 2000, epsilon 0.014255172347583332, time 737.0, rides 130\n",
      "Initial State is  [4, 5, 0]\n",
      "episode 848, reward 1083.0, memory_length 2000, epsilon 0.014183896485845416, time 726.0, rides 119\n",
      "Initial State is  [2, 3, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 849, reward 778.0, memory_length 2000, epsilon 0.014112977003416188, time 734.0, rides 125\n",
      "Initial State is  [2, 8, 6]\n",
      "episode 850, reward 1187.0, memory_length 2000, epsilon 0.014042412118399107, time 726.0, rides 123\n",
      "Initial State is  [3, 2, 4]\n",
      "episode 851, reward 1291.0, memory_length 2000, epsilon 0.013972200057807112, time 731.0, rides 130\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 852, reward 1014.0, memory_length 2000, epsilon 0.013902339057518077, time 736.0, rides 139\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 853, reward 1264.0, memory_length 2000, epsilon 0.013832827362230486, time 729.0, rides 133\n",
      "Initial State is  [2, 20, 6]\n",
      "episode 854, reward 1033.0, memory_length 2000, epsilon 0.013763663225419333, time 727.0, rides 118\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 855, reward 1020.0, memory_length 2000, epsilon 0.013694844909292236, time 728.0, rides 115\n",
      "Initial State is  [3, 14, 2]\n",
      "episode 856, reward 976.0, memory_length 2000, epsilon 0.013626370684745774, time 726.0, rides 123\n",
      "Initial State is  [4, 15, 5]\n",
      "episode 857, reward 1301.0, memory_length 2000, epsilon 0.013558238831322046, time 738.0, rides 115\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 858, reward 1039.0, memory_length 2000, epsilon 0.013490447637165436, time 732.0, rides 130\n",
      "Initial State is  [2, 20, 4]\n",
      "episode 859, reward 935.0, memory_length 2000, epsilon 0.013422995398979608, time 726.0, rides 139\n",
      "Initial State is  [0, 1, 6]\n",
      "episode 860, reward 956.0, memory_length 2000, epsilon 0.01335588042198471, time 730.0, rides 127\n",
      "Initial State is  [1, 2, 0]\n",
      "episode 861, reward 1422.0, memory_length 2000, epsilon 0.013289101019874787, time 727.0, rides 132\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 862, reward 1339.0, memory_length 2000, epsilon 0.013222655514775413, time 728.0, rides 129\n",
      "Initial State is  [3, 19, 4]\n",
      "episode 863, reward 1368.0, memory_length 2000, epsilon 0.013156542237201536, time 728.0, rides 129\n",
      "Initial State is  [0, 21, 4]\n",
      "episode 864, reward 962.0, memory_length 2000, epsilon 0.013090759526015528, time 725.0, rides 136\n",
      "Initial State is  [1, 21, 4]\n",
      "episode 865, reward 818.0, memory_length 2000, epsilon 0.01302530572838545, time 724.0, rides 124\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 866, reward 1242.0, memory_length 2000, epsilon 0.012960179199743523, time 723.0, rides 134\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 867, reward 848.0, memory_length 2000, epsilon 0.012895378303744804, time 731.0, rides 139\n",
      "Initial State is  [3, 17, 4]\n",
      "episode 868, reward 1009.0, memory_length 2000, epsilon 0.01283090141222608, time 733.0, rides 124\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 869, reward 1295.0, memory_length 2000, epsilon 0.012766746905164949, time 724.0, rides 127\n",
      "Initial State is  [3, 13, 0]\n",
      "episode 870, reward 932.0, memory_length 2000, epsilon 0.012702913170639124, time 732.0, rides 125\n",
      "Initial State is  [4, 20, 5]\n",
      "episode 871, reward 865.0, memory_length 2000, epsilon 0.012639398604785928, time 726.0, rides 116\n",
      "Initial State is  [2, 14, 5]\n",
      "episode 872, reward 1207.0, memory_length 2000, epsilon 0.012576201611761997, time 731.0, rides 132\n",
      "Initial State is  [3, 3, 5]\n",
      "episode 873, reward 849.0, memory_length 2000, epsilon 0.012513320603703188, time 728.0, rides 116\n",
      "Initial State is  [0, 12, 6]\n",
      "episode 874, reward 1364.0, memory_length 2000, epsilon 0.012450754000684672, time 726.0, rides 136\n",
      "Initial State is  [2, 3, 1]\n",
      "episode 875, reward 1345.0, memory_length 2000, epsilon 0.012388500230681249, time 728.0, rides 124\n",
      "Initial State is  [4, 19, 4]\n",
      "episode 876, reward 1316.0, memory_length 2000, epsilon 0.012326557729527843, time 732.0, rides 127\n",
      "Initial State is  [2, 23, 5]\n",
      "episode 877, reward 1037.0, memory_length 2000, epsilon 0.012264924940880204, time 731.0, rides 122\n",
      "Initial State is  [2, 19, 6]\n",
      "episode 878, reward 1240.0, memory_length 2000, epsilon 0.012203600316175803, time 735.0, rides 116\n",
      "Initial State is  [0, 20, 4]\n",
      "episode 879, reward 969.0, memory_length 2000, epsilon 0.012142582314594924, time 730.0, rides 121\n",
      "Initial State is  [1, 10, 5]\n",
      "episode 880, reward 1100.0, memory_length 2000, epsilon 0.01208186940302195, time 732.0, rides 121\n",
      "Initial State is  [2, 3, 1]\n",
      "episode 881, reward 1382.0, memory_length 2000, epsilon 0.01202146005600684, time 729.0, rides 125\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 882, reward 924.0, memory_length 2000, epsilon 0.011961352755726806, time 731.0, rides 118\n",
      "Initial State is  [4, 1, 2]\n",
      "episode 883, reward 1281.0, memory_length 2000, epsilon 0.01190154599194817, time 734.0, rides 129\n",
      "Initial State is  [3, 0, 3]\n",
      "episode 884, reward 1247.0, memory_length 2000, epsilon 0.01184203826198843, time 740.0, rides 123\n",
      "Initial State is  [4, 1, 6]\n",
      "episode 885, reward 1050.0, memory_length 2000, epsilon 0.011782828070678488, time 733.0, rides 132\n",
      "Initial State is  [3, 4, 5]\n",
      "episode 886, reward 1206.0, memory_length 2000, epsilon 0.011723913930325095, time 736.0, rides 124\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 887, reward 1207.0, memory_length 2000, epsilon 0.01166529436067347, time 727.0, rides 123\n",
      "Initial State is  [1, 5, 5]\n",
      "episode 888, reward 1299.0, memory_length 2000, epsilon 0.011606967888870102, time 729.0, rides 112\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 889, reward 924.0, memory_length 2000, epsilon 0.01154893304942575, time 727.0, rides 120\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 890, reward 1127.0, memory_length 2000, epsilon 0.011491188384178622, time 735.0, rides 118\n",
      "Initial State is  [4, 4, 5]\n",
      "episode 891, reward 989.0, memory_length 2000, epsilon 0.011433732442257729, time 729.0, rides 124\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 892, reward 1412.0, memory_length 2000, epsilon 0.01137656378004644, time 729.0, rides 112\n",
      "Initial State is  [1, 8, 2]\n",
      "episode 893, reward 1287.0, memory_length 2000, epsilon 0.011319680961146208, time 723.0, rides 115\n",
      "Initial State is  [3, 23, 0]\n",
      "episode 894, reward 1062.0, memory_length 2000, epsilon 0.011263082556340478, time 733.0, rides 121\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 895, reward 1088.0, memory_length 2000, epsilon 0.011206767143558775, time 726.0, rides 132\n",
      "Initial State is  [4, 18, 4]\n",
      "episode 896, reward 873.0, memory_length 2000, epsilon 0.011150733307840981, time 722.0, rides 117\n",
      "Initial State is  [1, 1, 0]\n",
      "episode 897, reward 1287.0, memory_length 2000, epsilon 0.011094979641301777, time 725.0, rides 134\n",
      "Initial State is  [2, 1, 0]\n",
      "episode 898, reward 1214.0, memory_length 2000, epsilon 0.011039504743095268, time 728.0, rides 121\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 899, reward 1055.0, memory_length 2000, epsilon 0.01098430721937979, time 732.0, rides 118\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 900, reward 983.0, memory_length 2000, epsilon 0.010929385683282892, time 734.0, rides 125\n",
      "Initial State is  [2, 8, 6]\n",
      "episode 901, reward 872.0, memory_length 2000, epsilon 0.010874738754866477, time 726.0, rides 111\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 902, reward 1060.0, memory_length 2000, epsilon 0.010820365061092144, time 736.0, rides 131\n",
      "Initial State is  [0, 19, 3]\n",
      "episode 903, reward 1143.0, memory_length 2000, epsilon 0.010766263235786683, time 731.0, rides 141\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 904, reward 1186.0, memory_length 2000, epsilon 0.01071243191960775, time 732.0, rides 117\n",
      "Initial State is  [1, 0, 0]\n",
      "episode 905, reward 1294.0, memory_length 2000, epsilon 0.010658869760009713, time 736.0, rides 123\n",
      "Initial State is  [3, 12, 4]\n",
      "episode 906, reward 1074.0, memory_length 2000, epsilon 0.010605575411209664, time 724.0, rides 127\n",
      "Initial State is  [4, 22, 1]\n",
      "episode 907, reward 1327.0, memory_length 2000, epsilon 0.010552547534153616, time 728.0, rides 125\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 908, reward 1195.0, memory_length 2000, epsilon 0.010499784796482848, time 730.0, rides 116\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 909, reward 1337.0, memory_length 2000, epsilon 0.010447285872500434, time 726.0, rides 118\n",
      "Initial State is  [2, 14, 5]\n",
      "episode 910, reward 1049.0, memory_length 2000, epsilon 0.01039504944313793, time 740.0, rides 126\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 911, reward 1042.0, memory_length 2000, epsilon 0.010343074195922241, time 725.0, rides 135\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 912, reward 1246.0, memory_length 2000, epsilon 0.01029135882494263, time 729.0, rides 118\n",
      "Initial State is  [0, 14, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 913, reward 1286.0, memory_length 2000, epsilon 0.010239902030817916, time 730.0, rides 120\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 914, reward 1155.0, memory_length 2000, epsilon 0.010188702520663827, time 729.0, rides 123\n",
      "Initial State is  [2, 22, 1]\n",
      "episode 915, reward 1186.0, memory_length 2000, epsilon 0.010137759008060509, time 728.0, rides 134\n",
      "Initial State is  [0, 21, 5]\n",
      "episode 916, reward 962.0, memory_length 2000, epsilon 0.010087070213020206, time 733.0, rides 121\n",
      "Initial State is  [4, 21, 3]\n",
      "episode 917, reward 867.0, memory_length 2000, epsilon 0.010036634861955105, time 731.0, rides 129\n",
      "Initial State is  [3, 15, 5]\n",
      "episode 918, reward 1117.0, memory_length 2000, epsilon 0.00998645168764533, time 724.0, rides 117\n",
      "Initial State is  [4, 3, 5]\n",
      "episode 919, reward 1103.0, memory_length 2000, epsilon 0.009936519429207103, time 732.0, rides 123\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 920, reward 930.0, memory_length 2000, epsilon 0.009886836832061067, time 730.0, rides 130\n",
      "Initial State is  [3, 17, 3]\n",
      "episode 921, reward 843.0, memory_length 2000, epsilon 0.009837402647900761, time 729.0, rides 124\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 922, reward 1135.0, memory_length 2000, epsilon 0.009788215634661257, time 728.0, rides 113\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 923, reward 1147.0, memory_length 2000, epsilon 0.00973927455648795, time 726.0, rides 137\n",
      "Initial State is  [2, 7, 2]\n",
      "episode 924, reward 902.0, memory_length 2000, epsilon 0.009690578183705511, time 727.0, rides 120\n",
      "Initial State is  [2, 6, 5]\n",
      "episode 925, reward 1032.0, memory_length 2000, epsilon 0.009642125292786984, time 730.0, rides 119\n",
      "Initial State is  [1, 4, 1]\n",
      "episode 926, reward 979.0, memory_length 2000, epsilon 0.009593914666323049, time 727.0, rides 122\n",
      "Initial State is  [3, 18, 4]\n",
      "episode 927, reward 995.0, memory_length 2000, epsilon 0.009545945092991434, time 731.0, rides 115\n",
      "Initial State is  [0, 16, 5]\n",
      "episode 928, reward 1329.0, memory_length 2000, epsilon 0.009498215367526477, time 731.0, rides 122\n",
      "Initial State is  [2, 11, 5]\n",
      "episode 929, reward 1157.0, memory_length 2000, epsilon 0.009450724290688845, time 726.0, rides 118\n",
      "Initial State is  [3, 15, 4]\n",
      "episode 930, reward 960.0, memory_length 2000, epsilon 0.0094034706692354, time 725.0, rides 117\n",
      "Initial State is  [0, 18, 5]\n",
      "episode 931, reward 1102.0, memory_length 2000, epsilon 0.009356453315889223, time 726.0, rides 125\n",
      "Initial State is  [2, 15, 2]\n",
      "episode 932, reward 1271.0, memory_length 2000, epsilon 0.009309671049309777, time 723.0, rides 122\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 933, reward 1061.0, memory_length 2000, epsilon 0.009263122694063227, time 733.0, rides 132\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 934, reward 1377.0, memory_length 2000, epsilon 0.009216807080592911, time 734.0, rides 123\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 935, reward 1193.0, memory_length 2000, epsilon 0.009170723045189946, time 737.0, rides 142\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 936, reward 796.0, memory_length 2000, epsilon 0.009124869429963996, time 728.0, rides 128\n",
      "Initial State is  [3, 9, 6]\n",
      "episode 937, reward 1029.0, memory_length 2000, epsilon 0.009079245082814175, time 726.0, rides 122\n",
      "Initial State is  [0, 19, 6]\n",
      "episode 938, reward 1105.0, memory_length 2000, epsilon 0.009033848857400105, time 729.0, rides 125\n",
      "Initial State is  [1, 9, 0]\n",
      "episode 939, reward 1322.0, memory_length 2000, epsilon 0.008988679613113105, time 735.0, rides 123\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 940, reward 880.0, memory_length 2000, epsilon 0.00894373621504754, time 724.0, rides 110\n",
      "Initial State is  [4, 22, 2]\n",
      "episode 941, reward 1069.0, memory_length 2000, epsilon 0.008899017533972303, time 732.0, rides 120\n",
      "Initial State is  [1, 4, 1]\n",
      "episode 942, reward 1057.0, memory_length 2000, epsilon 0.008854522446302441, time 729.0, rides 125\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 943, reward 1008.0, memory_length 2000, epsilon 0.00881024983407093, time 731.0, rides 119\n",
      "Initial State is  [3, 20, 0]\n",
      "episode 944, reward 1078.0, memory_length 2000, epsilon 0.008766198584900575, time 730.0, rides 135\n",
      "Initial State is  [2, 16, 3]\n",
      "episode 945, reward 1235.0, memory_length 2000, epsilon 0.008722367591976072, time 728.0, rides 124\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 946, reward 999.0, memory_length 2000, epsilon 0.008678755754016191, time 733.0, rides 127\n",
      "Initial State is  [3, 23, 4]\n",
      "episode 947, reward 1262.0, memory_length 2000, epsilon 0.00863536197524611, time 735.0, rides 117\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 948, reward 1383.0, memory_length 2000, epsilon 0.00859218516536988, time 725.0, rides 128\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 949, reward 1067.0, memory_length 2000, epsilon 0.00854922423954303, time 728.0, rides 137\n",
      "Initial State is  [3, 23, 5]\n",
      "episode 950, reward 959.0, memory_length 2000, epsilon 0.008506478118345316, time 727.0, rides 128\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 951, reward 1125.0, memory_length 2000, epsilon 0.008463945727753589, time 730.0, rides 129\n",
      "Initial State is  [4, 2, 2]\n",
      "episode 952, reward 1164.0, memory_length 2000, epsilon 0.00842162599911482, time 731.0, rides 127\n",
      "Initial State is  [2, 15, 0]\n",
      "episode 953, reward 1190.0, memory_length 2000, epsilon 0.008379517869119247, time 726.0, rides 118\n",
      "Initial State is  [2, 23, 1]\n",
      "episode 954, reward 1398.0, memory_length 2000, epsilon 0.008337620279773651, time 729.0, rides 124\n",
      "Initial State is  [3, 16, 2]\n",
      "episode 955, reward 1204.0, memory_length 2000, epsilon 0.008295932178374783, time 727.0, rides 118\n",
      "Initial State is  [4, 7, 1]\n",
      "episode 956, reward 1047.0, memory_length 2000, epsilon 0.008254452517482908, time 728.0, rides 117\n",
      "Initial State is  [3, 14, 5]\n",
      "episode 957, reward 976.0, memory_length 2000, epsilon 0.008213180254895494, time 733.0, rides 119\n",
      "Initial State is  [1, 13, 0]\n",
      "episode 958, reward 1426.0, memory_length 2000, epsilon 0.008172114353621017, time 729.0, rides 114\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 959, reward 1339.0, memory_length 2000, epsilon 0.008131253781852912, time 724.0, rides 122\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 960, reward 1043.0, memory_length 2000, epsilon 0.008090597512943647, time 723.0, rides 123\n",
      "Initial State is  [4, 17, 3]\n",
      "episode 961, reward 1004.0, memory_length 2000, epsilon 0.008050144525378928, time 725.0, rides 113\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 962, reward 1176.0, memory_length 2000, epsilon 0.008009893802752034, time 735.0, rides 121\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 963, reward 1110.0, memory_length 2000, epsilon 0.007969844333738273, time 735.0, rides 126\n",
      "Initial State is  [4, 15, 1]\n",
      "episode 964, reward 1225.0, memory_length 2000, epsilon 0.007929995112069581, time 732.0, rides 118\n",
      "Initial State is  [4, 13, 5]\n",
      "episode 965, reward 1305.0, memory_length 2000, epsilon 0.007890345136509233, time 730.0, rides 115\n",
      "Initial State is  [2, 2, 5]\n",
      "episode 966, reward 1123.0, memory_length 2000, epsilon 0.007850893410826686, time 724.0, rides 129\n",
      "Initial State is  [2, 3, 6]\n",
      "episode 967, reward 1093.0, memory_length 2000, epsilon 0.007811638943772553, time 728.0, rides 128\n",
      "Initial State is  [2, 12, 6]\n",
      "episode 968, reward 1263.0, memory_length 2000, epsilon 0.00777258074905369, time 728.0, rides 120\n",
      "Initial State is  [2, 4, 0]\n",
      "episode 969, reward 1297.0, memory_length 2000, epsilon 0.0077337178453084215, time 728.0, rides 127\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 970, reward 1262.0, memory_length 2000, epsilon 0.0076950492560818795, time 734.0, rides 127\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 971, reward 1205.0, memory_length 2000, epsilon 0.00765657400980147, time 732.0, rides 118\n",
      "Initial State is  [2, 9, 4]\n",
      "episode 972, reward 1259.0, memory_length 2000, epsilon 0.007618291139752462, time 727.0, rides 124\n",
      "Initial State is  [1, 6, 3]\n",
      "episode 973, reward 1071.0, memory_length 2000, epsilon 0.0075801996840537, time 728.0, rides 128\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 974, reward 1015.0, memory_length 2000, epsilon 0.007542298685633431, time 727.0, rides 124\n",
      "Initial State is  [3, 5, 3]\n",
      "episode 975, reward 1228.0, memory_length 2000, epsilon 0.007504587192205264, time 723.0, rides 116\n",
      "Initial State is  [3, 9, 3]\n",
      "episode 976, reward 1110.0, memory_length 2000, epsilon 0.0074670642562442375, time 732.0, rides 114\n",
      "Initial State is  [4, 3, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 977, reward 1165.0, memory_length 2000, epsilon 0.007429728934963016, time 727.0, rides 119\n",
      "Initial State is  [1, 15, 1]\n",
      "episode 978, reward 1034.0, memory_length 2000, epsilon 0.007392580290288201, time 723.0, rides 121\n",
      "Initial State is  [2, 3, 0]\n",
      "episode 979, reward 998.0, memory_length 2000, epsilon 0.00735561738883676, time 725.0, rides 124\n",
      "Initial State is  [3, 20, 0]\n",
      "episode 980, reward 1084.0, memory_length 2000, epsilon 0.007318839301892576, time 736.0, rides 124\n",
      "Initial State is  [2, 1, 4]\n",
      "episode 981, reward 957.0, memory_length 2000, epsilon 0.0072822451053831125, time 730.0, rides 114\n",
      "Initial State is  [0, 2, 0]\n",
      "episode 982, reward 1395.0, memory_length 2000, epsilon 0.007245833879856197, time 730.0, rides 132\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 983, reward 1366.0, memory_length 2000, epsilon 0.007209604710456916, time 722.0, rides 133\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 984, reward 1240.0, memory_length 2000, epsilon 0.0071735566869046315, time 730.0, rides 114\n",
      "Initial State is  [4, 5, 6]\n",
      "episode 985, reward 804.0, memory_length 2000, epsilon 0.007137688903470108, time 725.0, rides 132\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 986, reward 1513.0, memory_length 2000, epsilon 0.0071020004589527575, time 731.0, rides 121\n",
      "Initial State is  [1, 17, 4]\n",
      "episode 987, reward 1530.0, memory_length 2000, epsilon 0.0070664904566579935, time 744.0, rides 117\n",
      "Initial State is  [4, 14, 6]\n",
      "episode 988, reward 1368.0, memory_length 2000, epsilon 0.007031158004374704, time 727.0, rides 127\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 989, reward 1270.0, memory_length 2000, epsilon 0.00699600221435283, time 730.0, rides 128\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 990, reward 1275.0, memory_length 2000, epsilon 0.0069610222032810655, time 731.0, rides 133\n",
      "Initial State is  [0, 19, 3]\n",
      "episode 991, reward 1435.0, memory_length 2000, epsilon 0.0069262170922646605, time 725.0, rides 130\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 992, reward 1210.0, memory_length 2000, epsilon 0.006891586006803337, time 725.0, rides 121\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 993, reward 1102.0, memory_length 2000, epsilon 0.006857128076769321, time 728.0, rides 128\n",
      "Initial State is  [2, 2, 4]\n",
      "episode 994, reward 963.0, memory_length 2000, epsilon 0.006822842436385474, time 729.0, rides 115\n",
      "Initial State is  [0, 8, 6]\n",
      "episode 995, reward 1232.0, memory_length 2000, epsilon 0.006788728224203546, time 730.0, rides 130\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 996, reward 1324.0, memory_length 2000, epsilon 0.006754784583082528, time 724.0, rides 121\n",
      "Initial State is  [4, 14, 5]\n",
      "episode 997, reward 1118.0, memory_length 2000, epsilon 0.006721010660167116, time 729.0, rides 125\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 998, reward 1150.0, memory_length 2000, epsilon 0.00668740560686628, time 738.0, rides 130\n",
      "Initial State is  [2, 6, 6]\n",
      "episode 999, reward 893.0, memory_length 2000, epsilon 0.006653968578831948, time 729.0, rides 124\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 1000, reward 1234.0, memory_length 2000, epsilon 0.0066206987359377885, time 727.0, rides 123\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 1001, reward 1304.0, memory_length 2000, epsilon 0.0065875952422581, time 731.0, rides 117\n",
      "Initial State is  [4, 16, 5]\n",
      "episode 1002, reward 977.0, memory_length 2000, epsilon 0.006554657266046809, time 728.0, rides 122\n",
      "Initial State is  [4, 14, 2]\n",
      "episode 1003, reward 1256.0, memory_length 2000, epsilon 0.006521883979716575, time 724.0, rides 115\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 1004, reward 1050.0, memory_length 2000, epsilon 0.006489274559817992, time 728.0, rides 132\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 1005, reward 1152.0, memory_length 2000, epsilon 0.006456828187018902, time 733.0, rides 119\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 1006, reward 1144.0, memory_length 2000, epsilon 0.006424544046083807, time 728.0, rides 128\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 1007, reward 1135.0, memory_length 2000, epsilon 0.006392421325853387, time 732.0, rides 112\n",
      "Initial State is  [1, 22, 2]\n",
      "episode 1008, reward 855.0, memory_length 2000, epsilon 0.00636045921922412, time 729.0, rides 119\n",
      "Initial State is  [4, 22, 2]\n",
      "episode 1009, reward 1281.0, memory_length 2000, epsilon 0.006328656923128, time 743.0, rides 122\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 1010, reward 1162.0, memory_length 2000, epsilon 0.00629701363851236, time 723.0, rides 123\n",
      "Initial State is  [1, 1, 0]\n",
      "episode 1011, reward 893.0, memory_length 2000, epsilon 0.006265528570319798, time 736.0, rides 117\n",
      "Initial State is  [1, 2, 3]\n",
      "episode 1012, reward 1350.0, memory_length 2000, epsilon 0.006234200927468199, time 730.0, rides 114\n",
      "Initial State is  [1, 1, 1]\n",
      "episode 1013, reward 1076.0, memory_length 2000, epsilon 0.006203029922830858, time 734.0, rides 109\n",
      "Initial State is  [4, 19, 1]\n",
      "episode 1014, reward 1132.0, memory_length 2000, epsilon 0.006172014773216704, time 731.0, rides 121\n",
      "Initial State is  [4, 3, 4]\n",
      "episode 1015, reward 1181.0, memory_length 2000, epsilon 0.006141154699350621, time 732.0, rides 123\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 1016, reward 1085.0, memory_length 2000, epsilon 0.006110448925853868, time 724.0, rides 123\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 1017, reward 973.0, memory_length 2000, epsilon 0.006079896681224598, time 731.0, rides 127\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 1018, reward 1268.0, memory_length 2000, epsilon 0.006049497197818475, time 727.0, rides 117\n",
      "Initial State is  [3, 23, 6]\n",
      "episode 1019, reward 969.0, memory_length 2000, epsilon 0.006019249711829383, time 723.0, rides 123\n",
      "Initial State is  [2, 14, 5]\n",
      "episode 1020, reward 810.0, memory_length 2000, epsilon 0.005989153463270236, time 726.0, rides 112\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 1021, reward 1222.0, memory_length 2000, epsilon 0.005959207695953885, time 736.0, rides 123\n",
      "Initial State is  [2, 10, 3]\n",
      "episode 1022, reward 1054.0, memory_length 2000, epsilon 0.005929411657474116, time 728.0, rides 115\n",
      "Initial State is  [2, 22, 2]\n",
      "episode 1023, reward 1091.0, memory_length 2000, epsilon 0.005899764599186745, time 735.0, rides 128\n",
      "Initial State is  [0, 8, 2]\n",
      "episode 1024, reward 1172.0, memory_length 2000, epsilon 0.005870265776190812, time 727.0, rides 121\n",
      "Initial State is  [0, 2, 5]\n",
      "episode 1025, reward 1216.0, memory_length 2000, epsilon 0.0058409144473098576, time 726.0, rides 123\n",
      "Initial State is  [2, 11, 0]\n",
      "episode 1026, reward 1120.0, memory_length 2000, epsilon 0.005811709875073308, time 727.0, rides 134\n",
      "Initial State is  [2, 2, 2]\n",
      "episode 1027, reward 1202.0, memory_length 2000, epsilon 0.0057826513256979415, time 721.0, rides 137\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 1028, reward 796.0, memory_length 2000, epsilon 0.005753738069069452, time 725.0, rides 127\n",
      "Initial State is  [2, 1, 4]\n",
      "episode 1029, reward 1151.0, memory_length 2000, epsilon 0.005724969378724105, time 727.0, rides 134\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 1030, reward 1215.0, memory_length 2000, epsilon 0.005696344531830484, time 725.0, rides 122\n",
      "Initial State is  [0, 9, 3]\n",
      "episode 1031, reward 1142.0, memory_length 2000, epsilon 0.005667862809171332, time 731.0, rides 122\n",
      "Initial State is  [4, 14, 6]\n",
      "episode 1032, reward 1252.0, memory_length 2000, epsilon 0.005639523495125475, time 732.0, rides 123\n",
      "Initial State is  [3, 6, 0]\n",
      "episode 1033, reward 1164.0, memory_length 2000, epsilon 0.005611325877649847, time 727.0, rides 120\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 1034, reward 983.0, memory_length 2000, epsilon 0.005583269248261598, time 732.0, rides 121\n",
      "Initial State is  [3, 12, 4]\n",
      "episode 1035, reward 810.0, memory_length 2000, epsilon 0.00555535290202029, time 725.0, rides 127\n",
      "Initial State is  [3, 4, 0]\n",
      "episode 1036, reward 1008.0, memory_length 2000, epsilon 0.005527576137510188, time 733.0, rides 130\n",
      "Initial State is  [3, 8, 2]\n",
      "episode 1037, reward 781.0, memory_length 2000, epsilon 0.005499938256822637, time 725.0, rides 120\n",
      "Initial State is  [2, 20, 1]\n",
      "episode 1038, reward 886.0, memory_length 2000, epsilon 0.005472438565538524, time 733.0, rides 127\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 1039, reward 1247.0, memory_length 2000, epsilon 0.005445076372710831, time 728.0, rides 116\n",
      "Initial State is  [3, 5, 3]\n",
      "episode 1040, reward 1029.0, memory_length 2000, epsilon 0.005417850990847277, time 726.0, rides 132\n",
      "Initial State is  [4, 9, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1041, reward 1387.0, memory_length 2000, epsilon 0.00539076173589304, time 729.0, rides 129\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 1042, reward 1101.0, memory_length 2000, epsilon 0.005363807927213575, time 731.0, rides 127\n",
      "Initial State is  [1, 9, 6]\n",
      "episode 1043, reward 1003.0, memory_length 2000, epsilon 0.005336988887577507, time 727.0, rides 125\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 1044, reward 1084.0, memory_length 2000, epsilon 0.00531030394313962, time 738.0, rides 128\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 1045, reward 1039.0, memory_length 2000, epsilon 0.005283752423423922, time 732.0, rides 119\n",
      "Initial State is  [3, 11, 0]\n",
      "episode 1046, reward 1443.0, memory_length 2000, epsilon 0.005257333661306802, time 729.0, rides 128\n",
      "Initial State is  [1, 21, 5]\n",
      "episode 1047, reward 599.0, memory_length 2000, epsilon 0.005231046993000268, time 723.0, rides 117\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 1048, reward 1063.0, memory_length 2000, epsilon 0.005204891758035267, time 722.0, rides 119\n",
      "Initial State is  [0, 5, 2]\n",
      "episode 1049, reward 1166.0, memory_length 2000, epsilon 0.005178867299245091, time 732.0, rides 117\n",
      "Initial State is  [4, 20, 5]\n",
      "episode 1050, reward 1070.0, memory_length 2000, epsilon 0.0051529729627488655, time 734.0, rides 122\n",
      "Initial State is  [0, 2, 5]\n",
      "episode 1051, reward 949.0, memory_length 2000, epsilon 0.005127208097935121, time 732.0, rides 123\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 1052, reward 817.0, memory_length 2000, epsilon 0.0051015720574454455, time 722.0, rides 124\n",
      "Initial State is  [3, 10, 5]\n",
      "episode 1053, reward 1006.0, memory_length 2000, epsilon 0.0050760641971582185, time 731.0, rides 121\n",
      "Initial State is  [1, 5, 3]\n",
      "episode 1054, reward 661.0, memory_length 2000, epsilon 0.005050683876172427, time 723.0, rides 129\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 1055, reward 1054.0, memory_length 2000, epsilon 0.0050254304567915655, time 722.0, rides 121\n",
      "Initial State is  [4, 23, 4]\n",
      "episode 1056, reward 1289.0, memory_length 2000, epsilon 0.005000303304507608, time 730.0, rides 127\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 1057, reward 901.0, memory_length 2000, epsilon 0.004975301787985069, time 725.0, rides 116\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 1058, reward 855.0, memory_length 2000, epsilon 0.004950425279045144, time 722.0, rides 119\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 1059, reward 1168.0, memory_length 2000, epsilon 0.004925673152649918, time 729.0, rides 119\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 1060, reward 764.0, memory_length 2000, epsilon 0.004901044786886668, time 731.0, rides 138\n",
      "Initial State is  [1, 2, 2]\n",
      "episode 1061, reward 1020.0, memory_length 2000, epsilon 0.004876539562952234, time 732.0, rides 123\n",
      "Initial State is  [0, 17, 3]\n",
      "episode 1062, reward 1071.0, memory_length 2000, epsilon 0.004852156865137473, time 724.0, rides 119\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 1063, reward 1264.0, memory_length 2000, epsilon 0.004827896080811785, time 729.0, rides 121\n",
      "Initial State is  [1, 19, 5]\n",
      "episode 1064, reward 1089.0, memory_length 2000, epsilon 0.004803756600407726, time 730.0, rides 131\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 1065, reward 1102.0, memory_length 2000, epsilon 0.0047797378174056875, time 736.0, rides 116\n",
      "Initial State is  [4, 14, 4]\n",
      "episode 1066, reward 1040.0, memory_length 2000, epsilon 0.004755839128318659, time 729.0, rides 120\n",
      "Initial State is  [1, 22, 5]\n",
      "episode 1067, reward 1151.0, memory_length 2000, epsilon 0.004732059932677066, time 734.0, rides 133\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 1068, reward 1018.0, memory_length 2000, epsilon 0.004708399633013681, time 730.0, rides 123\n",
      "Initial State is  [1, 7, 5]\n",
      "episode 1069, reward 1198.0, memory_length 2000, epsilon 0.004684857634848613, time 725.0, rides 133\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 1070, reward 804.0, memory_length 2000, epsilon 0.004661433346674369, time 731.0, rides 128\n",
      "Initial State is  [2, 5, 3]\n",
      "episode 1071, reward 1403.0, memory_length 2000, epsilon 0.004638126179940997, time 727.0, rides 141\n",
      "Initial State is  [2, 23, 2]\n",
      "episode 1072, reward 1033.0, memory_length 2000, epsilon 0.004614935549041292, time 723.0, rides 125\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 1073, reward 1146.0, memory_length 2000, epsilon 0.004591860871296085, time 731.0, rides 122\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 1074, reward 983.0, memory_length 2000, epsilon 0.004568901566939605, time 731.0, rides 128\n",
      "Initial State is  [2, 11, 5]\n",
      "episode 1075, reward 1236.0, memory_length 2000, epsilon 0.004546057059104907, time 730.0, rides 125\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 1076, reward 1163.0, memory_length 2000, epsilon 0.004523326773809382, time 731.0, rides 124\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 1077, reward 1293.0, memory_length 2000, epsilon 0.004500710139940335, time 732.0, rides 113\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 1078, reward 1090.0, memory_length 2000, epsilon 0.004478206589240633, time 730.0, rides 117\n",
      "Initial State is  [1, 7, 5]\n",
      "episode 1079, reward 1029.0, memory_length 2000, epsilon 0.00445581555629443, time 730.0, rides 128\n",
      "Initial State is  [2, 15, 5]\n",
      "episode 1080, reward 1055.0, memory_length 2000, epsilon 0.004433536478512958, time 730.0, rides 122\n",
      "Initial State is  [0, 20, 1]\n",
      "episode 1081, reward 995.0, memory_length 2000, epsilon 0.004411368796120392, time 730.0, rides 126\n",
      "Initial State is  [3, 10, 4]\n",
      "episode 1082, reward 1200.0, memory_length 2000, epsilon 0.0043893119521397905, time 728.0, rides 129\n",
      "Initial State is  [3, 20, 5]\n",
      "episode 1083, reward 1266.0, memory_length 2000, epsilon 0.004367365392379092, time 732.0, rides 117\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 1084, reward 1024.0, memory_length 2000, epsilon 0.004345528565417196, time 732.0, rides 120\n",
      "Initial State is  [0, 8, 6]\n",
      "episode 1085, reward 1034.0, memory_length 2000, epsilon 0.00432380092259011, time 727.0, rides 117\n",
      "Initial State is  [0, 5, 5]\n",
      "episode 1086, reward 941.0, memory_length 2000, epsilon 0.00430218191797716, time 730.0, rides 129\n",
      "Initial State is  [0, 16, 1]\n",
      "episode 1087, reward 1149.0, memory_length 2000, epsilon 0.004280671008387274, time 732.0, rides 128\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 1088, reward 1270.0, memory_length 2000, epsilon 0.004259267653345338, time 734.0, rides 136\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 1089, reward 1088.0, memory_length 2000, epsilon 0.004237971315078611, time 732.0, rides 117\n",
      "Initial State is  [0, 23, 4]\n",
      "episode 1090, reward 1040.0, memory_length 2000, epsilon 0.004216781458503218, time 725.0, rides 125\n",
      "Initial State is  [2, 23, 4]\n",
      "episode 1091, reward 951.0, memory_length 2000, epsilon 0.004195697551210702, time 732.0, rides 128\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 1092, reward 1261.0, memory_length 2000, epsilon 0.004174719063454648, time 730.0, rides 132\n",
      "Initial State is  [3, 12, 2]\n",
      "episode 1093, reward 1179.0, memory_length 2000, epsilon 0.004153845468137375, time 728.0, rides 125\n",
      "Initial State is  [4, 4, 3]\n",
      "episode 1094, reward 1030.0, memory_length 2000, epsilon 0.004133076240796688, time 727.0, rides 114\n",
      "Initial State is  [0, 2, 6]\n",
      "episode 1095, reward 1089.0, memory_length 2000, epsilon 0.004112410859592705, time 732.0, rides 128\n",
      "Initial State is  [1, 5, 2]\n",
      "episode 1096, reward 1195.0, memory_length 2000, epsilon 0.004091848805294741, time 735.0, rides 121\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 1097, reward 1245.0, memory_length 2000, epsilon 0.004071389561268267, time 724.0, rides 129\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 1098, reward 1054.0, memory_length 2000, epsilon 0.004051032613461926, time 734.0, rides 126\n",
      "Initial State is  [1, 13, 6]\n",
      "episode 1099, reward 964.0, memory_length 2000, epsilon 0.004030777450394616, time 729.0, rides 115\n",
      "Initial State is  [3, 0, 1]\n",
      "episode 1100, reward 1226.0, memory_length 2000, epsilon 0.004010623563142642, time 728.0, rides 126\n",
      "Initial State is  [3, 9, 1]\n",
      "episode 1101, reward 1184.0, memory_length 2000, epsilon 0.0039905704453269296, time 728.0, rides 136\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 1102, reward 1151.0, memory_length 2000, epsilon 0.003970617593100295, time 730.0, rides 128\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 1103, reward 1324.0, memory_length 2000, epsilon 0.003950764505134793, time 726.0, rides 114\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 1104, reward 1022.0, memory_length 2000, epsilon 0.003931010682609119, time 730.0, rides 120\n",
      "Initial State is  [4, 10, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1105, reward 1119.0, memory_length 2000, epsilon 0.003911355629196074, time 727.0, rides 119\n",
      "Initial State is  [3, 0, 5]\n",
      "episode 1106, reward 1175.0, memory_length 2000, epsilon 0.0038917988510500934, time 737.0, rides 123\n",
      "Initial State is  [2, 23, 6]\n",
      "episode 1107, reward 1243.0, memory_length 2000, epsilon 0.003872339856794843, time 735.0, rides 128\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 1108, reward 1031.0, memory_length 2000, epsilon 0.0038529781575108685, time 726.0, rides 118\n",
      "Initial State is  [1, 13, 6]\n",
      "episode 1109, reward 1126.0, memory_length 2000, epsilon 0.003833713266723314, time 728.0, rides 125\n",
      "Initial State is  [1, 20, 3]\n",
      "episode 1110, reward 1175.0, memory_length 2000, epsilon 0.003814544700389697, time 737.0, rides 128\n",
      "Initial State is  [0, 0, 5]\n",
      "episode 1111, reward 1199.0, memory_length 2000, epsilon 0.0037954719768877486, time 723.0, rides 128\n",
      "Initial State is  [3, 13, 6]\n",
      "episode 1112, reward 1188.0, memory_length 2000, epsilon 0.0037764946170033096, time 729.0, rides 133\n",
      "Initial State is  [4, 4, 6]\n",
      "episode 1113, reward 1392.0, memory_length 2000, epsilon 0.003757612143918293, time 725.0, rides 123\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 1114, reward 1086.0, memory_length 2000, epsilon 0.003738824083198702, time 742.0, rides 117\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 1115, reward 961.0, memory_length 2000, epsilon 0.003720129962782708, time 724.0, rides 127\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 1116, reward 1248.0, memory_length 2000, epsilon 0.0037015293129687944, time 725.0, rides 127\n",
      "Initial State is  [1, 21, 3]\n",
      "episode 1117, reward 893.0, memory_length 2000, epsilon 0.0036830216664039505, time 732.0, rides 124\n",
      "Initial State is  [1, 16, 5]\n",
      "episode 1118, reward 937.0, memory_length 2000, epsilon 0.0036646065580719306, time 734.0, rides 117\n",
      "Initial State is  [2, 22, 5]\n",
      "episode 1119, reward 1350.0, memory_length 2000, epsilon 0.003646283525281571, time 724.0, rides 126\n",
      "Initial State is  [0, 13, 4]\n",
      "episode 1120, reward 1189.0, memory_length 2000, epsilon 0.003628052107655163, time 730.0, rides 115\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 1121, reward 1123.0, memory_length 2000, epsilon 0.0036099118471168874, time 728.0, rides 118\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 1122, reward 1053.0, memory_length 2000, epsilon 0.003591862287881303, time 728.0, rides 118\n",
      "Initial State is  [2, 20, 1]\n",
      "episode 1123, reward 1044.0, memory_length 2000, epsilon 0.0035739029764418964, time 731.0, rides 130\n",
      "Initial State is  [4, 14, 3]\n",
      "episode 1124, reward 1252.0, memory_length 2000, epsilon 0.003556033461559687, time 728.0, rides 135\n",
      "Initial State is  [3, 9, 1]\n",
      "episode 1125, reward 830.0, memory_length 2000, epsilon 0.0035382532942518884, time 725.0, rides 113\n",
      "Initial State is  [1, 2, 6]\n",
      "episode 1126, reward 1034.0, memory_length 2000, epsilon 0.003520562027780629, time 740.0, rides 125\n",
      "Initial State is  [1, 17, 4]\n",
      "episode 1127, reward 932.0, memory_length 2000, epsilon 0.0035029592176417258, time 726.0, rides 116\n",
      "Initial State is  [0, 0, 1]\n",
      "episode 1128, reward 1197.0, memory_length 2000, epsilon 0.0034854444215535172, time 732.0, rides 129\n",
      "Initial State is  [2, 1, 0]\n",
      "episode 1129, reward 1247.0, memory_length 2000, epsilon 0.0034680171994457497, time 729.0, rides 120\n",
      "Initial State is  [3, 20, 4]\n",
      "episode 1130, reward 955.0, memory_length 2000, epsilon 0.003450677113448521, time 727.0, rides 127\n",
      "Initial State is  [3, 1, 0]\n",
      "episode 1131, reward 961.0, memory_length 2000, epsilon 0.0034334237278812784, time 732.0, rides 123\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 1132, reward 1332.0, memory_length 2000, epsilon 0.003416256609241872, time 731.0, rides 123\n",
      "Initial State is  [1, 4, 6]\n",
      "episode 1133, reward 957.0, memory_length 2000, epsilon 0.003399175326195663, time 734.0, rides 119\n",
      "Initial State is  [4, 6, 1]\n",
      "episode 1134, reward 1114.0, memory_length 2000, epsilon 0.0033821794495646844, time 726.0, rides 113\n",
      "Initial State is  [0, 21, 4]\n",
      "episode 1135, reward 996.0, memory_length 2000, epsilon 0.003365268552316861, time 731.0, rides 117\n",
      "Initial State is  [0, 5, 5]\n",
      "episode 1136, reward 1147.0, memory_length 2000, epsilon 0.0033484422095552764, time 726.0, rides 119\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 1137, reward 1083.0, memory_length 2000, epsilon 0.0033316999985075002, time 730.0, rides 129\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 1138, reward 1015.0, memory_length 2000, epsilon 0.0033150414985149627, time 723.0, rides 127\n",
      "Initial State is  [1, 10, 3]\n",
      "episode 1139, reward 1249.0, memory_length 2000, epsilon 0.003298466291022388, time 725.0, rides 127\n",
      "Initial State is  [2, 21, 4]\n",
      "episode 1140, reward 1385.0, memory_length 2000, epsilon 0.003281973959567276, time 726.0, rides 126\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 1141, reward 1325.0, memory_length 2000, epsilon 0.0032655640897694396, time 728.0, rides 122\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 1142, reward 1029.0, memory_length 2000, epsilon 0.0032492362693205923, time 734.0, rides 118\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 1143, reward 1331.0, memory_length 2000, epsilon 0.0032329900879739895, time 728.0, rides 115\n",
      "Initial State is  [1, 10, 5]\n",
      "episode 1144, reward 871.0, memory_length 2000, epsilon 0.0032168251375341195, time 728.0, rides 115\n",
      "Initial State is  [3, 8, 6]\n",
      "episode 1145, reward 1343.0, memory_length 2000, epsilon 0.0032007410118464487, time 742.0, rides 117\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 1146, reward 1049.0, memory_length 2000, epsilon 0.0031847373067872163, time 723.0, rides 129\n",
      "Initial State is  [0, 20, 3]\n",
      "episode 1147, reward 981.0, memory_length 2000, epsilon 0.00316881362025328, time 733.0, rides 131\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 1148, reward 1270.0, memory_length 2000, epsilon 0.0031529695521520136, time 731.0, rides 131\n",
      "Initial State is  [0, 6, 2]\n",
      "episode 1149, reward 1044.0, memory_length 2000, epsilon 0.0031372047043912534, time 734.0, rides 135\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 1150, reward 1208.0, memory_length 2000, epsilon 0.0031215186808692973, time 724.0, rides 116\n",
      "Initial State is  [1, 11, 6]\n",
      "episode 1151, reward 1243.0, memory_length 2000, epsilon 0.003105911087464951, time 726.0, rides 119\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 1152, reward 1158.0, memory_length 2000, epsilon 0.003090381532027626, time 722.0, rides 117\n",
      "Initial State is  [2, 4, 2]\n",
      "episode 1153, reward 1084.0, memory_length 2000, epsilon 0.003074929624367488, time 728.0, rides 124\n",
      "Initial State is  [4, 2, 6]\n",
      "episode 1154, reward 1086.0, memory_length 2000, epsilon 0.0030595549762456506, time 726.0, rides 132\n",
      "Initial State is  [2, 16, 2]\n",
      "episode 1155, reward 973.0, memory_length 2000, epsilon 0.0030442572013644224, time 733.0, rides 120\n",
      "Initial State is  [3, 5, 4]\n",
      "episode 1156, reward 1145.0, memory_length 2000, epsilon 0.0030290359153576003, time 730.0, rides 121\n",
      "Initial State is  [3, 2, 2]\n",
      "episode 1157, reward 1310.0, memory_length 2000, epsilon 0.003013890735780812, time 727.0, rides 119\n",
      "Initial State is  [2, 14, 0]\n",
      "episode 1158, reward 1452.0, memory_length 2000, epsilon 0.002998821282101908, time 738.0, rides 130\n",
      "Initial State is  [4, 22, 5]\n",
      "episode 1159, reward 1089.0, memory_length 2000, epsilon 0.002983827175691398, time 738.0, rides 124\n",
      "Initial State is  [3, 18, 1]\n",
      "episode 1160, reward 987.0, memory_length 2000, epsilon 0.0029689080398129413, time 725.0, rides 137\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 1161, reward 1332.0, memory_length 2000, epsilon 0.0029540634996138766, time 728.0, rides 126\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 1162, reward 1074.0, memory_length 2000, epsilon 0.002939293182115807, time 725.0, rides 119\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 1163, reward 1270.0, memory_length 2000, epsilon 0.002924596716205228, time 732.0, rides 134\n",
      "Initial State is  [0, 2, 4]\n",
      "episode 1164, reward 1171.0, memory_length 2000, epsilon 0.002909973732624202, time 726.0, rides 125\n",
      "Initial State is  [4, 19, 6]\n",
      "episode 1165, reward 1672.0, memory_length 2000, epsilon 0.002895423863961081, time 729.0, rides 126\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 1166, reward 1091.0, memory_length 2000, epsilon 0.0028809467446412754, time 726.0, rides 124\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 1167, reward 1181.0, memory_length 2000, epsilon 0.002866542010918069, time 724.0, rides 117\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 1168, reward 1204.0, memory_length 2000, epsilon 0.0028522093008634787, time 726.0, rides 124\n",
      "Initial State is  [4, 18, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1169, reward 1123.0, memory_length 2000, epsilon 0.002837948254359161, time 728.0, rides 127\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 1170, reward 1101.0, memory_length 2000, epsilon 0.0028237585130873656, time 728.0, rides 120\n",
      "Initial State is  [0, 15, 3]\n",
      "episode 1171, reward 1141.0, memory_length 2000, epsilon 0.0028096397205219286, time 720.0, rides 123\n",
      "Initial State is  [0, 17, 3]\n",
      "episode 1172, reward 747.0, memory_length 2000, epsilon 0.002795591521919319, time 731.0, rides 127\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 1173, reward 1312.0, memory_length 2000, epsilon 0.0027816135643097223, time 729.0, rides 130\n",
      "Initial State is  [0, 3, 1]\n",
      "episode 1174, reward 1048.0, memory_length 2000, epsilon 0.0027677054964881736, time 734.0, rides 116\n",
      "Initial State is  [2, 19, 4]\n",
      "episode 1175, reward 1032.0, memory_length 2000, epsilon 0.0027538669690057326, time 731.0, rides 121\n",
      "Initial State is  [1, 16, 1]\n",
      "episode 1176, reward 1181.0, memory_length 2000, epsilon 0.002740097634160704, time 732.0, rides 137\n",
      "Initial State is  [3, 17, 6]\n",
      "episode 1177, reward 1127.0, memory_length 2000, epsilon 0.0027263971459899005, time 721.0, rides 122\n",
      "Initial State is  [2, 21, 1]\n",
      "episode 1178, reward 859.0, memory_length 2000, epsilon 0.002712765160259951, time 737.0, rides 129\n",
      "Initial State is  [3, 2, 2]\n",
      "episode 1179, reward 1180.0, memory_length 2000, epsilon 0.0026992013344586513, time 735.0, rides 131\n",
      "Initial State is  [2, 6, 5]\n",
      "episode 1180, reward 937.0, memory_length 2000, epsilon 0.002685705327786358, time 726.0, rides 119\n",
      "Initial State is  [2, 20, 1]\n",
      "episode 1181, reward 946.0, memory_length 2000, epsilon 0.002672276801147426, time 731.0, rides 122\n",
      "Initial State is  [2, 16, 1]\n",
      "episode 1182, reward 999.0, memory_length 2000, epsilon 0.002658915417141689, time 725.0, rides 117\n",
      "Initial State is  [4, 10, 0]\n",
      "episode 1183, reward 1395.0, memory_length 2000, epsilon 0.0026456208400559805, time 721.0, rides 113\n",
      "Initial State is  [3, 3, 2]\n",
      "episode 1184, reward 1176.0, memory_length 2000, epsilon 0.0026323927358557005, time 725.0, rides 135\n",
      "Initial State is  [3, 20, 4]\n",
      "episode 1185, reward 897.0, memory_length 2000, epsilon 0.002619230772176422, time 724.0, rides 133\n",
      "Initial State is  [0, 16, 5]\n",
      "episode 1186, reward 983.0, memory_length 2000, epsilon 0.00260613461831554, time 724.0, rides 135\n",
      "Initial State is  [1, 0, 3]\n",
      "episode 1187, reward 1000.0, memory_length 2000, epsilon 0.0025931039452239623, time 725.0, rides 133\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 1188, reward 1073.0, memory_length 2000, epsilon 0.0025801384254978423, time 728.0, rides 130\n",
      "Initial State is  [2, 4, 3]\n",
      "episode 1189, reward 1189.0, memory_length 2000, epsilon 0.002567237733370353, time 727.0, rides 124\n",
      "Initial State is  [4, 16, 5]\n",
      "episode 1190, reward 1111.0, memory_length 2000, epsilon 0.0025544015447035015, time 728.0, rides 112\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 1191, reward 1235.0, memory_length 2000, epsilon 0.002541629536979984, time 729.0, rides 128\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 1192, reward 1059.0, memory_length 2000, epsilon 0.002528921389295084, time 731.0, rides 130\n",
      "Initial State is  [4, 3, 4]\n",
      "episode 1193, reward 1287.0, memory_length 2000, epsilon 0.0025162767823486087, time 723.0, rides 116\n",
      "Initial State is  [0, 11, 2]\n",
      "episode 1194, reward 1119.0, memory_length 2000, epsilon 0.0025036953984368658, time 735.0, rides 118\n",
      "Initial State is  [1, 10, 6]\n",
      "episode 1195, reward 1049.0, memory_length 2000, epsilon 0.0024911769214446813, time 723.0, rides 122\n",
      "Initial State is  [0, 2, 4]\n",
      "episode 1196, reward 1037.0, memory_length 2000, epsilon 0.0024787210368374577, time 728.0, rides 119\n",
      "Initial State is  [3, 22, 1]\n",
      "episode 1197, reward 1336.0, memory_length 2000, epsilon 0.0024663274316532703, time 733.0, rides 125\n",
      "Initial State is  [1, 1, 3]\n",
      "episode 1198, reward 1135.0, memory_length 2000, epsilon 0.002453995794495004, time 722.0, rides 118\n",
      "Initial State is  [2, 0, 0]\n",
      "episode 1199, reward 1057.0, memory_length 2000, epsilon 0.002441725815522529, time 723.0, rides 121\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 1200, reward 1341.0, memory_length 2000, epsilon 0.0024295171864449163, time 726.0, rides 132\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 1201, reward 1159.0, memory_length 2000, epsilon 0.0024173696005126916, time 733.0, rides 123\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 1202, reward 1044.0, memory_length 2000, epsilon 0.0024052827525101283, time 731.0, rides 118\n",
      "Initial State is  [4, 23, 4]\n",
      "episode 1203, reward 1138.0, memory_length 2000, epsilon 0.0023932563387475776, time 726.0, rides 127\n",
      "Initial State is  [2, 20, 5]\n",
      "episode 1204, reward 1049.0, memory_length 2000, epsilon 0.00238129005705384, time 728.0, rides 132\n",
      "Initial State is  [3, 16, 1]\n",
      "episode 1205, reward 1404.0, memory_length 2000, epsilon 0.002369383606768571, time 730.0, rides 124\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 1206, reward 1186.0, memory_length 2000, epsilon 0.002357536688734728, time 729.0, rides 124\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 1207, reward 956.0, memory_length 2000, epsilon 0.0023457490052910543, time 728.0, rides 119\n",
      "Initial State is  [2, 14, 5]\n",
      "episode 1208, reward 993.0, memory_length 2000, epsilon 0.002334020260264599, time 724.0, rides 115\n",
      "Initial State is  [3, 20, 0]\n",
      "episode 1209, reward 1098.0, memory_length 2000, epsilon 0.002322350158963276, time 730.0, rides 117\n",
      "Initial State is  [3, 5, 2]\n",
      "episode 1210, reward 1065.0, memory_length 2000, epsilon 0.0023107384081684596, time 724.0, rides 117\n",
      "Initial State is  [2, 20, 5]\n",
      "episode 1211, reward 1307.0, memory_length 2000, epsilon 0.0022991847161276174, time 730.0, rides 121\n",
      "Initial State is  [0, 21, 1]\n",
      "episode 1212, reward 1186.0, memory_length 2000, epsilon 0.0022876887925469794, time 727.0, rides 133\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 1213, reward 1277.0, memory_length 2000, epsilon 0.0022762503485842444, time 731.0, rides 126\n",
      "Initial State is  [3, 7, 3]\n",
      "episode 1214, reward 1147.0, memory_length 2000, epsilon 0.0022648690968413232, time 731.0, rides 118\n",
      "Initial State is  [0, 0, 5]\n",
      "episode 1215, reward 1231.0, memory_length 2000, epsilon 0.0022535447513571164, time 723.0, rides 133\n",
      "Initial State is  [1, 16, 4]\n",
      "episode 1216, reward 1315.0, memory_length 2000, epsilon 0.002242277027600331, time 727.0, rides 129\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 1217, reward 587.0, memory_length 2000, epsilon 0.0022310656424623294, time 725.0, rides 114\n",
      "Initial State is  [4, 7, 2]\n",
      "episode 1218, reward 1180.0, memory_length 2000, epsilon 0.002219910314250018, time 730.0, rides 131\n",
      "Initial State is  [1, 6, 4]\n",
      "episode 1219, reward 1103.0, memory_length 2000, epsilon 0.0022088107626787677, time 726.0, rides 125\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 1220, reward 1185.0, memory_length 2000, epsilon 0.002197766708865374, time 723.0, rides 120\n",
      "Initial State is  [2, 11, 2]\n",
      "episode 1221, reward 1204.0, memory_length 2000, epsilon 0.002186777875321047, time 728.0, rides 119\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 1222, reward 1292.0, memory_length 2000, epsilon 0.0021758439859444418, time 727.0, rides 118\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 1223, reward 1043.0, memory_length 2000, epsilon 0.0021649647660147197, time 726.0, rides 126\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 1224, reward 1282.0, memory_length 2000, epsilon 0.002154139942184646, time 729.0, rides 131\n",
      "Initial State is  [4, 7, 6]\n",
      "episode 1225, reward 995.0, memory_length 2000, epsilon 0.002143369242473723, time 724.0, rides 128\n",
      "Initial State is  [1, 11, 4]\n",
      "episode 1226, reward 1428.0, memory_length 2000, epsilon 0.0021326523962613545, time 733.0, rides 117\n",
      "Initial State is  [2, 16, 1]\n",
      "episode 1227, reward 1000.0, memory_length 2000, epsilon 0.002121989134280048, time 729.0, rides 127\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 1228, reward 1117.0, memory_length 2000, epsilon 0.002111379188608648, time 725.0, rides 127\n",
      "Initial State is  [0, 18, 4]\n",
      "episode 1229, reward 1488.0, memory_length 2000, epsilon 0.0021008222926656044, time 731.0, rides 121\n",
      "Initial State is  [1, 20, 4]\n",
      "episode 1230, reward 1015.0, memory_length 2000, epsilon 0.0020903181812022766, time 728.0, rides 122\n",
      "Initial State is  [2, 1, 0]\n",
      "episode 1231, reward 1216.0, memory_length 2000, epsilon 0.002079866590296265, time 726.0, rides 116\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 1232, reward 1189.0, memory_length 2000, epsilon 0.0020694672573447837, time 730.0, rides 130\n",
      "Initial State is  [3, 12, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1233, reward 1053.0, memory_length 2000, epsilon 0.0020591199210580596, time 738.0, rides 129\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 1234, reward 914.0, memory_length 2000, epsilon 0.0020488243214527692, time 730.0, rides 122\n",
      "Initial State is  [0, 11, 6]\n",
      "episode 1235, reward 887.0, memory_length 2000, epsilon 0.0020385801998455055, time 731.0, rides 133\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 1236, reward 1185.0, memory_length 2000, epsilon 0.002028387298846278, time 730.0, rides 122\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 1237, reward 1355.0, memory_length 2000, epsilon 0.0020182453623520465, time 741.0, rides 123\n",
      "Initial State is  [2, 16, 0]\n",
      "episode 1238, reward 1206.0, memory_length 2000, epsilon 0.0020081541355402863, time 731.0, rides 130\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 1239, reward 1062.0, memory_length 2000, epsilon 0.0019981133648625847, time 728.0, rides 127\n",
      "Initial State is  [4, 1, 2]\n",
      "episode 1240, reward 1197.0, memory_length 2000, epsilon 0.001988122798038272, time 725.0, rides 128\n",
      "Initial State is  [2, 13, 4]\n",
      "episode 1241, reward 971.0, memory_length 2000, epsilon 0.0019781821840480804, time 730.0, rides 124\n",
      "Initial State is  [4, 9, 4]\n",
      "episode 1242, reward 1225.0, memory_length 2000, epsilon 0.00196829127312784, time 732.0, rides 130\n",
      "Initial State is  [0, 5, 6]\n",
      "episode 1243, reward 1240.0, memory_length 2000, epsilon 0.0019584498167622005, time 731.0, rides 129\n",
      "Initial State is  [3, 21, 2]\n",
      "episode 1244, reward 1126.0, memory_length 2000, epsilon 0.0019486575676783894, time 728.0, rides 112\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 1245, reward 848.0, memory_length 2000, epsilon 0.0019389142798399974, time 725.0, rides 121\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 1246, reward 1126.0, memory_length 2000, epsilon 0.0019292197084407974, time 730.0, rides 120\n",
      "Initial State is  [0, 1, 5]\n",
      "episode 1247, reward 1336.0, memory_length 2000, epsilon 0.0019195736098985934, time 731.0, rides 111\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 1248, reward 1325.0, memory_length 2000, epsilon 0.0019099757418491003, time 735.0, rides 123\n",
      "Initial State is  [4, 12, 5]\n",
      "episode 1249, reward 973.0, memory_length 2000, epsilon 0.0019004258631398548, time 726.0, rides 127\n",
      "Initial State is  [0, 6, 5]\n",
      "episode 1250, reward 1201.0, memory_length 2000, epsilon 0.0018909237338241556, time 728.0, rides 125\n",
      "Initial State is  [3, 9, 1]\n",
      "episode 1251, reward 1085.0, memory_length 2000, epsilon 0.0018814691151550348, time 727.0, rides 122\n",
      "Initial State is  [0, 8, 5]\n",
      "episode 1252, reward 1272.0, memory_length 2000, epsilon 0.0018720617695792596, time 728.0, rides 123\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 1253, reward 1164.0, memory_length 2000, epsilon 0.0018627014607313632, time 725.0, rides 130\n",
      "Initial State is  [4, 7, 1]\n",
      "episode 1254, reward 1071.0, memory_length 2000, epsilon 0.0018533879534277063, time 731.0, rides 117\n",
      "Initial State is  [1, 15, 3]\n",
      "episode 1255, reward 1188.0, memory_length 2000, epsilon 0.0018441210136605677, time 730.0, rides 123\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 1256, reward 891.0, memory_length 2000, epsilon 0.0018349004085922648, time 732.0, rides 116\n",
      "Initial State is  [2, 0, 4]\n",
      "episode 1257, reward 793.0, memory_length 2000, epsilon 0.0018257259065493036, time 727.0, rides 132\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 1258, reward 1087.0, memory_length 2000, epsilon 0.0018165972770165571, time 727.0, rides 111\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 1259, reward 1004.0, memory_length 2000, epsilon 0.0018075142906314743, time 725.0, rides 120\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 1260, reward 1186.0, memory_length 2000, epsilon 0.001798476719178317, time 727.0, rides 122\n",
      "Initial State is  [4, 23, 6]\n",
      "episode 1261, reward 1236.0, memory_length 2000, epsilon 0.0017894843355824254, time 727.0, rides 124\n",
      "Initial State is  [0, 16, 5]\n",
      "episode 1262, reward 1263.0, memory_length 2000, epsilon 0.0017805369139045131, time 731.0, rides 126\n",
      "Initial State is  [1, 17, 5]\n",
      "episode 1263, reward 959.0, memory_length 2000, epsilon 0.0017716342293349905, time 729.0, rides 127\n",
      "Initial State is  [1, 12, 3]\n",
      "episode 1264, reward 1024.0, memory_length 2000, epsilon 0.0017627760581883155, time 727.0, rides 112\n",
      "Initial State is  [1, 9, 3]\n",
      "episode 1265, reward 1163.0, memory_length 2000, epsilon 0.0017539621778973739, time 729.0, rides 116\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 1266, reward 1204.0, memory_length 2000, epsilon 0.001745192367007887, time 727.0, rides 119\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 1267, reward 1102.0, memory_length 2000, epsilon 0.0017364664051728476, time 737.0, rides 126\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 1268, reward 979.0, memory_length 2000, epsilon 0.0017277840731469835, time 725.0, rides 131\n",
      "Initial State is  [4, 5, 0]\n",
      "episode 1269, reward 838.0, memory_length 2000, epsilon 0.0017191451527812486, time 721.0, rides 123\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 1270, reward 1146.0, memory_length 2000, epsilon 0.0017105494270173423, time 732.0, rides 130\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 1271, reward 958.0, memory_length 2000, epsilon 0.0017019966798822556, time 734.0, rides 139\n",
      "Initial State is  [3, 15, 0]\n",
      "episode 1272, reward 1218.0, memory_length 2000, epsilon 0.0016934866964828444, time 728.0, rides 122\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 1273, reward 1212.0, memory_length 2000, epsilon 0.00168501926300043, time 728.0, rides 126\n",
      "Initial State is  [0, 1, 0]\n",
      "episode 1274, reward 1153.0, memory_length 2000, epsilon 0.0016765941666854278, time 728.0, rides 113\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 1275, reward 1054.0, memory_length 2000, epsilon 0.0016682111958520006, time 730.0, rides 135\n",
      "Initial State is  [0, 6, 5]\n",
      "episode 1276, reward 1051.0, memory_length 2000, epsilon 0.0016598701398727405, time 735.0, rides 122\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 1277, reward 980.0, memory_length 2000, epsilon 0.0016515707891733768, time 726.0, rides 119\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 1278, reward 1211.0, memory_length 2000, epsilon 0.0016433129352275099, time 733.0, rides 125\n",
      "Initial State is  [1, 10, 3]\n",
      "episode 1279, reward 1235.0, memory_length 2000, epsilon 0.0016350963705513723, time 728.0, rides 124\n",
      "Initial State is  [4, 15, 0]\n",
      "episode 1280, reward 1185.0, memory_length 2000, epsilon 0.0016269208886986154, time 734.0, rides 130\n",
      "Initial State is  [1, 10, 2]\n",
      "episode 1281, reward 885.0, memory_length 2000, epsilon 0.0016187862842551224, time 727.0, rides 122\n",
      "Initial State is  [3, 5, 4]\n",
      "episode 1282, reward 1242.0, memory_length 2000, epsilon 0.0016106923528338467, time 731.0, rides 123\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 1283, reward 889.0, memory_length 2000, epsilon 0.0016026388910696774, time 733.0, rides 117\n",
      "Initial State is  [3, 5, 0]\n",
      "episode 1284, reward 1051.0, memory_length 2000, epsilon 0.001594625696614329, time 725.0, rides 128\n",
      "Initial State is  [2, 6, 5]\n",
      "episode 1285, reward 973.0, memory_length 2000, epsilon 0.0015866525681312573, time 735.0, rides 125\n",
      "Initial State is  [2, 21, 0]\n",
      "episode 1286, reward 1126.0, memory_length 2000, epsilon 0.001578719305290601, time 732.0, rides 123\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 1287, reward 1112.0, memory_length 2000, epsilon 0.0015708257087641482, time 724.0, rides 128\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 1288, reward 1292.0, memory_length 2000, epsilon 0.0015629715802203273, time 734.0, rides 117\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 1289, reward 1227.0, memory_length 2000, epsilon 0.0015551567223192257, time 732.0, rides 129\n",
      "Initial State is  [2, 17, 4]\n",
      "episode 1290, reward 1026.0, memory_length 2000, epsilon 0.0015473809387076295, time 726.0, rides 126\n",
      "Initial State is  [0, 23, 5]\n",
      "episode 1291, reward 1093.0, memory_length 2000, epsilon 0.0015396440340140914, time 729.0, rides 124\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 1292, reward 1191.0, memory_length 2000, epsilon 0.0015319458138440209, time 742.0, rides 127\n",
      "Initial State is  [4, 21, 6]\n",
      "episode 1293, reward 1013.0, memory_length 2000, epsilon 0.0015242860847748008, time 732.0, rides 118\n",
      "Initial State is  [4, 18, 2]\n",
      "episode 1294, reward 1180.0, memory_length 2000, epsilon 0.0015166646543509268, time 735.0, rides 124\n",
      "Initial State is  [4, 11, 4]\n",
      "episode 1295, reward 1066.0, memory_length 2000, epsilon 0.0015090813310791721, time 724.0, rides 120\n",
      "Initial State is  [0, 5, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1296, reward 1180.0, memory_length 2000, epsilon 0.0015015359244237763, time 734.0, rides 125\n",
      "Initial State is  [2, 23, 6]\n",
      "episode 1297, reward 1256.0, memory_length 2000, epsilon 0.0014940282448016574, time 734.0, rides 118\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 1298, reward 1409.0, memory_length 2000, epsilon 0.001486558103577649, time 729.0, rides 124\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 1299, reward 1046.0, memory_length 2000, epsilon 0.0014791253130597608, time 729.0, rides 116\n",
      "Initial State is  [2, 22, 3]\n",
      "episode 1300, reward 1119.0, memory_length 2000, epsilon 0.001471729686494462, time 728.0, rides 120\n",
      "Initial State is  [4, 9, 4]\n",
      "episode 1301, reward 1251.0, memory_length 2000, epsilon 0.0014643710380619897, time 723.0, rides 123\n",
      "Initial State is  [3, 7, 5]\n",
      "episode 1302, reward 1231.0, memory_length 2000, epsilon 0.0014570491828716798, time 736.0, rides 125\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 1303, reward 925.0, memory_length 2000, epsilon 0.0014497639369573214, time 728.0, rides 123\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 1304, reward 1433.0, memory_length 2000, epsilon 0.0014425151172725347, time 727.0, rides 140\n",
      "Initial State is  [4, 23, 3]\n",
      "episode 1305, reward 980.0, memory_length 2000, epsilon 0.001435302541686172, time 725.0, rides 114\n",
      "Initial State is  [2, 4, 3]\n",
      "episode 1306, reward 1109.0, memory_length 2000, epsilon 0.0014281260289777413, time 732.0, rides 129\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 1307, reward 1201.0, memory_length 2000, epsilon 0.0014209853988328526, time 733.0, rides 114\n",
      "Initial State is  [0, 3, 3]\n",
      "episode 1308, reward 954.0, memory_length 2000, epsilon 0.0014138804718386883, time 727.0, rides 115\n",
      "Initial State is  [0, 16, 5]\n",
      "episode 1309, reward 1090.0, memory_length 2000, epsilon 0.0014068110694794949, time 727.0, rides 122\n",
      "Initial State is  [0, 23, 5]\n",
      "episode 1310, reward 798.0, memory_length 2000, epsilon 0.0013997770141320975, time 730.0, rides 121\n",
      "Initial State is  [1, 15, 4]\n",
      "episode 1311, reward 1150.0, memory_length 2000, epsilon 0.001392778129061437, time 730.0, rides 115\n",
      "Initial State is  [3, 17, 3]\n",
      "episode 1312, reward 1156.0, memory_length 2000, epsilon 0.00138581423841613, time 732.0, rides 120\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 1313, reward 1161.0, memory_length 2000, epsilon 0.0013788851672240493, time 729.0, rides 125\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 1314, reward 1106.0, memory_length 2000, epsilon 0.0013719907413879291, time 724.0, rides 122\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 1315, reward 1112.0, memory_length 2000, epsilon 0.0013651307876809894, time 734.0, rides 121\n",
      "Initial State is  [4, 21, 6]\n",
      "episode 1316, reward 1003.0, memory_length 2000, epsilon 0.0013583051337425845, time 733.0, rides 124\n",
      "Initial State is  [1, 11, 1]\n",
      "episode 1317, reward 1198.0, memory_length 2000, epsilon 0.0013515136080738716, time 729.0, rides 125\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 1318, reward 1330.0, memory_length 2000, epsilon 0.0013447560400335022, time 731.0, rides 131\n",
      "Initial State is  [2, 23, 0]\n",
      "episode 1319, reward 1061.0, memory_length 2000, epsilon 0.0013380322598333348, time 740.0, rides 120\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 1320, reward 1151.0, memory_length 2000, epsilon 0.0013313420985341681, time 730.0, rides 117\n",
      "Initial State is  [2, 22, 4]\n",
      "episode 1321, reward 1023.0, memory_length 2000, epsilon 0.0013246853880414973, time 724.0, rides 120\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 1322, reward 988.0, memory_length 2000, epsilon 0.0013180619611012898, time 726.0, rides 125\n",
      "Initial State is  [4, 22, 3]\n",
      "episode 1323, reward 1254.0, memory_length 2000, epsilon 0.0013114716512957834, time 729.0, rides 125\n",
      "Initial State is  [1, 1, 2]\n",
      "episode 1324, reward 1287.0, memory_length 2000, epsilon 0.0013049142930393045, time 732.0, rides 125\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 1325, reward 1091.0, memory_length 2000, epsilon 0.001298389721574108, time 729.0, rides 125\n",
      "Initial State is  [0, 9, 4]\n",
      "episode 1326, reward 1335.0, memory_length 2000, epsilon 0.0012918977729662374, time 727.0, rides 111\n",
      "Initial State is  [4, 20, 5]\n",
      "episode 1327, reward 1234.0, memory_length 2000, epsilon 0.0012854382841014063, time 736.0, rides 120\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 1328, reward 1383.0, memory_length 2000, epsilon 0.0012790110926808992, time 737.0, rides 119\n",
      "Initial State is  [4, 8, 6]\n",
      "episode 1329, reward 897.0, memory_length 2000, epsilon 0.0012726160372174948, time 729.0, rides 132\n",
      "Initial State is  [0, 3, 1]\n",
      "episode 1330, reward 1227.0, memory_length 2000, epsilon 0.0012662529570314073, time 730.0, rides 129\n",
      "Initial State is  [1, 14, 5]\n",
      "episode 1331, reward 1397.0, memory_length 2000, epsilon 0.0012599216922462503, time 728.0, rides 122\n",
      "Initial State is  [2, 18, 0]\n",
      "episode 1332, reward 1037.0, memory_length 2000, epsilon 0.001253622083785019, time 721.0, rides 122\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 1333, reward 1050.0, memory_length 2000, epsilon 0.001247353973366094, time 729.0, rides 116\n",
      "Initial State is  [1, 5, 5]\n",
      "episode 1334, reward 1339.0, memory_length 2000, epsilon 0.0012411172034992636, time 725.0, rides 131\n",
      "Initial State is  [1, 13, 4]\n",
      "episode 1335, reward 1027.0, memory_length 2000, epsilon 0.0012349116174817673, time 740.0, rides 119\n",
      "Initial State is  [1, 12, 0]\n",
      "episode 1336, reward 1174.0, memory_length 2000, epsilon 0.0012287370593943585, time 734.0, rides 138\n",
      "Initial State is  [2, 6, 6]\n",
      "episode 1337, reward 976.0, memory_length 2000, epsilon 0.0012225933740973866, time 723.0, rides 129\n",
      "Initial State is  [4, 11, 4]\n",
      "episode 1338, reward 1218.0, memory_length 2000, epsilon 0.0012164804072268998, time 730.0, rides 132\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 1339, reward 1277.0, memory_length 2000, epsilon 0.0012103980051907653, time 726.0, rides 126\n",
      "Initial State is  [4, 21, 0]\n",
      "episode 1340, reward 1006.0, memory_length 2000, epsilon 0.0012043460151648115, time 735.0, rides 116\n",
      "Initial State is  [1, 13, 6]\n",
      "episode 1341, reward 933.0, memory_length 2000, epsilon 0.0011983242850889873, time 732.0, rides 116\n",
      "Initial State is  [0, 22, 6]\n",
      "episode 1342, reward 1085.0, memory_length 2000, epsilon 0.0011923326636635425, time 736.0, rides 123\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 1343, reward 1239.0, memory_length 2000, epsilon 0.0011863710003452248, time 729.0, rides 125\n",
      "Initial State is  [0, 6, 1]\n",
      "episode 1344, reward 1022.0, memory_length 2000, epsilon 0.0011804391453434987, time 727.0, rides 122\n",
      "Initial State is  [3, 14, 5]\n",
      "episode 1345, reward 1006.0, memory_length 2000, epsilon 0.0011745369496167812, time 738.0, rides 120\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 1346, reward 1064.0, memory_length 2000, epsilon 0.0011686642648686973, time 724.0, rides 127\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 1347, reward 1367.0, memory_length 2000, epsilon 0.001162820943544354, time 726.0, rides 125\n",
      "Initial State is  [4, 8, 3]\n",
      "episode 1348, reward 1114.0, memory_length 2000, epsilon 0.0011570068388266322, time 724.0, rides 119\n",
      "Initial State is  [1, 11, 0]\n",
      "episode 1349, reward 1381.0, memory_length 2000, epsilon 0.001151221804632499, time 730.0, rides 124\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 1350, reward 1072.0, memory_length 2000, epsilon 0.0011454656956093366, time 725.0, rides 137\n",
      "Initial State is  [0, 19, 4]\n",
      "episode 1351, reward 1241.0, memory_length 2000, epsilon 0.00113973836713129, time 728.0, rides 128\n",
      "Initial State is  [3, 5, 3]\n",
      "episode 1352, reward 1128.0, memory_length 2000, epsilon 0.0011340396752956335, time 734.0, rides 127\n",
      "Initial State is  [1, 6, 3]\n",
      "episode 1353, reward 1253.0, memory_length 2000, epsilon 0.0011283694769191554, time 723.0, rides 121\n",
      "Initial State is  [0, 16, 1]\n",
      "episode 1354, reward 1012.0, memory_length 2000, epsilon 0.0011227276295345597, time 727.0, rides 117\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 1355, reward 1322.0, memory_length 2000, epsilon 0.001117113991386887, time 725.0, rides 128\n",
      "Initial State is  [3, 6, 3]\n",
      "episode 1356, reward 987.0, memory_length 2000, epsilon 0.0011115284214299524, time 726.0, rides 118\n",
      "Initial State is  [4, 9, 3]\n",
      "episode 1357, reward 1074.0, memory_length 2000, epsilon 0.0011059707793228026, time 736.0, rides 127\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 1358, reward 685.0, memory_length 2000, epsilon 0.0011004409254261886, time 725.0, rides 126\n",
      "Initial State is  [4, 8, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1359, reward 1220.0, memory_length 2000, epsilon 0.0010949387207990578, time 733.0, rides 127\n",
      "Initial State is  [1, 20, 3]\n",
      "episode 1360, reward 1177.0, memory_length 2000, epsilon 0.0010894640271950624, time 731.0, rides 127\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 1361, reward 1152.0, memory_length 2000, epsilon 0.0010840167070590872, time 731.0, rides 125\n",
      "Initial State is  [3, 8, 0]\n",
      "episode 1362, reward 973.0, memory_length 2000, epsilon 0.0010785966235237919, time 728.0, rides 112\n",
      "Initial State is  [2, 18, 1]\n",
      "episode 1363, reward 1171.0, memory_length 2000, epsilon 0.001073203640406173, time 733.0, rides 118\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 1364, reward 1182.0, memory_length 2000, epsilon 0.001067837622204142, time 725.0, rides 139\n",
      "Initial State is  [0, 19, 4]\n",
      "episode 1365, reward 1229.0, memory_length 2000, epsilon 0.0010624984340931213, time 733.0, rides 127\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 1366, reward 976.0, memory_length 2000, epsilon 0.0010571859419226557, time 730.0, rides 133\n",
      "Initial State is  [2, 22, 2]\n",
      "episode 1367, reward 1313.0, memory_length 2000, epsilon 0.0010519000122130424, time 726.0, rides 121\n",
      "Initial State is  [1, 14, 4]\n",
      "episode 1368, reward 1006.0, memory_length 2000, epsilon 0.0010466405121519772, time 729.0, rides 114\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 1369, reward 1144.0, memory_length 2000, epsilon 0.0010414073095912173, time 729.0, rides 132\n",
      "Initial State is  [0, 2, 6]\n",
      "episode 1370, reward 972.0, memory_length 2000, epsilon 0.001036200273043261, time 729.0, rides 122\n",
      "Initial State is  [2, 16, 1]\n",
      "episode 1371, reward 1153.0, memory_length 2000, epsilon 0.0010310192716780449, time 729.0, rides 113\n",
      "Initial State is  [2, 22, 4]\n",
      "episode 1372, reward 1031.0, memory_length 2000, epsilon 0.0010258641753196547, time 727.0, rides 114\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 1373, reward 1034.0, memory_length 2000, epsilon 0.0010207348544430564, time 729.0, rides 126\n",
      "Initial State is  [4, 2, 4]\n",
      "episode 1374, reward 1145.0, memory_length 2000, epsilon 0.001015631180170841, time 736.0, rides 128\n",
      "Initial State is  [4, 20, 3]\n",
      "episode 1375, reward 707.0, memory_length 2000, epsilon 0.0010105530242699868, time 729.0, rides 132\n",
      "Initial State is  [1, 17, 2]\n",
      "episode 1376, reward 1118.0, memory_length 2000, epsilon 0.001005500259148637, time 728.0, rides 133\n",
      "Initial State is  [0, 19, 6]\n",
      "episode 1377, reward 1102.0, memory_length 2000, epsilon 0.0010004727578528938, time 724.0, rides 114\n",
      "Initial State is  [3, 23, 3]\n",
      "episode 1378, reward 1067.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 126\n",
      "Initial State is  [4, 21, 3]\n",
      "episode 1379, reward 1096.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 125\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 1380, reward 1131.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 130\n",
      "Initial State is  [3, 0, 5]\n",
      "episode 1381, reward 1306.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 125\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 1382, reward 1206.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 117\n",
      "Initial State is  [1, 4, 3]\n",
      "episode 1383, reward 1170.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 118\n",
      "Initial State is  [4, 15, 2]\n",
      "episode 1384, reward 1300.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 122\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 1385, reward 1138.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 126\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 1386, reward 1146.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 113\n",
      "Initial State is  [1, 4, 1]\n",
      "episode 1387, reward 1123.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 121\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 1388, reward 1088.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 125\n",
      "Initial State is  [3, 0, 1]\n",
      "episode 1389, reward 856.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 119\n",
      "Initial State is  [3, 17, 1]\n",
      "episode 1390, reward 1397.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 136\n",
      "Initial State is  [2, 15, 0]\n",
      "episode 1391, reward 1051.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 122\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 1392, reward 1203.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 131\n",
      "Initial State is  [4, 20, 3]\n",
      "episode 1393, reward 1122.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 118\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 1394, reward 996.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 124\n",
      "Initial State is  [0, 2, 1]\n",
      "episode 1395, reward 1100.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 112\n",
      "Initial State is  [4, 12, 0]\n",
      "episode 1396, reward 1052.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 126\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 1397, reward 1273.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 127\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 1398, reward 1122.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 122\n",
      "Initial State is  [1, 19, 5]\n",
      "episode 1399, reward 762.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 126\n",
      "Initial State is  [0, 19, 4]\n",
      "episode 1400, reward 1192.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 127\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 1401, reward 999.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 138\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 1402, reward 991.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 124\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 1403, reward 1160.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 116\n",
      "Initial State is  [2, 23, 0]\n",
      "episode 1404, reward 1173.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 132\n",
      "Initial State is  [3, 12, 3]\n",
      "episode 1405, reward 1254.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 132\n",
      "Initial State is  [0, 21, 2]\n",
      "episode 1406, reward 1000.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 123\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 1407, reward 1111.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 110\n",
      "Initial State is  [4, 3, 5]\n",
      "episode 1408, reward 1495.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 133\n",
      "Initial State is  [3, 22, 4]\n",
      "episode 1409, reward 1322.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 136\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 1410, reward 967.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 120\n",
      "Initial State is  [0, 23, 4]\n",
      "episode 1411, reward 1129.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 118\n",
      "Initial State is  [3, 7, 6]\n",
      "episode 1412, reward 1295.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 124\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 1413, reward 1159.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 127\n",
      "Initial State is  [1, 7, 5]\n",
      "episode 1414, reward 998.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 135\n",
      "Initial State is  [4, 3, 5]\n",
      "episode 1415, reward 1068.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 119\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 1416, reward 1128.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 116\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 1417, reward 1071.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 123\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 1418, reward 1246.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 121\n",
      "Initial State is  [2, 12, 6]\n",
      "episode 1419, reward 1033.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 133\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 1420, reward 1069.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 122\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 1421, reward 983.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 123\n",
      "Initial State is  [3, 23, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1422, reward 1029.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 126\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 1423, reward 1015.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 121\n",
      "Initial State is  [2, 22, 3]\n",
      "episode 1424, reward 1096.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 117\n",
      "Initial State is  [3, 10, 4]\n",
      "episode 1425, reward 874.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 118\n",
      "Initial State is  [3, 11, 4]\n",
      "episode 1426, reward 1324.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 129\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 1427, reward 1285.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 118\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 1428, reward 1118.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 127\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 1429, reward 1190.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 127\n",
      "Initial State is  [3, 8, 6]\n",
      "episode 1430, reward 1165.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 140\n",
      "Initial State is  [1, 22, 2]\n",
      "episode 1431, reward 994.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 114\n",
      "Initial State is  [4, 14, 5]\n",
      "episode 1432, reward 1171.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 130\n",
      "Initial State is  [4, 4, 3]\n",
      "episode 1433, reward 1159.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 119\n",
      "Initial State is  [0, 8, 4]\n",
      "episode 1434, reward 1486.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 126\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 1435, reward 1039.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 131\n",
      "Initial State is  [2, 13, 3]\n",
      "episode 1436, reward 1082.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 126\n",
      "Initial State is  [2, 11, 2]\n",
      "episode 1437, reward 1453.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 117\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 1438, reward 1039.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 114\n",
      "Initial State is  [0, 23, 5]\n",
      "episode 1439, reward 1230.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 128\n",
      "Initial State is  [2, 13, 2]\n",
      "episode 1440, reward 1029.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 129\n",
      "Initial State is  [4, 10, 5]\n",
      "episode 1441, reward 1157.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 118\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 1442, reward 1244.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 118\n",
      "Initial State is  [3, 11, 4]\n",
      "episode 1443, reward 1400.0, memory_length 2000, epsilon 0.0009954703940636294, time 741.0, rides 137\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 1444, reward 1200.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 126\n",
      "Initial State is  [3, 13, 4]\n",
      "episode 1445, reward 1453.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 130\n",
      "Initial State is  [3, 1, 1]\n",
      "episode 1446, reward 1130.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 129\n",
      "Initial State is  [0, 8, 5]\n",
      "episode 1447, reward 1026.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 133\n",
      "Initial State is  [1, 23, 2]\n",
      "episode 1448, reward 1163.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 122\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 1449, reward 987.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 122\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 1450, reward 1237.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 130\n",
      "Initial State is  [0, 12, 2]\n",
      "episode 1451, reward 1009.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 119\n",
      "Initial State is  [1, 23, 2]\n",
      "episode 1452, reward 1294.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 131\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 1453, reward 1444.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 135\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 1454, reward 1068.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 129\n",
      "Initial State is  [1, 0, 5]\n",
      "episode 1455, reward 1267.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 135\n",
      "Initial State is  [0, 13, 4]\n",
      "episode 1456, reward 1229.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 123\n",
      "Initial State is  [1, 13, 2]\n",
      "episode 1457, reward 1285.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 128\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 1458, reward 1170.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 113\n",
      "Initial State is  [4, 14, 2]\n",
      "episode 1459, reward 1197.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 126\n",
      "Initial State is  [3, 9, 0]\n",
      "episode 1460, reward 1178.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 124\n",
      "Initial State is  [0, 19, 3]\n",
      "episode 1461, reward 1144.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 126\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 1462, reward 1086.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 121\n",
      "Initial State is  [0, 23, 0]\n",
      "episode 1463, reward 1551.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 132\n",
      "Initial State is  [4, 17, 2]\n",
      "episode 1464, reward 1296.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 130\n",
      "Initial State is  [0, 5, 2]\n",
      "episode 1465, reward 1007.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 129\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 1466, reward 1231.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 129\n",
      "Initial State is  [4, 20, 4]\n",
      "episode 1467, reward 1478.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 129\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 1468, reward 1042.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 129\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 1469, reward 1143.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 122\n",
      "Initial State is  [3, 9, 5]\n",
      "episode 1470, reward 1203.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 117\n",
      "Initial State is  [3, 10, 5]\n",
      "episode 1471, reward 1265.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 135\n",
      "Initial State is  [2, 0, 1]\n",
      "episode 1472, reward 1238.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 132\n",
      "Initial State is  [2, 11, 4]\n",
      "episode 1473, reward 1099.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 126\n",
      "Initial State is  [4, 20, 6]\n",
      "episode 1474, reward 1456.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 122\n",
      "Initial State is  [3, 22, 0]\n",
      "episode 1475, reward 1111.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 132\n",
      "Initial State is  [1, 20, 3]\n",
      "episode 1476, reward 1094.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 115\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 1477, reward 1176.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 119\n",
      "Initial State is  [3, 3, 2]\n",
      "episode 1478, reward 1170.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 123\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 1479, reward 1095.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 122\n",
      "Initial State is  [0, 12, 0]\n",
      "episode 1480, reward 994.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 123\n",
      "Initial State is  [3, 17, 1]\n",
      "episode 1481, reward 1125.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 130\n",
      "Initial State is  [4, 15, 4]\n",
      "episode 1482, reward 1069.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 132\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 1483, reward 853.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 125\n",
      "Initial State is  [0, 19, 4]\n",
      "episode 1484, reward 1138.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 127\n",
      "Initial State is  [4, 20, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1485, reward 1294.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 113\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 1486, reward 1382.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 131\n",
      "Initial State is  [2, 8, 1]\n",
      "episode 1487, reward 1149.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 130\n",
      "Initial State is  [2, 14, 1]\n",
      "episode 1488, reward 1163.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 117\n",
      "Initial State is  [0, 6, 2]\n",
      "episode 1489, reward 1219.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 139\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 1490, reward 1041.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 131\n",
      "Initial State is  [2, 4, 2]\n",
      "episode 1491, reward 1023.0, memory_length 2000, epsilon 0.0009954703940636294, time 720.0, rides 121\n",
      "Initial State is  [4, 23, 1]\n",
      "episode 1492, reward 866.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 118\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 1493, reward 1508.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 131\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 1494, reward 1344.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 116\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 1495, reward 1091.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 120\n",
      "Initial State is  [2, 2, 5]\n",
      "episode 1496, reward 1182.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 133\n",
      "Initial State is  [1, 12, 6]\n",
      "episode 1497, reward 1152.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 120\n",
      "Initial State is  [2, 8, 1]\n",
      "episode 1498, reward 1086.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 136\n",
      "Initial State is  [4, 10, 4]\n",
      "episode 1499, reward 1025.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 117\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 1500, reward 1142.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 118\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 1501, reward 1311.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 128\n",
      "Initial State is  [3, 7, 2]\n",
      "episode 1502, reward 1120.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 128\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 1503, reward 1185.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 125\n",
      "Initial State is  [0, 0, 1]\n",
      "episode 1504, reward 1091.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 119\n",
      "Initial State is  [1, 16, 0]\n",
      "episode 1505, reward 1108.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 116\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 1506, reward 1045.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 128\n",
      "Initial State is  [3, 1, 6]\n",
      "episode 1507, reward 1193.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 126\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 1508, reward 1059.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 122\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 1509, reward 979.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 125\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 1510, reward 1057.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 125\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 1511, reward 1050.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 127\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 1512, reward 1211.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 122\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 1513, reward 1124.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 119\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 1514, reward 1433.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 124\n",
      "Initial State is  [3, 11, 5]\n",
      "episode 1515, reward 1409.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 123\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 1516, reward 1273.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 131\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 1517, reward 1248.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 129\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 1518, reward 1054.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 133\n",
      "Initial State is  [3, 13, 3]\n",
      "episode 1519, reward 1253.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 127\n",
      "Initial State is  [0, 22, 2]\n",
      "episode 1520, reward 1419.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 127\n",
      "Initial State is  [0, 1, 1]\n",
      "episode 1521, reward 1037.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 114\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 1522, reward 1196.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 126\n",
      "Initial State is  [4, 22, 5]\n",
      "episode 1523, reward 1216.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 124\n",
      "Initial State is  [0, 1, 4]\n",
      "episode 1524, reward 1192.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 131\n",
      "Initial State is  [0, 11, 6]\n",
      "episode 1525, reward 1071.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 121\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 1526, reward 1282.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 128\n",
      "Initial State is  [0, 2, 0]\n",
      "episode 1527, reward 1262.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 115\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 1528, reward 1089.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 117\n",
      "Initial State is  [2, 13, 3]\n",
      "episode 1529, reward 1268.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 128\n",
      "Initial State is  [4, 12, 4]\n",
      "episode 1530, reward 1327.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 119\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 1531, reward 1216.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 127\n",
      "Initial State is  [0, 19, 0]\n",
      "episode 1532, reward 791.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 117\n",
      "Initial State is  [2, 5, 6]\n",
      "episode 1533, reward 1076.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 127\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 1534, reward 1037.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 119\n",
      "Initial State is  [1, 9, 0]\n",
      "episode 1535, reward 1116.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 126\n",
      "Initial State is  [3, 10, 1]\n",
      "episode 1536, reward 1299.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 128\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 1537, reward 925.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 123\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 1538, reward 1114.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 128\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 1539, reward 1233.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 125\n",
      "Initial State is  [4, 8, 5]\n",
      "episode 1540, reward 768.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 115\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 1541, reward 1146.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 122\n",
      "Initial State is  [1, 3, 2]\n",
      "episode 1542, reward 1165.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 121\n",
      "Initial State is  [1, 15, 1]\n",
      "episode 1543, reward 1210.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 124\n",
      "Initial State is  [2, 9, 0]\n",
      "episode 1544, reward 1218.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 131\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 1545, reward 1406.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 129\n",
      "Initial State is  [3, 3, 6]\n",
      "episode 1546, reward 1337.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 134\n",
      "Initial State is  [1, 2, 3]\n",
      "episode 1547, reward 1198.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 125\n",
      "Initial State is  [4, 14, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1548, reward 1164.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 130\n",
      "Initial State is  [1, 13, 2]\n",
      "episode 1549, reward 1104.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 112\n",
      "Initial State is  [2, 14, 1]\n",
      "episode 1550, reward 1237.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 122\n",
      "Initial State is  [4, 13, 6]\n",
      "episode 1551, reward 1191.0, memory_length 2000, epsilon 0.0009954703940636294, time 740.0, rides 120\n",
      "Initial State is  [4, 13, 5]\n",
      "episode 1552, reward 1215.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 122\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 1553, reward 1641.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 125\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 1554, reward 1316.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 122\n",
      "Initial State is  [4, 22, 2]\n",
      "episode 1555, reward 1201.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 137\n",
      "Initial State is  [4, 21, 3]\n",
      "episode 1556, reward 1272.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 125\n",
      "Initial State is  [1, 21, 5]\n",
      "episode 1557, reward 1448.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 139\n",
      "Initial State is  [2, 14, 2]\n",
      "episode 1558, reward 1243.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 130\n",
      "Initial State is  [3, 16, 3]\n",
      "episode 1559, reward 948.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 117\n",
      "Initial State is  [1, 6, 1]\n",
      "episode 1560, reward 1041.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 132\n",
      "Initial State is  [3, 3, 1]\n",
      "episode 1561, reward 1174.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 126\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 1562, reward 1021.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 126\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 1563, reward 1203.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 125\n",
      "Initial State is  [1, 12, 2]\n",
      "episode 1564, reward 1328.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 132\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 1565, reward 906.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 133\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 1566, reward 1236.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 123\n",
      "Initial State is  [4, 15, 0]\n",
      "episode 1567, reward 1134.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 122\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 1568, reward 1147.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 136\n",
      "Initial State is  [1, 13, 1]\n",
      "episode 1569, reward 1466.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 152\n",
      "Initial State is  [1, 20, 4]\n",
      "episode 1570, reward 1240.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 133\n",
      "Initial State is  [0, 21, 6]\n",
      "episode 1571, reward 1234.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 120\n",
      "Initial State is  [4, 11, 0]\n",
      "episode 1572, reward 1190.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 119\n",
      "Initial State is  [4, 3, 5]\n",
      "episode 1573, reward 1166.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 132\n",
      "Initial State is  [4, 4, 6]\n",
      "episode 1574, reward 1235.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 124\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 1575, reward 1202.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 116\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 1576, reward 1255.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 118\n",
      "Initial State is  [4, 2, 3]\n",
      "episode 1577, reward 1083.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 132\n",
      "Initial State is  [1, 17, 5]\n",
      "episode 1578, reward 1113.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 120\n",
      "Initial State is  [4, 23, 4]\n",
      "episode 1579, reward 940.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 128\n",
      "Initial State is  [2, 12, 6]\n",
      "episode 1580, reward 1054.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 123\n",
      "Initial State is  [4, 22, 3]\n",
      "episode 1581, reward 1186.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 131\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 1582, reward 955.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 128\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 1583, reward 1127.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 123\n",
      "Initial State is  [3, 6, 4]\n",
      "episode 1584, reward 1375.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 123\n",
      "Initial State is  [4, 4, 1]\n",
      "episode 1585, reward 1202.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 125\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 1586, reward 969.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 116\n",
      "Initial State is  [1, 5, 2]\n",
      "episode 1587, reward 1053.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 124\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 1588, reward 1246.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 122\n",
      "Initial State is  [4, 6, 1]\n",
      "episode 1589, reward 1226.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 125\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 1590, reward 907.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 122\n",
      "Initial State is  [4, 15, 1]\n",
      "episode 1591, reward 1249.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 123\n",
      "Initial State is  [2, 16, 1]\n",
      "episode 1592, reward 1468.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 131\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 1593, reward 1221.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 140\n",
      "Initial State is  [3, 6, 2]\n",
      "episode 1594, reward 829.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 121\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 1595, reward 1008.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 116\n",
      "Initial State is  [4, 17, 6]\n",
      "episode 1596, reward 1308.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 127\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 1597, reward 1083.0, memory_length 2000, epsilon 0.0009954703940636294, time 739.0, rides 116\n",
      "Initial State is  [4, 19, 4]\n",
      "episode 1598, reward 1070.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 133\n",
      "Initial State is  [3, 16, 0]\n",
      "episode 1599, reward 1080.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 133\n",
      "Initial State is  [3, 3, 1]\n",
      "episode 1600, reward 1018.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 131\n",
      "Initial State is  [3, 23, 0]\n",
      "episode 1601, reward 1219.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 123\n",
      "Initial State is  [4, 14, 3]\n",
      "episode 1602, reward 1005.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 126\n",
      "Initial State is  [0, 18, 5]\n",
      "episode 1603, reward 1162.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 113\n",
      "Initial State is  [1, 16, 5]\n",
      "episode 1604, reward 1134.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 123\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 1605, reward 1017.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 130\n",
      "Initial State is  [4, 18, 4]\n",
      "episode 1606, reward 875.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 134\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 1607, reward 1423.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 132\n",
      "Initial State is  [0, 12, 1]\n",
      "episode 1608, reward 1160.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 131\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 1609, reward 1218.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 121\n",
      "Initial State is  [2, 14, 6]\n",
      "episode 1610, reward 1217.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 113\n",
      "Initial State is  [0, 2, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1611, reward 908.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 117\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 1612, reward 1334.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 122\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 1613, reward 1040.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 128\n",
      "Initial State is  [4, 7, 3]\n",
      "episode 1614, reward 1176.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 124\n",
      "Initial State is  [2, 9, 1]\n",
      "episode 1615, reward 1321.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 134\n",
      "Initial State is  [3, 5, 3]\n",
      "episode 1616, reward 1297.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 117\n",
      "Initial State is  [2, 20, 0]\n",
      "episode 1617, reward 1224.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 117\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 1618, reward 1007.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 127\n",
      "Initial State is  [2, 6, 4]\n",
      "episode 1619, reward 1098.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 118\n",
      "Initial State is  [1, 17, 4]\n",
      "episode 1620, reward 1162.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 128\n",
      "Initial State is  [0, 6, 1]\n",
      "episode 1621, reward 819.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 127\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 1622, reward 1328.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 128\n",
      "Initial State is  [0, 15, 0]\n",
      "episode 1623, reward 1097.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 127\n",
      "Initial State is  [0, 3, 5]\n",
      "episode 1624, reward 1490.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 127\n",
      "Initial State is  [0, 5, 3]\n",
      "episode 1625, reward 937.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 116\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 1626, reward 1065.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 123\n",
      "Initial State is  [2, 17, 3]\n",
      "episode 1627, reward 1005.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 123\n",
      "Initial State is  [4, 20, 3]\n",
      "episode 1628, reward 1193.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 124\n",
      "Initial State is  [3, 0, 0]\n",
      "episode 1629, reward 1149.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 122\n",
      "Initial State is  [0, 4, 3]\n",
      "episode 1630, reward 1313.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 114\n",
      "Initial State is  [3, 1, 1]\n",
      "episode 1631, reward 1250.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 129\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 1632, reward 1397.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 119\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 1633, reward 1195.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 120\n",
      "Initial State is  [2, 21, 1]\n",
      "episode 1634, reward 1137.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 123\n",
      "Initial State is  [4, 8, 0]\n",
      "episode 1635, reward 1099.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 134\n",
      "Initial State is  [2, 8, 6]\n",
      "episode 1636, reward 935.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 127\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 1637, reward 975.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 125\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 1638, reward 1230.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 125\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 1639, reward 1087.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 124\n",
      "Initial State is  [0, 9, 0]\n",
      "episode 1640, reward 1420.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 130\n",
      "Initial State is  [3, 23, 4]\n",
      "episode 1641, reward 1367.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 125\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 1642, reward 1150.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 127\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 1643, reward 1251.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 121\n",
      "Initial State is  [0, 22, 2]\n",
      "episode 1644, reward 1172.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 120\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 1645, reward 1204.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 134\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 1646, reward 1459.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 125\n",
      "Initial State is  [0, 16, 2]\n",
      "episode 1647, reward 835.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 122\n",
      "Initial State is  [1, 18, 3]\n",
      "episode 1648, reward 1373.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 114\n",
      "Initial State is  [1, 9, 4]\n",
      "episode 1649, reward 1420.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 127\n",
      "Initial State is  [1, 22, 5]\n",
      "episode 1650, reward 827.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 111\n",
      "Initial State is  [4, 16, 4]\n",
      "episode 1651, reward 773.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 120\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 1652, reward 694.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 124\n",
      "Initial State is  [1, 1, 2]\n",
      "episode 1653, reward 1074.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 126\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 1654, reward 951.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 128\n",
      "Initial State is  [2, 5, 6]\n",
      "episode 1655, reward 1133.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 125\n",
      "Initial State is  [2, 5, 4]\n",
      "episode 1656, reward 1136.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 123\n",
      "Initial State is  [2, 5, 3]\n",
      "episode 1657, reward 1072.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 128\n",
      "Initial State is  [1, 14, 4]\n",
      "episode 1658, reward 1103.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 117\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 1659, reward 1493.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 130\n",
      "Initial State is  [1, 1, 3]\n",
      "episode 1660, reward 1040.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 135\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 1661, reward 931.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 124\n",
      "Initial State is  [3, 11, 1]\n",
      "episode 1662, reward 1274.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 127\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 1663, reward 1147.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 130\n",
      "Initial State is  [1, 9, 0]\n",
      "episode 1664, reward 905.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 128\n",
      "Initial State is  [1, 7, 5]\n",
      "episode 1665, reward 970.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 121\n",
      "Initial State is  [4, 5, 2]\n",
      "episode 1666, reward 1023.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 119\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 1667, reward 923.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 111\n",
      "Initial State is  [0, 5, 3]\n",
      "episode 1668, reward 1085.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 116\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 1669, reward 1056.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 128\n",
      "Initial State is  [4, 2, 4]\n",
      "episode 1670, reward 1160.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 128\n",
      "Initial State is  [1, 17, 4]\n",
      "episode 1671, reward 1146.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 129\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 1672, reward 1161.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 131\n",
      "Initial State is  [2, 11, 5]\n",
      "episode 1673, reward 1202.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 131\n",
      "Initial State is  [2, 7, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1674, reward 1280.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 127\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 1675, reward 1148.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 117\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 1676, reward 1113.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 135\n",
      "Initial State is  [4, 4, 0]\n",
      "episode 1677, reward 1402.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 127\n",
      "Initial State is  [2, 0, 0]\n",
      "episode 1678, reward 1172.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 122\n",
      "Initial State is  [1, 18, 4]\n",
      "episode 1679, reward 1384.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 128\n",
      "Initial State is  [4, 6, 5]\n",
      "episode 1680, reward 1120.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 130\n",
      "Initial State is  [4, 10, 3]\n",
      "episode 1681, reward 1181.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 138\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 1682, reward 1058.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 122\n",
      "Initial State is  [2, 16, 1]\n",
      "episode 1683, reward 1081.0, memory_length 2000, epsilon 0.0009954703940636294, time 738.0, rides 116\n",
      "Initial State is  [2, 13, 3]\n",
      "episode 1684, reward 970.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 122\n",
      "Initial State is  [1, 1, 2]\n",
      "episode 1685, reward 1332.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 126\n",
      "Initial State is  [1, 1, 1]\n",
      "episode 1686, reward 1557.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 118\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 1687, reward 1150.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 128\n",
      "Initial State is  [3, 16, 3]\n",
      "episode 1688, reward 1256.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 124\n",
      "Initial State is  [4, 14, 5]\n",
      "episode 1689, reward 1262.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 129\n",
      "Initial State is  [0, 18, 5]\n",
      "episode 1690, reward 1073.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 134\n",
      "Initial State is  [4, 3, 5]\n",
      "episode 1691, reward 1170.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 120\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 1692, reward 1206.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 141\n",
      "Initial State is  [3, 7, 2]\n",
      "episode 1693, reward 963.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 126\n",
      "Initial State is  [2, 1, 4]\n",
      "episode 1694, reward 1036.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 118\n",
      "Initial State is  [2, 7, 2]\n",
      "episode 1695, reward 1546.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 133\n",
      "Initial State is  [0, 16, 2]\n",
      "episode 1696, reward 1098.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 126\n",
      "Initial State is  [2, 5, 1]\n",
      "episode 1697, reward 1200.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 128\n",
      "Initial State is  [3, 15, 0]\n",
      "episode 1698, reward 1109.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 129\n",
      "Initial State is  [3, 21, 0]\n",
      "episode 1699, reward 957.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 115\n",
      "Initial State is  [3, 12, 0]\n",
      "episode 1700, reward 1058.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 116\n",
      "Initial State is  [4, 10, 0]\n",
      "episode 1701, reward 1446.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 121\n",
      "Initial State is  [2, 17, 0]\n",
      "episode 1702, reward 1249.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 128\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 1703, reward 1194.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 120\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 1704, reward 1177.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 127\n",
      "Initial State is  [0, 19, 6]\n",
      "episode 1705, reward 951.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 125\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 1706, reward 1080.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 121\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 1707, reward 796.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 129\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 1708, reward 1235.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 125\n",
      "Initial State is  [3, 0, 1]\n",
      "episode 1709, reward 844.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 119\n",
      "Initial State is  [1, 12, 0]\n",
      "episode 1710, reward 971.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 125\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 1711, reward 850.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 115\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 1712, reward 863.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 113\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 1713, reward 1259.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 129\n",
      "Initial State is  [4, 23, 6]\n",
      "episode 1714, reward 1212.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 128\n",
      "Initial State is  [3, 23, 6]\n",
      "episode 1715, reward 901.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 126\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 1716, reward 1148.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 116\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 1717, reward 1156.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 123\n",
      "Initial State is  [1, 19, 5]\n",
      "episode 1718, reward 1005.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 124\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 1719, reward 1220.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 129\n",
      "Initial State is  [3, 17, 5]\n",
      "episode 1720, reward 810.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 123\n",
      "Initial State is  [1, 1, 1]\n",
      "episode 1721, reward 1141.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 122\n",
      "Initial State is  [0, 1, 4]\n",
      "episode 1722, reward 1071.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 128\n",
      "Initial State is  [2, 13, 4]\n",
      "episode 1723, reward 1201.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 136\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 1724, reward 1043.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 120\n",
      "Initial State is  [0, 13, 1]\n",
      "episode 1725, reward 1302.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 124\n",
      "Initial State is  [0, 13, 4]\n",
      "episode 1726, reward 1237.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 127\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 1727, reward 986.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 141\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 1728, reward 1215.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 122\n",
      "Initial State is  [1, 0, 3]\n",
      "episode 1729, reward 1259.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 126\n",
      "Initial State is  [2, 20, 3]\n",
      "episode 1730, reward 929.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 125\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 1731, reward 1288.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 124\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 1732, reward 726.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 125\n",
      "Initial State is  [4, 22, 1]\n",
      "episode 1733, reward 989.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 129\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 1734, reward 1364.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 125\n",
      "Initial State is  [4, 19, 5]\n",
      "episode 1735, reward 1203.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 130\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 1736, reward 950.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 126\n",
      "Initial State is  [0, 22, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1737, reward 1182.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 115\n",
      "Initial State is  [0, 6, 2]\n",
      "episode 1738, reward 1129.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 132\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 1739, reward 1094.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 134\n",
      "Initial State is  [2, 19, 0]\n",
      "episode 1740, reward 1319.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 129\n",
      "Initial State is  [2, 1, 3]\n",
      "episode 1741, reward 951.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 122\n",
      "Initial State is  [4, 22, 1]\n",
      "episode 1742, reward 1045.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 124\n",
      "Initial State is  [3, 12, 2]\n",
      "episode 1743, reward 1038.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 137\n",
      "Initial State is  [2, 20, 6]\n",
      "episode 1744, reward 1103.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 121\n",
      "Initial State is  [4, 3, 6]\n",
      "episode 1745, reward 937.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 128\n",
      "Initial State is  [1, 4, 1]\n",
      "episode 1746, reward 1282.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 126\n",
      "Initial State is  [0, 5, 0]\n",
      "episode 1747, reward 1086.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 144\n",
      "Initial State is  [3, 3, 5]\n",
      "episode 1748, reward 1181.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 128\n",
      "Initial State is  [3, 21, 2]\n",
      "episode 1749, reward 1232.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 118\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 1750, reward 1268.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 125\n",
      "Initial State is  [4, 19, 4]\n",
      "episode 1751, reward 1342.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 133\n",
      "Initial State is  [0, 19, 0]\n",
      "episode 1752, reward 1229.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 131\n",
      "Initial State is  [3, 3, 3]\n",
      "episode 1753, reward 958.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 116\n",
      "Initial State is  [3, 14, 2]\n",
      "episode 1754, reward 1133.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 127\n",
      "Initial State is  [4, 8, 3]\n",
      "episode 1755, reward 955.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 125\n",
      "Initial State is  [0, 2, 1]\n",
      "episode 1756, reward 1171.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 120\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 1757, reward 1115.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 118\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 1758, reward 1126.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 128\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 1759, reward 1266.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 137\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 1760, reward 1172.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 142\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 1761, reward 976.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 124\n",
      "Initial State is  [3, 10, 4]\n",
      "episode 1762, reward 1586.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 128\n",
      "Initial State is  [3, 9, 3]\n",
      "episode 1763, reward 1273.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 130\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 1764, reward 1084.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 132\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 1765, reward 901.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 115\n",
      "Initial State is  [4, 18, 5]\n",
      "episode 1766, reward 1089.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 122\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 1767, reward 1184.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 122\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 1768, reward 1243.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 130\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 1769, reward 1099.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 136\n",
      "Initial State is  [4, 21, 5]\n",
      "episode 1770, reward 1130.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 130\n",
      "Initial State is  [1, 21, 2]\n",
      "episode 1771, reward 1134.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 129\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 1772, reward 1338.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 117\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 1773, reward 942.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 119\n",
      "Initial State is  [4, 7, 2]\n",
      "episode 1774, reward 987.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 133\n",
      "Initial State is  [4, 0, 5]\n",
      "episode 1775, reward 1135.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 125\n",
      "Initial State is  [4, 17, 5]\n",
      "episode 1776, reward 1250.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 133\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 1777, reward 902.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 136\n",
      "Initial State is  [2, 8, 4]\n",
      "episode 1778, reward 1109.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 137\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 1779, reward 1062.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 128\n",
      "Initial State is  [0, 23, 4]\n",
      "episode 1780, reward 1104.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 120\n",
      "Initial State is  [0, 8, 4]\n",
      "episode 1781, reward 1401.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 114\n",
      "Initial State is  [3, 0, 5]\n",
      "episode 1782, reward 1213.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 126\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 1783, reward 1243.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 144\n",
      "Initial State is  [0, 5, 2]\n",
      "episode 1784, reward 1122.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 123\n",
      "Initial State is  [1, 17, 5]\n",
      "episode 1785, reward 1387.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 121\n",
      "Initial State is  [1, 14, 4]\n",
      "episode 1786, reward 980.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 119\n",
      "Initial State is  [4, 21, 3]\n",
      "episode 1787, reward 1167.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 125\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 1788, reward 983.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 118\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 1789, reward 1059.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 126\n",
      "Initial State is  [0, 23, 0]\n",
      "episode 1790, reward 1241.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 125\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 1791, reward 911.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 117\n",
      "Initial State is  [4, 7, 2]\n",
      "episode 1792, reward 991.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 114\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 1793, reward 1053.0, memory_length 2000, epsilon 0.0009954703940636294, time 734.0, rides 111\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 1794, reward 1048.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 126\n",
      "Initial State is  [3, 8, 5]\n",
      "episode 1795, reward 974.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 123\n",
      "Initial State is  [0, 8, 1]\n",
      "episode 1796, reward 1028.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 111\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 1797, reward 1369.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 139\n",
      "Initial State is  [0, 20, 5]\n",
      "episode 1798, reward 1370.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 127\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 1799, reward 1184.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 123\n",
      "Initial State is  [0, 13, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1800, reward 1021.0, memory_length 2000, epsilon 0.0009954703940636294, time 743.0, rides 125\n",
      "Initial State is  [2, 4, 2]\n",
      "episode 1801, reward 1022.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 128\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 1802, reward 1363.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 124\n",
      "Initial State is  [4, 7, 6]\n",
      "episode 1803, reward 1019.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 134\n",
      "Initial State is  [2, 21, 1]\n",
      "episode 1804, reward 884.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 128\n",
      "Initial State is  [3, 19, 4]\n",
      "episode 1805, reward 1109.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 123\n",
      "Initial State is  [4, 22, 5]\n",
      "episode 1806, reward 1055.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 116\n",
      "Initial State is  [4, 5, 1]\n",
      "episode 1807, reward 1078.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 125\n",
      "Initial State is  [1, 20, 5]\n",
      "episode 1808, reward 1127.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 129\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 1809, reward 1120.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 122\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 1810, reward 1121.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 118\n",
      "Initial State is  [0, 19, 0]\n",
      "episode 1811, reward 943.0, memory_length 2000, epsilon 0.0009954703940636294, time 720.0, rides 123\n",
      "Initial State is  [1, 0, 3]\n",
      "episode 1836, reward 1122.0, memory_length 2000, epsilon 0.0009954703940636294, time 736.0, rides 130\n",
      "Initial State is  [0, 9, 2]\n",
      "episode 1837, reward 957.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 133\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 1838, reward 1079.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 127\n",
      "Initial State is  [0, 13, 1]\n",
      "episode 1839, reward 1219.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 121\n",
      "Initial State is  [1, 18, 0]\n",
      "episode 1840, reward 1187.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 123\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 1841, reward 1099.0, memory_length 2000, epsilon 0.0009954703940636294, time 737.0, rides 120\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 1842, reward 1082.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 117\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 1843, reward 1251.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 122\n",
      "Initial State is  [3, 15, 4]\n",
      "episode 1844, reward 1090.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 117\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 1845, reward 853.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 135\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 1846, reward 1285.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 127\n",
      "Initial State is  [2, 11, 5]\n",
      "episode 1847, reward 1148.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 123\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 1848, reward 767.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 121\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 1849, reward 1090.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 141\n",
      "Initial State is  [4, 14, 0]\n",
      "episode 1850, reward 1216.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 129\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 1851, reward 856.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 121\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 1852, reward 1339.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 124\n",
      "Initial State is  [1, 11, 5]\n",
      "episode 1853, reward 1243.0, memory_length 2000, epsilon 0.0009954703940636294, time 722.0, rides 123\n",
      "Initial State is  [0, 11, 6]\n",
      "episode 1854, reward 844.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 121\n",
      "Initial State is  [4, 12, 6]\n",
      "episode 1855, reward 1266.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 128\n",
      "Initial State is  [2, 18, 6]\n",
      "episode 1856, reward 799.0, memory_length 2000, epsilon 0.0009954703940636294, time 726.0, rides 123\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 1857, reward 1196.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 121\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 1858, reward 893.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 114\n",
      "Initial State is  [0, 17, 5]\n",
      "episode 1859, reward 1078.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 126\n",
      "Initial State is  [2, 20, 2]\n",
      "episode 1860, reward 1286.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 133\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 1861, reward 1006.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 125\n",
      "Initial State is  [3, 11, 4]\n",
      "episode 1862, reward 1074.0, memory_length 2000, epsilon 0.0009954703940636294, time 727.0, rides 131\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 1863, reward 1060.0, memory_length 2000, epsilon 0.0009954703940636294, time 723.0, rides 132\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 1864, reward 1158.0, memory_length 2000, epsilon 0.0009954703940636294, time 721.0, rides 140\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 1865, reward 902.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 125\n",
      "Initial State is  [3, 7, 5]\n",
      "episode 1866, reward 1073.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 117\n",
      "Initial State is  [2, 11, 4]\n",
      "episode 1867, reward 1246.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 128\n",
      "Initial State is  [3, 16, 1]\n",
      "episode 1868, reward 1165.0, memory_length 2000, epsilon 0.0009954703940636294, time 732.0, rides 127\n",
      "Initial State is  [2, 13, 3]\n",
      "episode 1869, reward 1283.0, memory_length 2000, epsilon 0.0009954703940636294, time 724.0, rides 125\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 1870, reward 1041.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 121\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 1871, reward 1319.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 127\n",
      "Initial State is  [1, 1, 1]\n",
      "episode 1872, reward 1076.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 127\n",
      "Initial State is  [3, 5, 1]\n",
      "episode 1873, reward 1502.0, memory_length 2000, epsilon 0.0009954703940636294, time 733.0, rides 127\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 1874, reward 1229.0, memory_length 2000, epsilon 0.0009954703940636294, time 728.0, rides 129\n",
      "Initial State is  [1, 7, 5]\n",
      "episode 1875, reward 1165.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 124\n",
      "Initial State is  [4, 10, 3]\n",
      "episode 1876, reward 1056.0, memory_length 2000, epsilon 0.0009954703940636294, time 731.0, rides 130\n",
      "Initial State is  [3, 8, 1]\n",
      "episode 1877, reward 943.0, memory_length 2000, epsilon 0.0009954703940636294, time 735.0, rides 112\n",
      "Initial State is  [4, 21, 0]\n",
      "episode 1878, reward 1190.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 115\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 1879, reward 970.0, memory_length 2000, epsilon 0.0009954703940636294, time 729.0, rides 111\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 1880, reward 847.0, memory_length 2000, epsilon 0.0009954703940636294, time 730.0, rides 123\n",
      "Initial State is  [3, 15, 5]\n",
      "episode 1881, reward 923.0, memory_length 2000, epsilon 0.0009954703940636294, time 725.0, rides 131\n",
      "Initial State is  [3, 16, 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "agent = DQNAgent(36,21)\n",
    "rewards_per_episode, episodes = [], []\n",
    "\n",
    "for episode in range(Episodes):\n",
    "\n",
    "    # Write code here\n",
    "    # Call the environment\n",
    "    env = CabDriver()\n",
    "    # Call all the initialised variables of the environment\n",
    "    state_space = env.state_space\n",
    "    action_space = env.action_space\n",
    "    state = env.state_init\n",
    "    print(\"Initial State is \",state)\n",
    "    time = 0\n",
    "    #Call the DQN agent\n",
    "    terminal_state = False\n",
    "    score = 0\n",
    "    action = agent.get_action(env.state_encod_arch1(state),env)\n",
    "    score += env.reward_func(state,action_space[action],Time_matrix)\n",
    "    next_state,ride_time = env.next_state_func(state,action_space[action],Time_matrix)\n",
    "    time += ride_time\n",
    "    if time >= 24*30:\n",
    "        agent.append_sample(env.state_encod_arch1(state),action,score,env.state_encod_arch1(next_state),True)\n",
    "    else:\n",
    "        agent.append_sample(env.state_encod_arch1(state),action,score,env.state_encod_arch1(next_state),False)\n",
    "    loop = 0\n",
    "    \n",
    "    while not terminal_state:\n",
    "        \n",
    "        # Write your code here\n",
    "        \n",
    "        if time >= 24*30:\n",
    "            terminal_state = True\n",
    "            pass\n",
    "        state = next_state\n",
    "        # 1. Pick epsilon-greedy action from possible actions for the current state\n",
    "        action = agent.get_action(env.state_encod_arch1(state),env)\n",
    "        # 2. Evaluate your reward and next state\n",
    "        reward_curr_ride = env.reward_func(state,action_space[action],Time_matrix)\n",
    "        score+= reward_curr_ride\n",
    "        next_state,ride_time = env.next_state_func(next_state,action_space[action],Time_matrix)\n",
    "        time += ride_time\n",
    "        # 3. Append the experience to the memory\n",
    "        if time >= 24*30:\n",
    "            agent.append_sample(env.state_encod_arch1(state),action,reward_curr_ride,env.state_encod_arch1(next_state),True)\n",
    "        else:\n",
    "            agent.append_sample(env.state_encod_arch1(state),action,reward_curr_ride,env.state_encod_arch1(next_state),False)\n",
    "        # 4. Train the model by calling function agent.train_model\n",
    "        agent.train_model(env)\n",
    "        #print('Time elapsed {} and current loop {}'.format(time,loop))\n",
    "        loop+= 1\n",
    "        # 5. Keep a track of rewards, Q-values, loss\n",
    "    \n",
    "    rewards_per_episode.append(score)   \n",
    "    episodes.append(episode)\n",
    "    \n",
    "    if agent.epsilon > agent.epsilon_min:\n",
    "        agent.epsilon *= agent.epsilon_decay\n",
    "\n",
    "    # every episode:\n",
    "    print(\"episode {0}, reward {1}, memory_length {2}, epsilon {3}, time {4}, rides {5}\".format(episode,\n",
    "                                                                         score,\n",
    "                                                                         len(agent.memory),\n",
    "                                                                         agent.epsilon,time,loop))\n",
    "    # every few episodes:\n",
    "    if episode % 1000 == 0:\n",
    "        # store q-values of some prespecified state-action pairs\n",
    "        # q_dict = agent.store_q_values()\n",
    "\n",
    "        # save model weights\n",
    "        agent.save(name=\"model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try building a similar epsilon-decay function for your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,10000)\n",
    "epsilon = []\n",
    "for i in range(0,10000):\n",
    "    epsilon.append(0 + (1 - 0) * np.exp(-0.0009*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
